{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "498cb0bb",
   "metadata": {},
   "source": [
    "# irp-dbk24 - \"Optimising Demand Response Strategies for Carbon-Intelligent Electricity Use\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d6cd5cc",
   "metadata": {},
   "source": [
    "# Developing Marginal Emissions Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7b39099",
   "metadata": {},
   "source": [
    "**NOTEBOOK PURPOSE(S):**\n",
    "\n",
    "**LIMITATIONS:**\n",
    "\n",
    "**NOTEBOOK OUTPUTS:**\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04ffa399",
   "metadata": {},
   "source": [
    "### Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "23a9e701",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ────────────────────────────────────────────────────────────────────────────\n",
    "# Future (must be first)\n",
    "# ────────────────────────────────────────────────────────────────────────────\n",
    "from __future__ import annotations\n",
    "\n",
    "# ────────────────────────────────────────────────────────────────────────────\n",
    "# Jupyter/Notebook Setup\n",
    "# ────────────────────────────────────────────────────────────────────────────\n",
    "%matplotlib inline\n",
    "from IPython.display import display\n",
    "\n",
    "# ────────────────────────────────────────────────────────────────────────────\n",
    "# Standard Library\n",
    "# ────────────────────────────────────────────────────────────────────────────\n",
    "import binascii\n",
    "import calendar\n",
    "import hashlib\n",
    "import inspect\n",
    "import json\n",
    "import logging\n",
    "import math\n",
    "import os\n",
    "import random\n",
    "import re\n",
    "import time\n",
    "from contextlib import contextmanager\n",
    "from copy import deepcopy\n",
    "from dataclasses import dataclass\n",
    "from datetime import datetime, timedelta\n",
    "from functools import partial, wraps\n",
    "from itertools import combinations, product\n",
    "from multiprocessing import Lock, Manager, Pool, cpu_count\n",
    "from multiprocessing.pool import ThreadPool\n",
    "from pathlib import Path\n",
    "from typing import (\n",
    "    Any, Callable, Dict, Iterable, List, Mapping, Optional,\n",
    "    Sequence, Tuple, Union\n",
    ")\n",
    "from zoneinfo import ZoneInfo\n",
    "\n",
    "# ────────────────────────────────────────────────────────────────────────────\n",
    "# Core Data Handling\n",
    "# ────────────────────────────────────────────────────────────────────────────\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import polars as pl\n",
    "\n",
    "from functools import reduce\n",
    "from operator import add\n",
    "\n",
    "# ────────────────────────────────────────────────────────────────────────────\n",
    "# Machine Learning & Statistics\n",
    "# ────────────────────────────────────────────────────────────────────────────\n",
    "from feature_engine.creation import CyclicalFeatures\n",
    "from pygam import LinearGAM, l, s\n",
    "import statsmodels.api as sm\n",
    "import statsmodels.formula.api as smf\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "from scipy.stats import kurtosis, skew, zscore\n",
    "\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.kernel_approximation import RBFSampler\n",
    "from sklearn.linear_model import LinearRegression, RidgeCV\n",
    "from sklearn.metrics import (\n",
    "    mean_absolute_error,\n",
    "    mean_squared_error,\n",
    "    r2_score,\n",
    "    root_mean_squared_error,\n",
    ")\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import OneHotEncoder, SplineTransformer\n",
    "from sklearn.utils.validation import check_is_fitted\n",
    "\n",
    "# ────────────────────────────────────────────────────────────────────────────\n",
    "# Visualization\n",
    "# ────────────────────────────────────────────────────────────────────────────\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "import matplotlib.ticker as ticker\n",
    "import seaborn as sns\n",
    "\n",
    "# ────────────────────────────────────────────────────────────────────────────\n",
    "# Geospatial\n",
    "# ────────────────────────────────────────────────────────────────────────────\n",
    "import geopandas as gpd\n",
    "from shapely.geometry import Point, Polygon\n",
    "from shapely.wkb import loads\n",
    "from pyproj import Proj, transform"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22fdf4be",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daee84a3",
   "metadata": {},
   "source": [
    "### Utilities"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7061bb1e",
   "metadata": {},
   "source": [
    "#### CSV File Handling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0828268c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _drop_hash_from_part(\n",
    "        part_path: Path,\n",
    "        model_hash: str,\n",
    "        *,\n",
    "        chunk_size: int = 200_000,\n",
    "        delete_if_empty: bool = False,\n",
    ") -> int:\n",
    "    \"\"\"\n",
    "    Remove rows with model_id_hash == `model_hash` from a CSV part file.\n",
    "\n",
    "    - Streams in chunks (no huge memory spikes)\n",
    "    - Writes to a temp file, then atomically replaces the original\n",
    "    - Returns number of rows dropped\n",
    "    - If all rows are dropped:\n",
    "        • delete the file if `delete_if_empty=True`\n",
    "        • otherwise keep a header-only CSV\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    part_path : Path\n",
    "        CSV file to edit in place.\n",
    "    model_hash : str\n",
    "        Value to filter out from the 'model_id_hash' column.\n",
    "    chunk_size : int, default 200_000\n",
    "        Pandas read_csv chunk size.\n",
    "    delete_if_empty : bool, default False\n",
    "        If True and all rows are removed, delete the part file.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    int\n",
    "        Number of rows removed.\n",
    "    \"\"\"\n",
    "    part_path = Path(part_path)\n",
    "    if not part_path.exists():\n",
    "        return 0\n",
    "\n",
    "    # Quick header check\n",
    "    try:\n",
    "        header_df = pd.read_csv(part_path, nrows=0)\n",
    "    except Exception:\n",
    "        # Broken file — leave as-is\n",
    "        return 0\n",
    "    if \"model_id_hash\" not in header_df.columns:\n",
    "        return 0\n",
    "\n",
    "    dropped = 0\n",
    "    kept = 0\n",
    "    tmp_path = part_path.with_suffix(part_path.suffix + \".tmp\")\n",
    "\n",
    "    # Ensure no stale tmp\n",
    "    if tmp_path.exists():\n",
    "        try:\n",
    "            tmp_path.unlink()\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "    first_write = True\n",
    "    try:\n",
    "        for chunk in pd.read_csv(\n",
    "            part_path,\n",
    "            chunksize=chunk_size,\n",
    "            dtype={\"model_id_hash\": \"string\"},  # force string, avoid numeric coercion\n",
    "        ):\n",
    "            if \"model_id_hash\" not in chunk.columns:\n",
    "                # schema changed mid-file? abort safely\n",
    "                dropped = 0\n",
    "                kept = -1\n",
    "                break\n",
    "            mask = chunk[\"model_id_hash\"] != model_hash\n",
    "            kept_chunk = chunk.loc[mask]\n",
    "            n_dropped = int((~mask).sum())\n",
    "            dropped += n_dropped\n",
    "            kept += int(mask.sum())\n",
    "\n",
    "            if kept_chunk.empty:\n",
    "                continue\n",
    "\n",
    "            kept_chunk.to_csv(\n",
    "                tmp_path,\n",
    "                index=False,\n",
    "                mode=\"w\" if first_write else \"a\",\n",
    "                header=first_write,\n",
    "            )\n",
    "            first_write = False\n",
    "\n",
    "        # Nothing matched → no change\n",
    "        if dropped == 0:\n",
    "            if tmp_path.exists():\n",
    "                # wrote identical content; discard temp\n",
    "                try: tmp_path.unlink()\n",
    "                except Exception: pass\n",
    "            return 0\n",
    "\n",
    "        # All rows removed\n",
    "        if kept == 0:\n",
    "            if delete_if_empty:\n",
    "                # Delete original; remove temp if created\n",
    "                try: part_path.unlink()\n",
    "                except Exception: pass\n",
    "                if tmp_path.exists():\n",
    "                    try: tmp_path.unlink()\n",
    "                    except Exception: pass\n",
    "            else:\n",
    "                # Replace with header-only CSV\n",
    "                header_df.to_csv(tmp_path, index=False)\n",
    "                os.replace(tmp_path, part_path)\n",
    "            return dropped\n",
    "\n",
    "        # Normal case: replace atomically\n",
    "        os.replace(tmp_path, part_path)\n",
    "        return dropped\n",
    "\n",
    "    finally:\n",
    "        # Best-effort cleanup\n",
    "        if tmp_path.exists():\n",
    "            try: os.remove(tmp_path)\n",
    "            except Exception: pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "140dcc5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_model_logged_rotating_csv(\n",
    "        model_hash: str,\n",
    "        base_dir: str | Path,\n",
    "        file_prefix: str\n",
    ") -> bool:\n",
    "    \"\"\"\n",
    "    Return True if `model_hash` appears in the rolling-log index for `file_prefix`.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    model_hash : str\n",
    "        The 'model_id_hash' value to look up.\n",
    "    base_dir : str | Path\n",
    "        Directory holding the rolling CSV parts and index.\n",
    "    file_prefix : str\n",
    "        Prefix of the rolling log.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    bool\n",
    "        True if present in the index; False otherwise.\n",
    "    \"\"\"\n",
    "    idx = _read_index(_index_path(Path(base_dir), file_prefix))\n",
    "    if idx.empty or \"model_id_hash\" not in idx.columns:\n",
    "        return False\n",
    "    return str(model_hash) in idx[\"model_id_hash\"].astype(\"string\").values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "59da62ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _list_part_files(\n",
    "    base_dir: Path,\n",
    "    file_prefix: str,\n",
    "    ext: str = \"csv\",\n",
    ") -> list[Path]:\n",
    "    \"\"\"\n",
    "    List existing rolling CSV parts for a given prefix, sorted by numeric part index.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    base_dir : Path\n",
    "        Directory to search.\n",
    "    file_prefix : str\n",
    "        Prefix of the rolling CSV set (e.g., 'marginal_emissions_log').\n",
    "    ext : str, default 'csv'\n",
    "        File extension (without dot).\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    list[Path]\n",
    "        Sorted list of matching part files, e.g. [.../prefix.part000.csv, .../prefix.part001.csv, ...]\n",
    "    \"\"\"\n",
    "    if not base_dir.exists():\n",
    "        return []\n",
    "\n",
    "    rx = re.compile(rf\"^{re.escape(file_prefix)}\\.part(\\d+)\\.{re.escape(ext)}$\")\n",
    "    parts: list[tuple[int, Path]] = []\n",
    "\n",
    "    for p in base_dir.glob(f\"{file_prefix}.part*.{ext}\"):\n",
    "        if not p.is_file():\n",
    "            continue\n",
    "        m = rx.match(p.name)\n",
    "        if m:\n",
    "            parts.append((int(m.group(1)), p))\n",
    "\n",
    "    parts.sort(key=lambda t: t[0])\n",
    "    return [p for _, p in parts]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "aeab441e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_all_logs_rotating_csv(\n",
    "    results_dir: str | Path = \".\",\n",
    "    file_prefix: str = \"marginal_emissions_log\",\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Read only parts referenced by the index; drop duplicate hashes (keep last).\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    results_dir: str | Path\n",
    "        The directory containing the results.\n",
    "    file_prefix: str\n",
    "        The prefix of the log files to read.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pd.DataFrame\n",
    "        The concatenated DataFrame containing the logs.\n",
    "    \"\"\"\n",
    "    # Read the index file\n",
    "    base_dir = Path(results_dir)\n",
    "    idx = _read_index(_index_path(base_dir, file_prefix))\n",
    "    # Check if the index is empty\n",
    "    if idx.empty:\n",
    "        return pd.DataFrame()\n",
    "    # Get the unique parts to read\n",
    "    parts = idx[\"part_file\"].unique().tolist()\n",
    "    # Read the parts into DataFrames\n",
    "    dfs = [pd.read_csv(p) for p in parts if Path(p).exists()]\n",
    "    # Check if any DataFrames were read\n",
    "    if not dfs:\n",
    "        return pd.DataFrame()\n",
    "    # Concatenate the DataFrames\n",
    "    out = pd.concat(dfs, ignore_index=True)\n",
    "    # Drop duplicate model_id_hash entries\n",
    "    if \"model_id_hash\" in out.columns:\n",
    "        out = out.drop_duplicates(subset=[\"model_id_hash\"], keep=\"last\")\n",
    "    return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7e6decb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _read_index(index_path: Path) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Read the rolling-log index CSV (id→part mapping).\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    index_path : Path\n",
    "        Path to '<file_prefix>_index.csv'.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pd.DataFrame\n",
    "        Columns ['model_id_hash','part_file'] or empty frame if not found/invalid.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        idx = pd.read_csv(index_path, dtype={\"model_id_hash\": \"string\", \"part_file\": \"string\"})\n",
    "        if not {\"model_id_hash\",\"part_file\"}.issubset(idx.columns):\n",
    "            raise ValueError(\"Index missing required columns.\")\n",
    "        return idx\n",
    "    except FileNotFoundError:\n",
    "        return pd.DataFrame(columns=[\"model_id_hash\",\"part_file\"])\n",
    "    except Exception:\n",
    "        # Be permissive but return the expected schema\n",
    "        return pd.DataFrame(columns=[\"model_id_hash\",\"part_file\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "77c18bbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_model_from_rotating_csv(\n",
    "        model_hash: str,\n",
    "        results_dir: str | Path = \".\",\n",
    "        file_prefix: str = \"marginal_emissions_log\",\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Remove all rows with `model_id_hash == model_hash` from the rolling CSV set.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    model_hash : str\n",
    "        Identifier to remove.\n",
    "    results_dir : str | Path, default \".\"\n",
    "        Directory holding parts and index.\n",
    "    file_prefix : str, default \"marginal_emissions_log\"\n",
    "        Prefix of the rolling log files.\n",
    "    \"\"\"\n",
    "    base_dir = _ensure_dir(Path(results_dir))\n",
    "    idx_path = _index_path(base_dir, file_prefix)\n",
    "\n",
    "    # Lock the index for the whole operation to avoid races with concurrent writers/readers\n",
    "    with _file_lock(_index_lock_path(idx_path)):\n",
    "        idx = _read_index(idx_path)\n",
    "        if idx.empty:\n",
    "            return\n",
    "\n",
    "        # Drop from referenced part files\n",
    "        for pf in idx.loc[idx[\"model_id_hash\"] == model_hash, \"part_file\"].dropna().unique():\n",
    "            _drop_hash_from_part(Path(pf), model_hash)\n",
    "\n",
    "        # Update index\n",
    "        idx = idx[idx[\"model_id_hash\"] != model_hash]\n",
    "        idx.to_csv(idx_path, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3f9f7e5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_summary_to_rotating_csv(\n",
    "        summary_df: pd.DataFrame,\n",
    "        results_dir: str | Path = \".\",\n",
    "        file_prefix: str = \"marginal_emissions_log\",\n",
    "        max_mb: int = 95,\n",
    "        force_overwrite: bool = False,\n",
    "        naming: PartNaming | None = None,\n",
    "        fsync: bool = False,\n",
    ") -> Path:\n",
    "    \"\"\"\n",
    "    Append a single-row summary to a rolling CSV (<prefix>.partNNN.csv) with strict rotation:\n",
    "    - Per-file lock during append (prevents interleaved writes/duplicate headers)\n",
    "    - Under-lock preflight ensures the write will NOT push the file over `max_mb`\n",
    "      (allocates a new shard if necessary)\n",
    "    - Atomic index update under lock\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    summary_df : pd.DataFrame\n",
    "        Single-row DataFrame with at least a 'model_id_hash' column.\n",
    "    results_dir : str | Path, default \".\"\n",
    "        Directory to write parts and the index into.\n",
    "    file_prefix : str, default \"marginal_emissions_log\"\n",
    "        Prefix of the part files ('<prefix>.partNNN.csv').\n",
    "    max_mb : int, default 95\n",
    "        Rotate when current part would exceed this size (MiB) after the append.\n",
    "    force_overwrite : bool, default False\n",
    "        If True, delete existing rows with the same hash before appending.\n",
    "    naming : PartNaming, optional\n",
    "        Naming convention (token/width/ext). If provided, `ext` should include the dot\n",
    "        (e.g., \".csv\"). Internally we use the extension without the dot for matching.\n",
    "    fsync : bool, default False\n",
    "        If True, call fsync() on the file after writing to ensure data is flushed to disk.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    Path\n",
    "        The part file path that received the append.\n",
    "\n",
    "    Raises\n",
    "    ------\n",
    "    ValueError\n",
    "        If `summary_df` is empty or missing 'model_id_hash'.\n",
    "    \"\"\"\n",
    "    if summary_df.empty:\n",
    "        raise ValueError(\"summary_df is empty.\")\n",
    "    if \"model_id_hash\" not in summary_df.columns:\n",
    "        raise ValueError(\"summary_df must contain 'model_id_hash'.\")\n",
    "    if len(summary_df) != 1:\n",
    "        summary_df = summary_df.iloc[:1].copy()\n",
    "\n",
    "    naming = naming or PartNaming()\n",
    "    base_dir = _ensure_dir(Path(results_dir))\n",
    "    idx_path = _index_path(base_dir, file_prefix)\n",
    "    model_hash = str(summary_df[\"model_id_hash\"].iloc[0])\n",
    "    ext_nodot = naming.ext.lstrip(\".\")\n",
    "\n",
    "    # Optional overwrite: remove old rows (parts + index)\n",
    "    if force_overwrite:\n",
    "        remove_model_from_rotating_csv(model_hash, base_dir, file_prefix)\n",
    "    else:\n",
    "        if is_model_logged_rotating_csv(model_hash, base_dir, file_prefix):\n",
    "            print(f\"[SKIP] Hash already indexed: {model_hash}\")\n",
    "            parts = _list_part_files(base_dir, file_prefix, ext=ext_nodot)\n",
    "            return parts[-1] if parts else base_dir / naming.format(file_prefix, 0)\n",
    "\n",
    "    # Determine candidate shard\n",
    "    parts = _list_part_files(base_dir, file_prefix, ext=ext_nodot)\n",
    "    if parts:\n",
    "        target = parts[-1]\n",
    "    else:\n",
    "        target = allocate_next_part(base_dir, file_prefix, width=naming.width, ext=ext_nodot)\n",
    "\n",
    "    threshold_bytes = int(max_mb * 1024 * 1024)\n",
    "\n",
    "    # --- LOCK AND WRITE TO SHARD SAFELY ---\n",
    "    while True:\n",
    "        shard_lock = Path(str(target) + \".lock\")\n",
    "        with _file_lock(shard_lock):\n",
    "            current_size = Path(target).stat().st_size if Path(target).exists() else 0\n",
    "            write_header = (current_size == 0)\n",
    "            csv_payload = summary_df.to_csv(index=False, header=write_header)\n",
    "            payload_bytes = len(csv_payload.encode(\"utf-8\"))\n",
    "\n",
    "            if current_size + payload_bytes > threshold_bytes:\n",
    "                # rotate: leave lock, allocate new shard, try again\n",
    "                pass\n",
    "            else:\n",
    "                with open(target, \"a\", encoding=\"utf-8\", newline=\"\") as f:\n",
    "                    f.write(csv_payload)\n",
    "                    f.flush()\n",
    "                    if fsync:\n",
    "                        os.fsync(f.fileno())\n",
    "                break\n",
    "\n",
    "        target = allocate_next_part(base_dir, file_prefix, width=naming.width, ext=ext_nodot)\n",
    "\n",
    "    # --- LOCK AND UPDATE INDEX (atomic replace + optional fsync) ---\n",
    "    lock_path = _index_lock_path(idx_path)\n",
    "    with _file_lock(lock_path):\n",
    "        idx = _read_index(idx_path)\n",
    "        already = (\"model_id_hash\" in idx.columns) and (model_hash in idx[\"model_id_hash\"].astype(\"string\").values)\n",
    "        if not already:\n",
    "            idx = pd.concat(\n",
    "                [idx, pd.DataFrame([{\"model_id_hash\": model_hash, \"part_file\": str(target)}])],\n",
    "                ignore_index=True,\n",
    "            )\n",
    "            tmp_idx = idx_path.with_suffix(idx_path.suffix + \".tmp\")\n",
    "            with open(tmp_idx, \"w\", encoding=\"utf-8\", newline=\"\") as fh:\n",
    "                idx.to_csv(fh, index=False)\n",
    "                fh.flush()\n",
    "                if fsync:\n",
    "                    os.fsync(fh.fileno())\n",
    "            os.replace(tmp_idx, idx_path)\n",
    "            if fsync:\n",
    "                # Ensure directory entry for index is durable\n",
    "                dir_fd = os.open(str(idx_path.parent), os.O_DIRECTORY)\n",
    "                try:\n",
    "                    os.fsync(dir_fd)\n",
    "                finally:\n",
    "                    os.close(dir_fd)\n",
    "\n",
    "    print(f\"[SAVE] Appended to {target}, index updated.\")\n",
    "    return target"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "800a561a",
   "metadata": {},
   "source": [
    "#### General"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9432677e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _file_size_mb(path: Path) -> float:\n",
    "    \"\"\"\n",
    "    Return size of `path` in MiB. If file doesn't exist, returns 0.0.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    path : Path\n",
    "        Path to the file.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    float\n",
    "        Size of the file in MiB.\n",
    "    \"\"\"\n",
    "    p = Path(path)\n",
    "    if not p.exists():\n",
    "        return 0.0\n",
    "    return p.stat().st_size / (1024 * 1024.0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88e93f12",
   "metadata": {},
   "source": [
    "#### Logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8a1f9c95",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_existing_hashes(\n",
    "        results_dir: str | Path,\n",
    "        file_prefix: str,\n",
    ") -> set[str]:\n",
    "    \"\"\"\n",
    "    Get all unique `model_id_hash` values from the rolling-log index.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    results_dir : str | Path\n",
    "        Directory containing the rolling CSV parts and index.\n",
    "    file_prefix : str\n",
    "        Prefix of the rolling log files.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    set[str]\n",
    "        Unique model_id_hash values present in the index.\n",
    "    \"\"\"\n",
    "    idx = _read_index(_index_path(Path(results_dir), file_prefix))\n",
    "    if idx.empty or \"model_id_hash\" not in idx.columns:\n",
    "        return set()\n",
    "    # Ensure NA is dropped and cast to Python strings\n",
    "    return set(idx[\"model_id_hash\"].dropna().astype(str).tolist())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "42a7900d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_config_key(\n",
    "        config: Mapping[str, Any],\n",
    "          algo: str = \"sha256\"\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    Create a deterministic hash key for a configuration mapping.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    config : Mapping[str, Any]\n",
    "        Configuration to serialize. Keys should be stringable.\n",
    "    algo : {'sha256','md5','sha1',...}, default 'sha256'\n",
    "        Hash algorithm name passed to hashlib.new.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    str\n",
    "        Hex digest of the normalized, JSON-serialized configuration.\n",
    "    \"\"\"\n",
    "    def _norm(x):\n",
    "        # Order/JSON-stable normalization.\n",
    "        if isinstance(x, Mapping):\n",
    "            # sort by key string to be robust to non-string keys\n",
    "            return {str(k): _norm(v) for k, v in sorted(x.items(), key=lambda kv: str(kv[0]))}\n",
    "        if isinstance(x, (list, tuple)):\n",
    "            return [_norm(v) for v in x]\n",
    "        if isinstance(x, set):\n",
    "            # sets are unordered; sort normalized elements\n",
    "            return sorted(_norm(v) for v in x)\n",
    "        if isinstance(x, (np.floating, np.integer, np.bool_)):\n",
    "            return x.item()\n",
    "        if isinstance(x, (datetime,)):\n",
    "            return x.isoformat()\n",
    "        return x  # strings, ints, floats, bools, None, etc.\n",
    "\n",
    "    payload = json.dumps(\n",
    "        _norm(config),\n",
    "        sort_keys=True,\n",
    "        separators=(\",\", \":\"),\n",
    "        ensure_ascii=False,\n",
    "        default=str,   # last-resort for odd objects\n",
    "    )\n",
    "    h = hashlib.new(algo)\n",
    "    h.update(payload.encode(\"utf-8\"))\n",
    "    return h.hexdigest()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0a39bb2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def signature_for_run(\n",
    "        user_pipeline: Pipeline,\n",
    "        x_columns: list[str],\n",
    "        y: pd.Series | pd.DataFrame,\n",
    "        *,\n",
    "        random_state: int,\n",
    "        eval_splits: tuple[str, ...] = (\"train\", \"validation\"),\n",
    "        compute_test: bool = False,\n",
    "        extra_info: dict | None = None,\n",
    ") -> tuple[str, dict]:\n",
    "    \"\"\"\n",
    "    Build a stable config mapping for a model run and return (hash_key, mapping).\n",
    "\n",
    "    This just standardizes what goes into the signature so different call sites\n",
    "    don’t accidentally diverge.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    user_pipeline : Pipeline\n",
    "        The user-defined pipeline to run.\n",
    "    x_columns : list[str]\n",
    "        The feature columns to use for the model.\n",
    "    y : pd.Series | pd.DataFrame\n",
    "        The target variable(s) for the model.\n",
    "    random_state : int\n",
    "        The random seed to use for the model.\n",
    "    eval_splits : tuple[str, ...], default=(\"train\", \"validation\")\n",
    "        The data splits to evaluate the model on.\n",
    "    compute_test : bool, default=False\n",
    "        Whether to compute metrics on the test split.\n",
    "    extra_info : dict | None, default=None\n",
    "        Any extra information to include in the signature.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    tuple[str, dict]\n",
    "        The hash key and the signature mapping.\n",
    "    \"\"\"\n",
    "    sig = {\n",
    "        \"pipeline_params\": user_pipeline.get_params(deep=True),\n",
    "        \"x_columns\": list(x_columns),\n",
    "        \"y_columns\": _y_columns_for_signature(y),\n",
    "        \"random_state\": int(random_state),\n",
    "        \"eval_splits\": tuple(eval_splits),\n",
    "        \"compute_test\": bool(compute_test),\n",
    "        **(extra_info or {}),\n",
    "    }\n",
    "    return make_config_key(sig), sig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "74c5acd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _y_columns_for_signature(y: pd.Series | pd.DataFrame) -> list[str]:\n",
    "    \"\"\"\n",
    "    Normalize y to a list of column names for signature purposes.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    y : pd.Series | pd.DataFrame\n",
    "        The target variable(s) for the model.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    list[str]\n",
    "        The list of column names for the target variable(s).\n",
    "    \"\"\"\n",
    "    if isinstance(y, pd.DataFrame):\n",
    "        if y.shape[1] != 1:\n",
    "            raise ValueError(\"y must be a Series or single-column DataFrame for signature.\")\n",
    "        return [str(y.columns[0])]\n",
    "    name = getattr(y, \"name\", None)\n",
    "    return [str(name)] if name is not None else [\"y\"]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e0c708b",
   "metadata": {},
   "source": [
    "#### MPI Management"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "aa54ea7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def allocate_next_part(\n",
    "        base_dir: Path,\n",
    "        file_prefix: str,\n",
    "        width: int = 3,\n",
    "        ext: str = \"csv\",\n",
    "        max_retries: int = 32,\n",
    "        jitter_ms: tuple[int, int] = (1, 40),\n",
    ") -> Path:\n",
    "    \"\"\"\n",
    "    Atomically allocate the next rotating part file by creating it exclusively.\n",
    "\n",
    "    Uses os.open(..., O_CREAT|O_EXCL) so only one process can create a given part.\n",
    "    If another process wins the race, we re-scan and try the next part number.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    base_dir : Path\n",
    "        Directory to write part files into (created if missing).\n",
    "    file_prefix : str\n",
    "        Prefix used before \".partNNN.<ext>\".\n",
    "    width : int, default 3\n",
    "        Minimum zero-padding for part numbers if none exist.\n",
    "    ext : str, default \"csv\"\n",
    "        Extension without dot.\n",
    "    max_retries : int, default 32\n",
    "        Maximum attempts before giving up.\n",
    "    jitter_ms : (int, int), default (1, 40)\n",
    "        Random backoff (min,max) milliseconds between retries.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    Path\n",
    "        The newly created, zero-length part file path (claimed for you).\n",
    "\n",
    "    Raises\n",
    "    ------\n",
    "    RuntimeError\n",
    "        If a unique part file cannot be allocated within max_retries.\n",
    "    \"\"\"\n",
    "    base_dir = Path(base_dir)\n",
    "    base_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    for _ in range(max_retries):\n",
    "        path = _next_csv_part_path(base_dir, file_prefix, width=width, ext=ext)\n",
    "        flags = os.O_CREAT | os.O_EXCL | os.O_WRONLY\n",
    "        try:\n",
    "            fd = os.open(path, flags)  # atomic claim\n",
    "            os.close(fd)               # leave it for normal open() later\n",
    "            return path\n",
    "        except FileExistsError:\n",
    "            # Someone else grabbed it; small random backoff, then try again\n",
    "            time.sleep(random.uniform(*jitter_ms) / 1000.0)\n",
    "            continue\n",
    "\n",
    "    raise RuntimeError(\"Failed to allocate a unique part file after many attempts\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "488f17ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _distribute_configs(\n",
    "        configs: list[dict],\n",
    "        rank: int,\n",
    "        size: int,\n",
    "        mode: str = \"stride\"\n",
    ") -> list[dict]:\n",
    "    \"\"\"\n",
    "    Distribute configurations across multiple ranks.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    configs: list[dict]\n",
    "        The list of configurations to distribute.\n",
    "    rank: int\n",
    "        The rank of the current process.\n",
    "    size: int\n",
    "        The total number of processes.\n",
    "    mode: str\n",
    "        The distribution mode (\"stride\" or \"chunked\").\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    list[dict]\n",
    "        The distributed list of configurations.\n",
    "    \"\"\"\n",
    "    # Handle single process case\n",
    "    if size <= 1:\n",
    "        return configs\n",
    "    # Handle multi-process case\n",
    "    if mode == \"stride\":\n",
    "        return configs[rank::size]\n",
    "    # chunked\n",
    "    n = len(configs)\n",
    "    start = (n * rank) // size\n",
    "    end   = (n * (rank + 1)) // size\n",
    "    return configs[start:end]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5b9a18b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "@contextmanager\n",
    "def _file_lock(lock_path: Path, max_wait_s: float = 30.0, jitter_ms: tuple[int,int]=(2,25)):\n",
    "    \"\"\"\n",
    "    Simple cross-process lock using O_CREAT|O_EXCL on a lockfile.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    lock_path : Path\n",
    "        Path to the lock file to create.\n",
    "    max_wait_s : float, default 30.0\n",
    "        Maximum time to wait for the lock before raising TimeoutError.\n",
    "    jitter_ms : (int,int), default (2,25)\n",
    "        Randomized backoff between retries, in milliseconds.\n",
    "\n",
    "    Yields\n",
    "    ------\n",
    "    None\n",
    "        The lock is held for the duration of the context.\n",
    "    \"\"\"\n",
    "    # Create the lock file\n",
    "    deadline = time.time() + float(max_wait_s)\n",
    "    lock_path = Path(lock_path)\n",
    "    last_err = None\n",
    "    # Wait for the lock to be available\n",
    "    while time.time() < deadline:\n",
    "        try:\n",
    "            fd = os.open(lock_path, os.O_CREAT | os.O_EXCL | os.O_WRONLY)\n",
    "            os.close(fd)\n",
    "            try:\n",
    "                yield\n",
    "            finally:\n",
    "                try:\n",
    "                    os.unlink(lock_path)\n",
    "                except FileNotFoundError:\n",
    "                    pass\n",
    "            return\n",
    "        except FileExistsError as e:\n",
    "            last_err = e\n",
    "            time.sleep(random.uniform(*jitter_ms) / 1000.0)\n",
    "    raise TimeoutError(f\"Could not acquire lock: {lock_path}\") from last_err"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b2625a93",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _mpi_context():\n",
    "    \"\"\"\n",
    "    Get the MPI context for distributed training.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    Tuple[COMM, int, int]\n",
    "        The MPI communicator, rank, and size.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        from mpi4py import MPI  # ensures import\n",
    "        comm = MPI.COMM_WORLD\n",
    "        return comm, comm.Get_rank(), comm.Get_size()\n",
    "    except Exception:\n",
    "        class _Dummy:  # single-process stub\n",
    "            def bcast(self, x, root=0): return x\n",
    "            def Barrier(self): pass\n",
    "        return _Dummy(), 0, 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "724d06a5",
   "metadata": {},
   "source": [
    "#### Naming Conventions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6e3f8eb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass(frozen=True)\n",
    "class PartNaming:\n",
    "    token: str = \".part\"   # separator between stem and index\n",
    "    width: int = 3         # zero-pad width\n",
    "    ext: str = \".csv\"      # file extension, with leading dot\n",
    "\n",
    "    def format(self,\n",
    "            stem: str,\n",
    "            idx: int\n",
    "    ) -> str:\n",
    "        \"\"\"\n",
    "        Format a part filename.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        stem : str\n",
    "            The base name of the file (without extension or part token).\n",
    "        idx : int\n",
    "            The part index (zero-padded).\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        str\n",
    "            The formatted part filename.\n",
    "        \"\"\"\n",
    "        return f\"{stem}{self.token}{idx:0{self.width}d}{self.ext}\"\n",
    "\n",
    "    def split(self,\n",
    "            name: str\n",
    "    ) -> Tuple[str, int | None]:\n",
    "        \"\"\"\n",
    "        Split a part filename into its stem and index.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        name : str\n",
    "            The part filename to split.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        Tuple[str, int | None]\n",
    "            The stem and index of the part filename.\n",
    "        \"\"\"\n",
    "        # returns (stem, idx) where idx is None if no part index present\n",
    "        if not name.endswith(self.ext):\n",
    "            # unknown extension; treat everything before first '.' as stem\n",
    "            p = Path(name)\n",
    "            return (p.stem, None)\n",
    "        base = name[: -len(self.ext)]\n",
    "        if self.token in base:\n",
    "            stem, idx_str = base.split(self.token, 1)\n",
    "            if idx_str.isdigit():\n",
    "                return stem, int(idx_str)\n",
    "        return base, None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec78db08",
   "metadata": {},
   "source": [
    "#### Path and Directory Management"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "dbdf1672",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _ensure_dir(\n",
    "        d: str | Path,\n",
    "        *,\n",
    "        resolve: bool = True\n",
    ") -> Path:\n",
    "    \"\"\"\n",
    "    Ensure directory `d` exists and return it as a Path.\n",
    "\n",
    "    - Creates parent directories as needed.\n",
    "    - Raises a clear error if a non-directory already exists at `d`.\n",
    "    - Optionally returns the resolved (absolute) path.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    d : str | Path\n",
    "        Directory path to create if missing.\n",
    "    resolve : bool, default True\n",
    "        If True, return Path.resolve(strict=False) to normalize/absolutize.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    Path\n",
    "        The (optionally resolved) directory path.\n",
    "    \"\"\"\n",
    "    p = Path(d)\n",
    "    if p.exists() and not p.is_dir():\n",
    "        raise NotADirectoryError(f\"Path exists and is not a directory: {p}\")\n",
    "    p.mkdir(parents=True, exist_ok=True)\n",
    "    return p.resolve(strict=False) if resolve else p\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "43d3f9f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _index_lock_path(index_path: Path) -> Path:\n",
    "    \"\"\"\n",
    "    Derive the lock file path for an index CSV (same directory, '.lock' suffix).\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    index_path : Path\n",
    "        Path to the index CSV file.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    Path\n",
    "        Path to the lock file.\n",
    "    \"\"\"\n",
    "    return index_path.with_suffix(index_path.suffix + \".lock\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "842df3e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _index_path(\n",
    "        base_dir: Path,\n",
    "        file_prefix: str\n",
    ") -> Path:\n",
    "    \"\"\"\n",
    "    Build the path to the global index CSV for a given rolling log set.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    base_dir : Path\n",
    "        Directory that holds the rolling CSV parts.\n",
    "    file_prefix : str\n",
    "        Prefix used by the rolling CSV (e.g., 'marginal_emissions_log').\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    Path\n",
    "        '<base_dir>/<file_prefix>_index.csv'\n",
    "    \"\"\"\n",
    "    return Path(base_dir) / f\"{file_prefix}_index.csv\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1e87df59",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _next_csv_part_path(base_dir: Path, file_prefix: str, width: int = 3, ext: str = \"csv\") -> Path:\n",
    "    \"\"\"\n",
    "    Return the next available rotating-CSV part path.\n",
    "\n",
    "    Scans for files named \"<file_prefix>.partNNN.<ext>\" in `base_dir`, where NNN is an\n",
    "    integer with zero-padding. Picks max(N) and returns the next. If none exist, returns\n",
    "    \"...part000.<ext>\" (or the padding width you pass).\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    base_dir : Path\n",
    "        Directory to scan for part files.\n",
    "    file_prefix : str\n",
    "        Prefix used before \".partNNN.<ext>\".\n",
    "    width : int, default 3\n",
    "        Minimum zero-padding width if no files exist yet.\n",
    "    ext : str, default \"csv\"\n",
    "        File extension (without dot).\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    Path\n",
    "        Path for the next part file (not created).\n",
    "    \"\"\"\n",
    "    if width < 1:\n",
    "        raise ValueError(\"width must be >= 1\")\n",
    "\n",
    "    base_dir = Path(base_dir)\n",
    "    pattern = re.compile(rf\"^{re.escape(file_prefix)}\\.part(\\d+)\\.{re.escape(ext)}$\")\n",
    "\n",
    "    max_n = -1\n",
    "    pad = width\n",
    "\n",
    "    for p in base_dir.glob(f\"{file_prefix}.part*.{ext}\"):\n",
    "        m = pattern.match(p.name)\n",
    "        if not m:\n",
    "            continue\n",
    "        n_str = m.group(1)\n",
    "        pad = max(pad, len(n_str))\n",
    "        try:\n",
    "            n = int(n_str)\n",
    "        except ValueError:\n",
    "            continue\n",
    "        if n > max_n:\n",
    "            max_n = n\n",
    "\n",
    "    next_n = max_n + 1\n",
    "    n_str = f\"{next_n:0{pad}d}\"\n",
    "    return base_dir / f\"{file_prefix}.part{n_str}.{ext}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "eb47b71c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _roll_if_needed(\n",
    "        path: Path,\n",
    "        max_mb: int,\n",
    "        *,\n",
    "        naming: PartNaming | None = None\n",
    ") -> Path:\n",
    "    \"\"\"\n",
    "    If `path` exists and is >= max_mb, return the *next* part filename.\n",
    "    Otherwise return `path` unchanged.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    path : Path\n",
    "        Current part file path (e.g., 'prefix.part007.csv').\n",
    "    max_mb : int\n",
    "        Rotation threshold in mebibytes (MiB).\n",
    "    naming : PartNaming, optional\n",
    "        Naming convention (token/width/ext). Uses defaults if not provided.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    Path\n",
    "        Either `path` or a new sibling with incremented part index.\n",
    "    \"\"\"\n",
    "    if not path.exists() or _file_size_mb(path) < float(max_mb):\n",
    "        return path\n",
    "    naming = naming or PartNaming()\n",
    "    stem, idx = naming.split(path.name)\n",
    "    next_idx = (idx or 0) + 1\n",
    "    return path.with_name(naming.format(stem=stem, idx=next_idx))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7397f3e2",
   "metadata": {},
   "source": [
    "#### Scoring & Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6f8d1cdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _compute_group_energy_weights(\n",
    "    df: pd.DataFrame,\n",
    "    group_col: str,\n",
    "    q_col: str,\n",
    "    interval_hours: float = 0.5,\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Aggregate energy weights by group.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pd.DataFrame\n",
    "        Rows for a single split after preprocessing (must contain `group_col` and `q_col`).\n",
    "    group_col : str\n",
    "        Name of the group id column (e.g., 'median_group_id', 'quantile_group_id').\n",
    "    q_col : str\n",
    "        Name of the demand/quantity column used as Q in the regression (usually x_vars[0]).\n",
    "    interval_hours : float, default 0.5\n",
    "        Duration represented by each row in hours (half-hourly = 0.5).\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pd.DataFrame\n",
    "        Columns: [group_col, 'q_sum', 'energy_MWh']\n",
    "        where energy_MWh = q_sum * interval_hours.\n",
    "    \"\"\"\n",
    "    if group_col not in df.columns:\n",
    "        raise KeyError(f\"'{group_col}' not found in df\")\n",
    "    if q_col not in df.columns:\n",
    "        raise KeyError(f\"'{q_col}' not found in df\")\n",
    "    if not np.issubdtype(np.asarray(df[q_col]).dtype, np.number):\n",
    "        raise TypeError(f\"'{q_col}' must be numeric\")\n",
    "    if interval_hours <= 0:\n",
    "        raise ValueError(\"interval_hours must be > 0\")\n",
    "\n",
    "    g = (\n",
    "        df.groupby(group_col, observed=True)[q_col]\n",
    "          .sum()\n",
    "          .rename(\"q_sum\")\n",
    "          .reset_index()\n",
    "    )\n",
    "    g[\"energy_MWh\"] = g[\"q_sum\"] * float(interval_hours)\n",
    "    return g\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6da05a8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def finite_difference_me_metrics(\n",
    "    df: pd.DataFrame,\n",
    "    time_col: str = \"timestamp\",\n",
    "    q_col: str = \"demand_met\",\n",
    "    y_col: str = \"tons_co2\",\n",
    "    me_col: str = \"ME\",\n",
    "    group_keys: list[str] | tuple[str, ...] = (\"city\",),\n",
    "    max_dt: pd.Timedelta = pd.Timedelta(\"2h\"),\n",
    "    min_abs_dq: float = 1e-6,\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Compare predicted ME to observed short-horizon slopes s = Δy/ΔQ on held-out data.\n",
    "\n",
    "    For each group in `group_keys`:\n",
    "      Δy = y_t - y_{t-1}, ΔQ = Q_t - Q_{t-1}, Δt = t - t_{t-1}\n",
    "      Keep pairs with Δt ≤ max_dt and |ΔQ| ≥ min_abs_dq.\n",
    "      s_t = Δy / ΔQ, ME_avg = 0.5*(ME_t + ME_{t-1})\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pd.DataFrame\n",
    "        One row per group and an optional pooled 'ALL' row:\n",
    "        ['pearson_r','spearman_r','rmse','mae','n_pairs', *group_keys]\n",
    "    \"\"\"\n",
    "    if time_col not in df.columns:\n",
    "        raise KeyError(f\"'{time_col}' not in df\")\n",
    "    # ensure datetime for Δt filtering\n",
    "    dt_series = pd.to_datetime(df[time_col], errors=\"coerce\")\n",
    "    if dt_series.isna().any():\n",
    "        raise ValueError(f\"Column '{time_col}' contains non-parseable datetimes\")\n",
    "    work = df.copy()\n",
    "    work[time_col] = dt_series\n",
    "\n",
    "    def _per_group(gdf: pd.DataFrame) -> dict:\n",
    "        gdf = gdf.sort_values(time_col).copy()\n",
    "        gdf[\"dt\"] = gdf[time_col].diff()\n",
    "        gdf[\"dQ\"] = gdf[q_col].diff()\n",
    "        gdf[\"dY\"] = gdf[y_col].diff()\n",
    "        gdf[\"ME_avg\"] = 0.5 * (gdf[me_col] + gdf[me_col].shift(1))\n",
    "\n",
    "        mask = (\n",
    "            gdf[\"dt\"].notna() & (gdf[\"dt\"] <= max_dt)\n",
    "            & gdf[\"dQ\"].notna() & (np.abs(gdf[\"dQ\"]) >= float(min_abs_dq))\n",
    "            & gdf[\"dY\"].notna() & gdf[\"ME_avg\"].notna()\n",
    "        )\n",
    "        sub = gdf.loc[mask, [\"dY\", \"dQ\", \"ME_avg\"]]\n",
    "        if sub.empty:\n",
    "            return {\"pearson_r\": np.nan, \"spearman_r\": np.nan, \"rmse\": np.nan, \"mae\": np.nan, \"n_pairs\": 0}\n",
    "\n",
    "        s = sub[\"dY\"].to_numpy(dtype=float) / sub[\"dQ\"].to_numpy(dtype=float)\n",
    "        me = sub[\"ME_avg\"].to_numpy(dtype=float)\n",
    "        return {\n",
    "            \"pearson_r\": float(pd.Series(s).corr(pd.Series(me))),\n",
    "            \"spearman_r\": float(pd.Series(s).corr(pd.Series(me), method=\"spearman\")),\n",
    "            \"rmse\": float(root_mean_squared_error(s, me)),\n",
    "            \"mae\": float(mean_absolute_error(s, me)),\n",
    "            \"n_pairs\": int(len(sub)),\n",
    "        }\n",
    "\n",
    "    parts: list[dict] = []\n",
    "    if group_keys:\n",
    "        for keys, gdf in work.groupby(list(group_keys), observed=True, sort=True):\n",
    "            row = _per_group(gdf)\n",
    "            if isinstance(keys, tuple):\n",
    "                for kname, kval in zip(group_keys, keys):\n",
    "                    row[kname] = kval\n",
    "            else:\n",
    "                row[group_keys[0]] = keys\n",
    "            parts.append(row)\n",
    "    else:\n",
    "        parts.append(_per_group(work) | {\"group\": \"ALL\"})\n",
    "\n",
    "    out = pd.DataFrame(parts)\n",
    "\n",
    "    # pooled row\n",
    "    if group_keys and (not out.empty) and out[\"n_pairs\"].sum() > 0:\n",
    "        tmp = []\n",
    "        for _, gdf in work.groupby(list(group_keys), observed=True, sort=True):\n",
    "            gdf = gdf.sort_values(time_col).copy()\n",
    "            gdf[\"dt\"] = gdf[time_col].diff()\n",
    "            gdf[\"dQ\"] = gdf[q_col].diff()\n",
    "            gdf[\"dY\"] = gdf[y_col].diff()\n",
    "            gdf[\"ME_avg\"] = 0.5 * (gdf[me_col] + gdf[me_col].shift(1))\n",
    "            mask = (\n",
    "                gdf[\"dt\"].notna() & (gdf[\"dt\"] <= max_dt)\n",
    "                & gdf[\"dQ\"].notna() & (np.abs(gdf[\"dQ\"]) >= float(min_abs_dq))\n",
    "                & gdf[\"dY\"].notna() & gdf[\"ME_avg\"].notna()\n",
    "            )\n",
    "            sub = gdf.loc[mask, [\"dY\", \"dQ\", \"ME_avg\"]]\n",
    "            if not sub.empty:\n",
    "                tmp.append(\n",
    "                    pd.DataFrame({\n",
    "                        \"s\": sub[\"dY\"].to_numpy(dtype=float) / sub[\"dQ\"].to_numpy(dtype=float),\n",
    "                        \"ME_avg\": sub[\"ME_avg\"].to_numpy(dtype=float),\n",
    "                    })\n",
    "                )\n",
    "        if tmp:\n",
    "            pooled = pd.concat(tmp, ignore_index=True)\n",
    "            pooled_row = {\n",
    "                \"pearson_r\": float(pooled[\"s\"].corr(pooled[\"ME_avg\"])),\n",
    "                \"spearman_r\": float(pooled[\"s\"].corr(pooled[\"ME_avg\"], method=\"spearman\")),\n",
    "                \"rmse\": float(root_mean_squared_error(pooled[\"s\"], pooled[\"ME_avg\"])),\n",
    "                \"mae\": float(mean_absolute_error(pooled[\"s\"], pooled[\"ME_avg\"])),\n",
    "                \"n_pairs\": int(len(pooled)),\n",
    "            }\n",
    "            for k in group_keys:\n",
    "                pooled_row[k] = \"ALL\"\n",
    "            out = pd.concat([out, pd.DataFrame([pooled_row])], ignore_index=True)\n",
    "\n",
    "    return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "efa7017a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def macro_micro_means(df: pd.DataFrame, metric: str, weight_col: str = \"n_obs\") -> dict:\n",
    "    \"\"\"\n",
    "    Compute macro (simple mean) and micro (weighted by `weight_col`) for a metric.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pd.DataFrame\n",
    "        Per-group metrics.\n",
    "    metric : str\n",
    "        Column name to average.\n",
    "    weight_col : str, default \"n_obs\"\n",
    "        Column to use as weights for micro average.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    dict\n",
    "        {\"macro\": float, \"micro\": float}\n",
    "    \"\"\"\n",
    "    macro = float(np.nanmean(df[metric].to_numpy(dtype=float)))\n",
    "    if (weight_col in df) and np.nansum(df[weight_col].to_numpy(dtype=float)) > 0:\n",
    "        micro = float(np.average(df[metric], weights=df[weight_col]))\n",
    "    else:\n",
    "        micro = np.nan\n",
    "    return {\"macro\": macro, \"micro\": micro}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d86c328c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_absolute_percentage_error(\n",
    "        y_true,\n",
    "        y_pred,\n",
    "        eps: float = 1e-6\n",
    ") -> float:\n",
    "    \"\"\"\n",
    "    Compute MAPE robustly - adding small constant to avoid division by zero.\n",
    "\n",
    "    MAPE = mean(|(y_true - y_pred) / (|y_true| + eps)|) * 100\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    y_true : array-like\n",
    "        Ground-truth values.\n",
    "    y_pred : array-like\n",
    "        Predicted values.\n",
    "    eps : float, default 1e-6\n",
    "        Small constant to avoid division by zero.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    float\n",
    "        Mean absolute percentage error in percent.\n",
    "    \"\"\"\n",
    "    # true values for y\n",
    "    yt = np.asarray(y_true, dtype=float)\n",
    "    # predicted values for y\n",
    "    yp = np.asarray(y_pred, dtype=float)\n",
    "    # denominator\n",
    "    denom = np.abs(yt) + float(eps)\n",
    "    # compute MAPE\n",
    "    m = np.abs((yt - yp) / denom)\n",
    "    # return as percentage (*100)\n",
    "    return float(np.nanmean(m) * 100.0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "2ac69557",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_metric(df: pd.DataFrame, metric: str) -> float:\n",
    "    \"\"\"\n",
    "    Compute the mean of a metric, with a special case for MSE derived from RMSE.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pd.DataFrame\n",
    "        DataFrame containing metric columns.\n",
    "    metric : {\"r2\",\"rmse\",\"mae\",\"mape\",\"n_obs\",\"mse\"}\n",
    "        Metric to aggregate.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    float\n",
    "        NaN-safe mean of the requested metric.\n",
    "\n",
    "    Raises\n",
    "    ------\n",
    "    KeyError\n",
    "        If required columns are missing.\n",
    "    \"\"\"\n",
    "    if metric == \"mse\":\n",
    "        if \"rmse\" not in df:\n",
    "            raise KeyError(\"Cannot compute 'mse': 'rmse' column missing.\")\n",
    "        return float(np.nanmean(df[\"rmse\"].to_numpy(dtype=float) ** 2))\n",
    "    if metric not in df:\n",
    "        raise KeyError(f\"Metric '{metric}' not found in DataFrame.\")\n",
    "    return float(np.nanmean(df[metric].to_numpy(dtype=float)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b7dc326b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pooled_co2_metrics(\n",
    "    regressor,                  # fitted GroupwiseRegressor\n",
    "    transformed_df: pd.DataFrame,\n",
    "    y_col: str | None = None,\n",
    "    group_col: str | None = None,\n",
    ") -> dict:\n",
    "    \"\"\"\n",
    "    Compute pooled (all bins together) out-of-sample metrics for CO2.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    regressor : GroupwiseRegressor\n",
    "        Must be fitted; `regressor.group_models_` is used per group.\n",
    "    transformed_df : pd.DataFrame\n",
    "        Contains features used by the regressor, the group column, and the true y.\n",
    "        (Typically validation/test X after feature+binner, with y added).\n",
    "    y_col : str, optional\n",
    "        Target column name. Defaults to regressor.y_var.\n",
    "    group_col : str, optional\n",
    "        Group column name. Defaults to regressor.group_col.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    dict\n",
    "        {'r2','rmse','mae','mape','n_obs'} (NaNs if insufficient data).\n",
    "    \"\"\"\n",
    "    y_col = y_col or regressor.y_var\n",
    "    group_col = group_col or regressor.group_col\n",
    "    if y_col not in transformed_df.columns:\n",
    "        raise KeyError(f\"'{y_col}' not found in transformed_df\")\n",
    "    if group_col not in transformed_df.columns:\n",
    "        raise KeyError(f\"'{group_col}' not found in transformed_df\")\n",
    "\n",
    "    preds = pd.Series(index=transformed_df.index, dtype=float)\n",
    "    for g, gdf in transformed_df.groupby(group_col, sort=True):\n",
    "        model = regressor.group_models_.get(g)\n",
    "        if model is None:\n",
    "            continue\n",
    "        preds.loc[gdf.index] = model.predict(gdf)\n",
    "\n",
    "    mask = preds.notna()\n",
    "    n_obs = int(mask.sum())\n",
    "    if n_obs == 0:\n",
    "        return {\"r2\": np.nan, \"rmse\": np.nan, \"mae\": np.nan, \"mape\": np.nan, \"n_obs\": 0}\n",
    "\n",
    "    y_true = transformed_df.loc[mask, y_col].to_numpy(dtype=float)\n",
    "    y_pred = preds.loc[mask].to_numpy(dtype=float)\n",
    "\n",
    "    # r2 can error for <2 samples or constant y\n",
    "    try:\n",
    "        r2 = float(r2_score(y_true, y_pred))\n",
    "    except Exception:\n",
    "        r2 = np.nan\n",
    "\n",
    "    return {\n",
    "        \"r2\": r2,\n",
    "        \"rmse\": float(root_mean_squared_error(y_true, y_pred)),\n",
    "        \"mae\": float(mean_absolute_error(y_true, y_pred)),\n",
    "        \"mape\": float(mean_absolute_percentage_error(y_true, y_pred)),\n",
    "        \"n_obs\": n_obs,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "1623002c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarise_metrics_logs(\n",
    "        train_logs: pd.DataFrame,\n",
    "        val_logs: pd.DataFrame,\n",
    "        test_logs: pd.DataFrame | None = None,\n",
    "        user_pipeline: Pipeline = None,\n",
    "        x_columns: list | None = None,\n",
    "        random_state: int = 12,\n",
    "        group_col_name: str = \"group\",\n",
    "        pooled_metrics_by_split: dict[str, dict] | None = None,\n",
    "        fd_me_metrics_by_split: dict[str, dict] | None = None,\n",
    "        energy_weight_col: str = \"energy_MWh\",\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Summarise per-split, per-group metrics and pipeline metadata into a single-row DataFrame.\n",
    "\n",
    "    This variant allows `test_logs` to be None (can skip test during tuning).\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    train_logs, val_logs : pd.DataFrame\n",
    "        Metrics frames for train/validation.\n",
    "    test_logs : pd.DataFrame or None, default None\n",
    "        Test metrics; if None, test columns are omitted from the summary.\n",
    "    user_pipeline : Pipeline\n",
    "        The fitted or configured pipeline (used for metadata).\n",
    "    x_columns : list, optional\n",
    "        Feature names used by the model.\n",
    "    random_state : int, default 12\n",
    "        Random seed to record.\n",
    "    group_col_name : str, default \"group\"\n",
    "        Canonical name for the group column.\n",
    "    pooled_metrics_by_split, fd_me_metrics_by_split : dict, optional\n",
    "        Optional extra diagnostics keyed by split.\n",
    "    energy_weight_col : str, default \"energy_MWh\"\n",
    "        Column name to use for energy-weighted micro-averages if present.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pd.DataFrame\n",
    "        One-row summary. Only includes split columns for the splits provided.\n",
    "    \"\"\"\n",
    "    def _norm(df: pd.DataFrame) -> pd.DataFrame:\n",
    "        if df is None or df.empty:\n",
    "            return df\n",
    "\n",
    "        cols = list(df.columns)\n",
    "\n",
    "        # If desired already present, use it\n",
    "        if group_col_name in cols:\n",
    "            return df\n",
    "\n",
    "        # If a plain 'group' exists, rename it to the desired name\n",
    "        if \"group\" in cols:\n",
    "            return df.rename(columns={\"group\": group_col_name})\n",
    "\n",
    "        # Known aliases we can rename from\n",
    "        candidates = [\n",
    "            \"multi_group_id\",\n",
    "            \"quantile_group_id\",\n",
    "            \"median_group_id\",\n",
    "            \"original_quantile_group_id\",\n",
    "            \"group_id\",\n",
    "        ]\n",
    "\n",
    "        # Any *_group_id pattern\n",
    "        pattern_hits = [c for c in cols if c.endswith(\"_group_id\")]\n",
    "\n",
    "        # Prefer known aliases in order\n",
    "        for c in candidates:\n",
    "            if c in cols:\n",
    "                return df.rename(columns={c: group_col_name})\n",
    "\n",
    "        # If exactly one *_group_id exists, use it\n",
    "        if len(pattern_hits) == 1:\n",
    "            return df.rename(columns={pattern_hits[0]: group_col_name})\n",
    "\n",
    "        # Nothing we recognize → fail loudly with context\n",
    "        raise KeyError(\n",
    "            f\"Could not locate a group column; expected '{group_col_name}' or any of \"\n",
    "            f\"{[c for c in candidates if c in cols] + (['group'] if 'group' in cols else []) or candidates + ['group']}. \"\n",
    "            f\"Available columns: {cols}\"\n",
    "        )\n",
    "    splits: dict[str, pd.DataFrame] = {\n",
    "        \"train\": _norm(train_logs.copy()),\n",
    "        \"validation\": _norm(val_logs.copy()),\n",
    "    }\n",
    "    if test_logs is not None:\n",
    "        splits[\"test\"] = _norm(test_logs.copy())\n",
    "\n",
    "    required = {\"r2\", \"rmse\", \"mae\", \"mape\", \"n_obs\"}\n",
    "    for name, df in splits.items():\n",
    "        missing = required.difference(df.columns)\n",
    "        if missing:\n",
    "            raise ValueError(f\"{name} logs missing metrics: {sorted(missing)}\")\n",
    "\n",
    "    first = next(iter(splits.values()))\n",
    "    model_id = first.get(\"model_id_hash\", pd.Series([np.nan])).iloc[0]\n",
    "    log_time = first.get(\"log_time\", pd.Series([np.nan])).iloc[0]\n",
    "    model_name = user_pipeline._final_estimator.__class__.__name__ if user_pipeline is not None else \"\"\n",
    "    pipeline_steps = list(user_pipeline.named_steps.keys()) if user_pipeline is not None else []\n",
    "\n",
    "    summary: dict[str, Any] = {\n",
    "        \"model_id_hash\": model_id,\n",
    "        \"random_state\": random_state,\n",
    "        \"params_json\": json.dumps(\n",
    "            user_pipeline.get_params(deep=True), sort_keys=True, separators=(\",\", \":\"), default=str\n",
    "        ) if user_pipeline is not None else \"{}\",\n",
    "        \"log_time\": log_time,\n",
    "        \"model_name\": model_name,\n",
    "        \"pipeline_steps\": pipeline_steps,\n",
    "        \"pipeline_n_steps\": len(pipeline_steps),\n",
    "        \"x_columns\": x_columns or [],\n",
    "        \"metrics_by_group\": {},\n",
    "    }\n",
    "\n",
    "    nested: dict[str, dict] = {}\n",
    "    for split, df in splits.items():\n",
    "        # macro means\n",
    "        summary[f\"r2_{split}\"] = float(df[\"r2\"].mean())\n",
    "        summary[f\"rmse_{split}\"] = float(df[\"rmse\"].mean())\n",
    "        summary[f\"mae_{split}\"] = float(df[\"mae\"].mean())\n",
    "        summary[f\"mape_{split}\"] = float(df[\"mape\"].mean())\n",
    "        # counts should be sums, not means\n",
    "        summary[f\"n_obs_{split}\"] = int(df[\"n_obs\"].sum())\n",
    "        summary[f\"mse_{split}\"] = float((df[\"rmse\"] ** 2).mean())\n",
    "\n",
    "        # micro by n_obs\n",
    "        if df[\"n_obs\"].sum() > 0:\n",
    "            w = df[\"n_obs\"].to_numpy(dtype=float)\n",
    "            summary[f\"r2_{split}_micro\"] = float(np.average(df[\"r2\"], weights=w))\n",
    "            summary[f\"rmse_{split}_micro\"] = float(np.average(df[\"rmse\"], weights=w))\n",
    "            summary[f\"mae_{split}_micro\"] = float(np.average(df[\"mae\"], weights=w))\n",
    "            summary[f\"mape_{split}_micro\"] = float(np.average(df[\"mape\"], weights=w))\n",
    "        else:\n",
    "            summary[f\"r2_{split}_micro\"] = np.nan\n",
    "            summary[f\"rmse_{split}_micro\"] = np.nan\n",
    "            summary[f\"mae_{split}_micro\"] = np.nan\n",
    "            summary[f\"mape_{split}_micro\"] = np.nan\n",
    "\n",
    "        # energy-weighted micro (if provided)\n",
    "        if (energy_weight_col in df.columns) and (df[energy_weight_col].fillna(0).sum() > 0):\n",
    "            wE = df[energy_weight_col].fillna(0).to_numpy(dtype=float)\n",
    "            summary[f\"r2_{split}_energy_micro\"] = float(np.average(df[\"r2\"], weights=wE))\n",
    "            summary[f\"rmse_{split}_energy_micro\"] = float(np.average(df[\"rmse\"], weights=wE))\n",
    "            summary[f\"mae_{split}_energy_micro\"] = float(np.average(df[\"mae\"], weights=wE))\n",
    "            summary[f\"mape_{split}_energy_micro\"] = float(np.average(df[\"mape\"], weights=wE))\n",
    "            summary[f\"{energy_weight_col}_{split}_total\"] = float(wE.sum())\n",
    "        else:\n",
    "            summary[f\"r2_{split}_energy_micro\"] = np.nan\n",
    "            summary[f\"rmse_{split}_energy_micro\"] = np.nan\n",
    "            summary[f\"mae_{split}_energy_micro\"] = np.nan\n",
    "            summary[f\"mape_{split}_energy_micro\"] = np.nan\n",
    "            summary[f\"{energy_weight_col}_{split}_total\"] = 0.0\n",
    "\n",
    "        cols = [\"r2\", \"rmse\", \"mae\", \"mape\", \"n_obs\"]\n",
    "        if energy_weight_col in df.columns:\n",
    "            cols.append(energy_weight_col)\n",
    "        nested[split] = df.set_index(group_col_name)[cols].to_dict(orient=\"index\")\n",
    "\n",
    "    summary[\"metrics_by_group\"] = nested\n",
    "\n",
    "    pooled_metrics_by_split = pooled_metrics_by_split or {}\n",
    "    fd_me_metrics_by_split = fd_me_metrics_by_split or {}\n",
    "    for split in splits.keys():\n",
    "        summary[f\"pooled_co2_{split}\"] = json.dumps(pooled_metrics_by_split.get(split, {}))\n",
    "        summary[f\"fd_me_{split}\"] = json.dumps(fd_me_metrics_by_split.get(split, {}))\n",
    "\n",
    "    return pd.DataFrame([summary])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a77c9ad9",
   "metadata": {},
   "source": [
    "### Transformers / Classes "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc771dd5",
   "metadata": {},
   "source": [
    "#### Feature Engineering Transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "9c6a3318",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AnalysisFeatureAdder(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"\n",
    "    Add core temporal and quantitative features used in the original analysis.\n",
    "\n",
    "    Adds:\n",
    "      - time_id:              HH-MM string from `timestamp_col`\n",
    "      - <Q>_sqrd:             square of `demand_met_col`\n",
    "      - log_<Q>:              log(demand_met + ε)\n",
    "      - log_<Q>_sqrd:         (log_<Q>)^2\n",
    "      - log_<CO2>:            log(tons_co2 + ε) (only if `co2_col` present)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        timestamp_col: str = \"timestamp\",\n",
    "        demand_met_col: str = \"demand_met\",\n",
    "        co2_col: str = \"tons_co2\",\n",
    "        epsilon: float = 1e-6,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        timestamp_col : str\n",
    "            Name of the datetime column (parseable by pandas).\n",
    "        demand_met_col : str\n",
    "            Name of the demand column.\n",
    "        co2_col : str\n",
    "            Name of the CO2 column (optional at transform time).\n",
    "        epsilon : float, default 1e-6\n",
    "            Small constant to avoid log(0).\n",
    "        \"\"\"\n",
    "        if not isinstance(timestamp_col, str):\n",
    "            raise ValueError(\"timestamp_col must be a string\")\n",
    "        if not isinstance(demand_met_col, str):\n",
    "            raise ValueError(\"demand_met_col must be a string\")\n",
    "        if not isinstance(co2_col, str):\n",
    "            raise ValueError(\"co2_col must be a string\")\n",
    "        if not isinstance(epsilon, (float, int)):\n",
    "            raise ValueError(\"epsilon must be a float or int\")\n",
    "\n",
    "        self.timestamp_col = timestamp_col\n",
    "        self.demand_met_col = demand_met_col\n",
    "        self.co2_col = co2_col\n",
    "        self.epsilon = float(epsilon)\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        if not isinstance(X, pd.DataFrame):\n",
    "            raise TypeError(\"Input must be a pandas DataFrame\")\n",
    "        for col in [self.timestamp_col, self.demand_met_col]:\n",
    "            if col not in X.columns:\n",
    "                raise ValueError(f\"Missing required column '{col}' in input DataFrame\")\n",
    "        self.n_features_in_ = X.shape[1]\n",
    "        self.is_fitted_ = True\n",
    "        return self\n",
    "\n",
    "    def transform(self, X: pd.DataFrame, y: pd.Series | None = None) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : pd.DataFrame\n",
    "            Must contain `timestamp_col` and `demand_met_col`.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        pd.DataFrame\n",
    "            Copy of X with additional feature columns.\n",
    "        \"\"\"\n",
    "        if not isinstance(X, pd.DataFrame):\n",
    "            raise TypeError(\"Input must be a pandas DataFrame\")\n",
    "\n",
    "        df = X.copy()\n",
    "\n",
    "        for col in [self.timestamp_col, self.demand_met_col]:\n",
    "            if col not in df.columns:\n",
    "                raise ValueError(f\"Missing required column '{col}'\")\n",
    "\n",
    "        df[self.timestamp_col] = pd.to_datetime(df[self.timestamp_col], errors=\"coerce\")\n",
    "        if df[self.timestamp_col].isna().any():\n",
    "            raise ValueError(f\"Column '{self.timestamp_col}' contains non-parseable datetimes\")\n",
    "\n",
    "        # temporal\n",
    "        df[\"time_id\"] = df[self.timestamp_col].dt.strftime(\"%H-%M\").astype(\"string\")\n",
    "\n",
    "        # quantitative\n",
    "        q = self.demand_met_col\n",
    "        df[f\"{q}_sqrd\"] = df[q] ** 2\n",
    "        df[f\"log_{q}\"] = np.log(df[q] + self.epsilon)\n",
    "        df[f\"log_{q}_sqrd\"] = df[f\"log_{q}\"] ** 2\n",
    "\n",
    "        if self.co2_col in df.columns:\n",
    "            df[f\"log_{self.co2_col}\"] = np.log(df[self.co2_col] + self.epsilon)\n",
    "\n",
    "        return df\n",
    "\n",
    "    def get_feature_names_out(self, input_features=None):\n",
    "        base = []\n",
    "        base.append(\"time_id\")\n",
    "        base += [\n",
    "            f\"{self.demand_met_col}_sqrd\",\n",
    "            f\"log_{self.demand_met_col}\",\n",
    "            f\"log_{self.demand_met_col}_sqrd\",\n",
    "        ]\n",
    "        # optional; only present if co2 is in input\n",
    "        base.append(f\"log_{self.co2_col}\")\n",
    "        if input_features is not None:\n",
    "            return np.array(list(input_features) + base)\n",
    "        return np.array(base)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "b0f2e0c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DateTimeFeatureAdder(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"\n",
    "    Add datetime-based features from a timestamp column.\n",
    "\n",
    "    New columns:\n",
    "      - year (int)\n",
    "      - month (int)\n",
    "      - week_of_year (ISO week, int)\n",
    "      - day (int)\n",
    "      - hour (int)\n",
    "      - half_hour (0..47, int)\n",
    "      - day_of_week (1=Mon..7=Sun, int)\n",
    "      - is_weekend (0/1, int)\n",
    "\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    timestamp_col : str, default=\"timestamp\"\n",
    "        Name of the column containing datetime strings or pd.Timestamp.\n",
    "    drop_original : bool, default=True\n",
    "        Whether to drop the original timestamp column after extraction.\n",
    "\n",
    "    Raises\n",
    "    ------\n",
    "    TypeError\n",
    "        If `timestamp_col` is not found in the DataFrame.\n",
    "    KeyError\n",
    "        If `timestamp_col` is not present in X.\n",
    "\\\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        timestamp_col: str = \"timestamp\",\n",
    "        drop_original: bool = False,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Initialize the feature adder.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        timestamp_col : str\n",
    "            Column name to parse as datetime.\n",
    "        \"\"\"\n",
    "        if not isinstance(timestamp_col, str):\n",
    "            raise TypeError(\"timestamp_col must be a string.\")\n",
    "        if not isinstance(drop_original, bool):\n",
    "            raise TypeError(\"drop_original must be a bool.\")\n",
    "        self.timestamp_col = timestamp_col\n",
    "        self.drop_original = drop_original\n",
    "\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        \"\"\"\n",
    "        No-op fit. Exists for sklearn compatibility.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        self : DateTimeFeatureAdder\n",
    "        \"\"\"\n",
    "        if not isinstance(X, pd.DataFrame):\n",
    "            raise TypeError(\"Input X must be a pandas DataFrame.\")\n",
    "        if self.timestamp_col not in X.columns:\n",
    "            raise KeyError(f\"Column '{self.timestamp_col}' not found in DataFrame.\")\n",
    "\n",
    "        self.n_features_in_ = X.shape[1]\n",
    "        self.is_fitted_ = True\n",
    "        return self\n",
    "\n",
    "    def transform(self, X: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Transform X by adding:\n",
    "\n",
    "        - year (int)\n",
    "        - month (int)\n",
    "        - week_of_year (int)\n",
    "        - day (int)\n",
    "        - hour (int)\n",
    "        - half_hour (int, 0-47)\n",
    "        - day_of_week (int, 1=Mon)\n",
    "        - is_weekend (0/1)\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : pd.DataFrame\n",
    "            Input DataFrame with a column named `self.timestamp_col`.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        X_out : pd.DataFrame\n",
    "            Copy of X with the above new columns appended.\n",
    "\n",
    "        Raises\n",
    "        ------\n",
    "        KeyError\n",
    "            If `self.timestamp_col` is not present in X.\n",
    "        \"\"\"\n",
    "        df = X.copy()\n",
    "        # Attempt to convert the timestamp column to datetime (if not already)\n",
    "        try:\n",
    "            df[self.timestamp_col] = pd.to_datetime(df[self.timestamp_col], errors='raise')\n",
    "        except Exception as e:\n",
    "            raise TypeError(f\"Column '{self.timestamp_col}' could not be converted to datetime: {e}\")\n",
    "\n",
    "        dt = df[self.timestamp_col]\n",
    "        df[\"year\"] = dt.dt.year.astype('int32')\n",
    "        df[\"month\"] = dt.dt.month.astype('int32')\n",
    "        df[\"week_of_year\"] = dt.dt.isocalendar().week.astype('int32')\n",
    "        df[\"day\"] = dt.dt.day.astype('int32')\n",
    "        df[\"hour\"] = dt.dt.hour.astype('int32')\n",
    "        df[\"half_hour\"]    = (dt.dt.hour * 2 + (dt.dt.minute // 30)).astype(\"int32\")\n",
    "        df[\"day_of_week\"] = (dt.dt.dayofweek).astype('int32') + 1  # Monday=1\n",
    "        df[\"is_weekend\"] = (df[\"day_of_week\"] >= 6).astype('int32')\n",
    "\n",
    "        if self.drop_original:\n",
    "            df = df.drop(columns=[self.timestamp_col])\n",
    "\n",
    "        return df\n",
    "\n",
    "    def get_feature_names_out(self, input_features=None):\n",
    "        \"\"\"\n",
    "        Get the names of the output features.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        input_features : array-like, optional\n",
    "            The input feature names. If None, the original feature names are used.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        np.ndarray\n",
    "            The output feature names.\n",
    "        \"\"\"\n",
    "        added = [\"year\",\"month\",\"week_of_year\",\"day\",\"hour\",\"half_hour\",\"day_of_week\",\"is_weekend\"]\n",
    "        if self.drop_original or input_features is None:\n",
    "            base = [] if input_features is None else [c for c in input_features if c != self.timestamp_col]\n",
    "        else:\n",
    "            base = list(input_features)\n",
    "        return np.array(base + added, dtype=object)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "ff18f49c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GenerationShareAdder(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"\n",
    "    Add percentage‐share features for specified generation columns relative to a total.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    generation_cols : List[str]\n",
    "        Columns whose shares of `total_col` are computed.\n",
    "    total_col : str, default=\"total_generation\"\n",
    "        Denominator column.\n",
    "    suffix : str, default=\"_share\"\n",
    "        Suffix appended to new share columns.\n",
    "    as_percent : bool, default=True\n",
    "        If True, multiply shares by 100; otherwise keep as 0..1 fraction.\n",
    "    clip_0_100 : bool, default=False\n",
    "        If True and `as_percent=True`, clip results into [0, 100].\n",
    "        If True and `as_percent=False`, clip into [0, 1].\n",
    "\n",
    "    Raises\n",
    "    ------\n",
    "    TypeError\n",
    "        Bad argument types.\n",
    "    KeyError\n",
    "        Missing `generation_cols` or `total_col`.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        generation_cols: List[str],\n",
    "        total_col: str = \"total_generation\",\n",
    "        suffix: str = \"_share\",\n",
    "        as_percent: bool = True,\n",
    "        clip_0_100: bool = False,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Initialize the share adder.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        generation_cols : List[str]\n",
    "            Columns to convert into percentage shares.\n",
    "        total_col : str\n",
    "            Column used as the denominator in share calculation.\n",
    "        suffix : str\n",
    "            Suffix for the new share columns.\n",
    "\n",
    "        Raises\n",
    "        ------\n",
    "        TypeError\n",
    "            If `generation_cols` is not a list of strings, or if `total_col` or `suffix` are not strings.\n",
    "        \"\"\"\n",
    "        if not isinstance(generation_cols, list) or not all(isinstance(col, str) for col in generation_cols):\n",
    "            raise TypeError(\"generation_cols must be a list of strings.\")\n",
    "        if not isinstance(total_col, str):\n",
    "            raise TypeError(\"total_col must be a string.\")\n",
    "        if not isinstance(suffix, str):\n",
    "            raise TypeError(\"suffix must be a string.\")\n",
    "        if not isinstance(as_percent, bool):\n",
    "            raise TypeError(\"as_percent must be a bool.\")\n",
    "        if not isinstance(clip_0_100, bool):\n",
    "            raise TypeError(\"clip_0_100 must be a bool.\")\n",
    "\n",
    "        self.generation_cols = generation_cols\n",
    "        self.total_col = total_col\n",
    "        self.suffix = suffix\n",
    "        self.as_percent = as_percent\n",
    "        self.clip_0_100 = clip_0_100\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        \"\"\"\n",
    "        No‐op fit for compatibility with sklearn’s transformer API.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : pd.DataFrame\n",
    "            Input DataFrame.\n",
    "        y : Ignored\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        self : GenerationShareAdder\n",
    "\n",
    "        Raises\n",
    "        ------\n",
    "        TypeError\n",
    "            If `X` is not a pandas DataFrame.\n",
    "        KeyError\n",
    "            If any of the specified `generation_cols` or `total_col` is not present in the DataFrame.\n",
    "        \"\"\"\n",
    "        if not isinstance(X, pd.DataFrame):\n",
    "            raise TypeError(\"Input X must be a pandas DataFrame.\")\n",
    "        missing_cols = [col for col in self.generation_cols if col not in X.columns]\n",
    "        if missing_cols:\n",
    "            raise KeyError(f\"Generation columns {missing_cols} not found in input DataFrame.\")\n",
    "        if self.total_col not in X.columns:\n",
    "            raise KeyError(f\"Total column '{self.total_col}' not found in input DataFrame.\")\n",
    "        return self\n",
    "\n",
    "\n",
    "    def transform(self, X: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Compute and append share columns.\n",
    "\n",
    "        For each `col` in `generation_cols`, creates a new column\n",
    "        `col + suffix` = 100 * (X[col] / X[total_col]). Zeros in `total_col`\n",
    "        are treated as NaN to avoid division‐by‐zero.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : pd.DataFrame\n",
    "            Input DataFrame containing `generation_cols` and `total_col`.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        X_out : pd.DataFrame\n",
    "            Copy of X with additional `<col><suffix>` columns.\n",
    "\n",
    "        \"\"\"\n",
    "        df = X.copy()\n",
    "        # avoid integer division & div-by-zero\n",
    "        total = df[self.total_col].astype(\"float64\").replace({0.0: np.nan})\n",
    "        scale = 100.0 if self.as_percent else 1.0\n",
    "\n",
    "        for col in self.generation_cols:\n",
    "            share_col = f\"{col}{self.suffix}\"\n",
    "            df[share_col] = (df[col].astype(\"float64\") / total) * scale\n",
    "            if self.clip_0_100:\n",
    "                lo, hi = (0.0, 100.0) if self.as_percent else (0.0, 1.0)\n",
    "                df[share_col] = df[share_col].clip(lower=lo, upper=hi)\n",
    "\n",
    "        return df\n",
    "\n",
    "\n",
    "    def get_feature_names_out(self, input_features=None):\n",
    "        added = [f\"{c}{self.suffix}\" for c in self.generation_cols]\n",
    "        base = [] if input_features is None else list(input_features)\n",
    "        return np.array(base + added, dtype=object)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96dc5119",
   "metadata": {},
   "source": [
    "#### Multi-Quantile Binner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "5a1a9696",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiQuantileBinner(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"\n",
    "    Quantile bin multiple variables, then combine their per-variable bin IDs into\n",
    "    a single mixed-radix group ID (1-based).\n",
    "\n",
    "    Example: with bin_specs={'v1':5, 'v2':4}:\n",
    "      - Fit stores quantile edges for each var.\n",
    "      - Transform assigns v1_group∈{1..5}, v2_group∈{1..4},\n",
    "        then builds group_col_name = 1 + (v1_group-1)*4 + (v2_group-1)*1.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        bin_specs: dict[str, int],\n",
    "        group_col_name: str = \"quantile_group_id\",\n",
    "        retain_flags: bool = True,\n",
    "        oob_policy: str = \"clip\",\n",
    "        max_oob_rate: float | None = None,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        bin_specs : dict[str, int]\n",
    "            Mapping of variable -> # of quantile bins (positive integers).\n",
    "        group_col_name : str, default \"quantile_group_id\"\n",
    "            Output column for the combined mixed-radix group ID (1-based).\n",
    "        retain_flags : bool, default True\n",
    "            If True, keep per-variable `<var>_group` columns.\n",
    "        oob_policy : {\"clip\",\"edge\",\"error\"}, default \"clip\"\n",
    "            Handling for values falling outside learned edges at transform time:\n",
    "              - \"clip\": send to nearest bin (1 or max)\n",
    "              - \"edge\": send to the first bin\n",
    "              - \"error\": raise ValueError\n",
    "        max_oob_rate : float or None, default None\n",
    "            If set, raise an error when an individual variable sees\n",
    "            OOB rate > max_oob_rate during transform.\n",
    "        \"\"\"\n",
    "        if not isinstance(bin_specs, dict) or not bin_specs:\n",
    "            raise ValueError(\"bin_specs must be a non-empty dict\")\n",
    "        if oob_policy not in {\"clip\", \"edge\", \"error\"}:\n",
    "            raise ValueError(\"oob_policy must be one of {'clip','edge','error'}\")\n",
    "\n",
    "        self.bin_specs = self.validate_and_convert_bins(bin_specs)\n",
    "        self.group_col_name = str(group_col_name)\n",
    "        self.retain_flags = bool(retain_flags)\n",
    "        self.oob_policy = oob_policy\n",
    "        self.max_oob_rate = max_oob_rate\n",
    "\n",
    "        self.variables_: list[str] | None = None\n",
    "        self.quantile_edges_: dict[str, list[float]] = {}\n",
    "        self.bin_sizes_: dict[str, int] = {}\n",
    "        self.multipliers_: list[int] | None = None\n",
    "        self.oob_counts_: dict[str, int] = {}\n",
    "\n",
    "    def fit(self, X: pd.DataFrame, y=None):\n",
    "        \"\"\"\n",
    "        Learn quantile edges for each variable.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : pd.DataFrame\n",
    "            Must contain all variables in `bin_specs`.\n",
    "        \"\"\"\n",
    "        self.variables_ = list(self.bin_specs.keys())\n",
    "        self.quantile_edges_.clear()\n",
    "        self.bin_sizes_.clear()\n",
    "        self.oob_counts_.clear()\n",
    "\n",
    "        eps = 1e-4\n",
    "        for var in self.variables_:\n",
    "            n_bins = self.bin_specs[var]\n",
    "            if var not in X.columns:\n",
    "                raise ValueError(f\"Column '{var}' not found in X\")\n",
    "            qs = np.linspace(0, 1, n_bins + 1)\n",
    "            raw = X[var].quantile(qs, interpolation=\"midpoint\").values\n",
    "            vmin, vmax = X[var].min(), X[var].max()\n",
    "            edges = np.unique(np.concatenate([[vmin - eps], raw, [vmax + eps]]))\n",
    "            edges.sort()\n",
    "            self.quantile_edges_[var] = edges.tolist()\n",
    "            self.bin_sizes_[var] = len(edges) - 1\n",
    "\n",
    "        bases = [self.bin_sizes_[v] for v in self.variables_]\n",
    "        m = [1]\n",
    "        for b in reversed(bases[1:]):\n",
    "            m.insert(0, m[0] * b)\n",
    "        self.multipliers_ = m\n",
    "        return self\n",
    "\n",
    "    def transform(self, X: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Assign per-variable quantile bins and the combined group ID.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        pd.DataFrame\n",
    "            X plus `<var>_group` (optional) and `group_col_name`.\n",
    "        \"\"\"\n",
    "        if not self.quantile_edges_:\n",
    "            raise RuntimeError(\"Must fit binner before transform()\")\n",
    "        df = X.copy()\n",
    "        self.oob_counts_ = {var: 0 for var in self.variables_}\n",
    "\n",
    "        for var in self.variables_:\n",
    "            edges = self.quantile_edges_[var]\n",
    "            n = len(edges) - 1\n",
    "            s = pd.cut(df[var], bins=edges, labels=range(1, n + 1), include_lowest=True, right=True)\n",
    "\n",
    "            if s.isna().any():\n",
    "                n_oob = int(s.isna().sum())\n",
    "                self.oob_counts_[var] += n_oob\n",
    "                if self.oob_policy == \"error\":\n",
    "                    bad = df.loc[s.isna(), var].unique()\n",
    "                    raise ValueError(f\"OOB values for '{var}': {bad[:10]} ...\")\n",
    "                elif self.oob_policy == \"clip\":\n",
    "                    below = df[var] < edges[1]\n",
    "                    s = s.astype(\"Float64\")\n",
    "                    s.loc[s.isna() & below] = 1\n",
    "                    s.loc[s.isna() & ~below] = n\n",
    "                    s = s.astype(\"Int64\")\n",
    "                else:  # \"edge\"\n",
    "                    s = s.fillna(1)\n",
    "\n",
    "            df[f\"{var}_group\"] = s.astype(int)\n",
    "\n",
    "        total = len(df)\n",
    "        if self.max_oob_rate is not None and total > 0:\n",
    "            for var, cnt in self.oob_counts_.items():\n",
    "                rate = cnt / total\n",
    "                if rate > self.max_oob_rate:\n",
    "                    raise ValueError(\n",
    "                        f\"OOB rate {rate:.2%} exceeds max_oob_rate={self.max_oob_rate:.2%} for '{var}'\"\n",
    "                    )\n",
    "\n",
    "        df[self.group_col_name] = 1\n",
    "        for v, m in zip(self.variables_, self.multipliers_):\n",
    "            df[self.group_col_name] += (df[f\"{v}_group\"] - 1) * m\n",
    "\n",
    "        if not self.retain_flags:\n",
    "            df.drop(columns=[f\"{v}_group\" for v in self.variables_], inplace=True)\n",
    "\n",
    "        return df\n",
    "\n",
    "    @staticmethod\n",
    "    def validate_and_convert_bins(bin_specs: dict) -> dict[str, int]:\n",
    "        converted: dict[str, int] = {}\n",
    "        for k, v in bin_specs.items():\n",
    "            try:\n",
    "                v_int = int(float(v))\n",
    "                if v_int != float(v) or v_int <= 0:\n",
    "                    raise ValueError\n",
    "                converted[str(k)] = v_int\n",
    "            except (ValueError, TypeError) as e:\n",
    "                raise TypeError(f\"Bin spec '{k}' value '{v}' must be a positive integer\") from e\n",
    "        return converted\n",
    "\n",
    "    def get_feature_names_out(self, input_features=None):\n",
    "        names = []\n",
    "        if self.retain_flags and self.variables_:\n",
    "            names += [f\"{v}_group\" for v in self.variables_]\n",
    "        names.append(self.group_col_name)\n",
    "        if input_features is not None:\n",
    "            return np.array(list(input_features) + names)\n",
    "        return np.array(names)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64560abc",
   "metadata": {},
   "source": [
    "#### Multi-Median Binner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "f5f35608",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiMedianBinner(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"\n",
    "    Median-split each variable and combine flags into a 1-based group ID.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, variables: list[str], group_col_name: str = \"median_group_id\", retain_flags: bool = True):\n",
    "        if not isinstance(variables, list) or len(variables) == 0:\n",
    "            raise ValueError(\"`variables` must be a non-empty list of column names.\")\n",
    "        if any(not isinstance(v, str) for v in variables):\n",
    "            raise TypeError(\"All entries in `variables` must be strings.\")\n",
    "        if not isinstance(group_col_name, str) or not group_col_name:\n",
    "            raise TypeError(\"`group_col_name` must be a non-empty string.\")\n",
    "        if not isinstance(retain_flags, bool):\n",
    "            raise TypeError(\"`retain_flags` must be a boolean value.\")\n",
    "\n",
    "        self.variables = variables\n",
    "        self.group_col_name = group_col_name\n",
    "        self.retain_flags = retain_flags\n",
    "        self.medians_: dict[str, float] = {}\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        if not isinstance(X, pd.DataFrame):\n",
    "            raise TypeError(\"Input X must be a pandas DataFrame.\")\n",
    "        missing = [v for v in self.variables if v not in X.columns]\n",
    "        if missing:\n",
    "            raise ValueError(f\"Columns not found in input DataFrame: {missing}\")\n",
    "        self.medians_ = X[self.variables].median(skipna=True).to_dict()\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        \"\"\"\n",
    "        Returns\n",
    "        -------\n",
    "        pd.DataFrame\n",
    "            Copy of X with optional `<var>_group` flags (0/1) and `group_col_name`.\n",
    "        \"\"\"\n",
    "        if not self.medians_:\n",
    "            raise RuntimeError(\"Must call fit() before transform().\")\n",
    "        if not isinstance(X, pd.DataFrame):\n",
    "            raise TypeError(\"Input X must be a pandas DataFrame.\")\n",
    "        missing = [v for v in self.variables if v not in X.columns]\n",
    "        if missing:\n",
    "            raise ValueError(f\"Columns missing at transform time: {missing}\")\n",
    "\n",
    "        df = X.copy()\n",
    "        # compare each column to its scalar median (aligned by column name)\n",
    "        flags = (df[self.variables] > pd.Series(self.medians_)).astype(int)\n",
    "\n",
    "        multipliers = 2 ** np.arange(len(self.variables))[::-1]\n",
    "        df[self.group_col_name] = flags.values.dot(multipliers) + 1\n",
    "\n",
    "        if self.retain_flags:\n",
    "            for var in self.variables:\n",
    "                df[f\"{var}_group\"] = flags[var]\n",
    "\n",
    "        return df\n",
    "\n",
    "    def get_feature_names_out(self, input_features=None):\n",
    "        names = []\n",
    "        if self.retain_flags:\n",
    "            names += [f\"{v}_group\" for v in self.variables]\n",
    "        names.append(self.group_col_name)\n",
    "        if input_features is not None:\n",
    "            return np.array(list(input_features) + names)\n",
    "        return np.array(names)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a48a6341",
   "metadata": {},
   "source": [
    "#### GroupwiseRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "4bf4c192",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GroupwiseRegressor(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"\n",
    "    Runs separate OLS regressions in each group and computes marginal emission factors.\n",
    "\n",
    "    For each group k, we fit:\n",
    "        y_t = α₁ₖ · x₁_t + α₂ₖ · x₂_t + Σ β_i·C(f_i)_t + ε_t\n",
    "    and compute the marginal effect:\n",
    "        ME_t = ∂y_t/∂x₁_t = α₁ₖ + 2·α₂ₖ·x₁_t.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    y_var : str\n",
    "        Target column name (e.g. 'tons_co2').\n",
    "    x_vars : List[str]\n",
    "        Predictor columns; first is Q, second is Q².\n",
    "    fe_vars : List[str], optional\n",
    "        Categorical fixed-effect columns.\n",
    "    group_col : str\n",
    "        Column with integer group IDs.\n",
    "    min_group_size : int\n",
    "        Minimum observations per group to run regression.\n",
    "    track_metrics : bool\n",
    "        If True, store per-group models and metrics.\n",
    "    verbose : bool\n",
    "        If True, log progress and metrics.\n",
    "\n",
    "    Attributes\n",
    "    ----------\n",
    "    group_models_ : dict\n",
    "        Fitted statsmodels results per group (if track_metrics=True).\n",
    "    group_metrics_ : dict\n",
    "        Computed metrics per group (if track_metrics=True).\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        y_var: str = \"tons_co2\",\n",
    "        x_vars: List[str] = [\"total_generation\", \"total_generation_sqrd\"],\n",
    "        fe_vars: Optional[List[str]] = None,\n",
    "        group_col: str = \"k\",\n",
    "        min_group_size: int = 10,\n",
    "        track_metrics: bool = True,\n",
    "        verbose: bool = True,\n",
    "        random_state: int | None = 12,\n",
    "    ):\n",
    "        if not isinstance(y_var, str):\n",
    "            raise TypeError(\"y_var must be a string\")\n",
    "        if not isinstance(x_vars, list) or not x_vars or not all(isinstance(v, str) for v in x_vars):\n",
    "            raise TypeError(\"x_vars must be a non-empty list of strings\")\n",
    "        if fe_vars is not None and (not isinstance(fe_vars, list) or not all(isinstance(v, str) for v in fe_vars)):\n",
    "            raise TypeError(\"fe_vars must be a list of strings or None\")\n",
    "        if not isinstance(group_col, str):\n",
    "            raise TypeError(\"group_col must be a string\")\n",
    "        if not isinstance(min_group_size, int) or min_group_size < 1:\n",
    "            raise ValueError(\"min_group_size must be a positive integer\")\n",
    "        if not isinstance(track_metrics, bool):\n",
    "            raise TypeError(\"track_metrics must be a boolean\")\n",
    "        if not isinstance(verbose, bool):\n",
    "            raise TypeError(\"verbose must be a boolean\")\n",
    "\n",
    "        self.y_var = y_var\n",
    "        self.x_vars = x_vars\n",
    "        self.fe_vars = fe_vars or []\n",
    "        self.group_col = group_col\n",
    "        self.min_group_size = min_group_size\n",
    "        self.track_metrics = track_metrics\n",
    "        self.verbose = verbose\n",
    "        self.random_state = random_state\n",
    "        if self.track_metrics:\n",
    "            self.group_models_: dict[Any, Any] = {}\n",
    "            self.group_metrics_: dict[Any, dict[str, float]] = {}\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        if self.random_state is not None:\n",
    "            np.random.seed(self.random_state)\n",
    "        if not isinstance(X, pd.DataFrame):\n",
    "            raise TypeError(\"X must be a pandas DataFrame\")\n",
    "        if y is None:\n",
    "            raise ValueError(\"y must be provided for fitting\")\n",
    "        if len(X) != len(y):\n",
    "            raise ValueError(f\"X and y have different lengths: {len(X)} != {len(y)}\")\n",
    "\n",
    "        df = X.copy()\n",
    "        df[self.y_var] = np.asarray(y).reshape(-1)\n",
    "\n",
    "        # avoid uint in formula design matrix\n",
    "        uint_cols = [c for c in df.columns if str(df[c].dtype).startswith((\"uint\", \"UInt\"))]\n",
    "        if uint_cols:\n",
    "            df[uint_cols] = df[uint_cols].astype(\"int64\")\n",
    "\n",
    "        if self.track_metrics:\n",
    "            self.group_models_.clear()\n",
    "            self.group_metrics_.clear()\n",
    "\n",
    "        # cast FEs to ordered categoricals\n",
    "        if \"month\" in self.fe_vars:\n",
    "            df[\"month\"] = pd.Categorical(df[\"month\"].astype(int), categories=range(1, 13), ordered=True)\n",
    "        if \"hour\" in self.fe_vars:\n",
    "            df[\"hour\"] = pd.Categorical(df[\"hour\"].astype(int), categories=range(24), ordered=True)\n",
    "        if \"day_of_week\" in self.fe_vars:\n",
    "            df[\"day_of_week\"] = pd.Categorical(df[\"day_of_week\"].astype(int), categories=range(1, 8), ordered=True)\n",
    "        if \"week_of_year\" in self.fe_vars:\n",
    "            df[\"week_of_year\"] = pd.Categorical(df[\"week_of_year\"].astype(int), categories=range(1, 54), ordered=True)\n",
    "        if \"half_hour\" in self.fe_vars:\n",
    "            df[\"half_hour\"] = pd.Categorical(df[\"half_hour\"].astype(int), categories=range(0, 48), ordered=True)\n",
    "\n",
    "        self._fitted_groups: list[Any] = []\n",
    "\n",
    "        for grp, df_grp in df.groupby(self.group_col, sort=True):\n",
    "            n = len(df_grp)\n",
    "            if n < self.min_group_size:\n",
    "                if self.verbose:\n",
    "                    logging.warning(f\"Skipping group {grp!r}: only {n} < {self.min_group_size}\")\n",
    "                continue\n",
    "\n",
    "            reg = \" + \".join(self.x_vars)\n",
    "            fe = \" + \".join(f\"C({f})\" for f in self.fe_vars)\n",
    "            formula = f\"{self.y_var} ~ {reg}\" + (f\" + {fe}\" if fe else \"\")\n",
    "\n",
    "            model = smf.ols(formula, data=df_grp).fit()\n",
    "            self._fitted_groups.append(grp)\n",
    "\n",
    "            if self.track_metrics:\n",
    "                preds = model.predict(df_grp)\n",
    "                rmse = float(np.sqrt(np.mean((preds - df_grp[self.y_var]) ** 2)))\n",
    "                mae = float(np.mean(np.abs(preds - df_grp[self.y_var])))\n",
    "                mape = float(mean_absolute_percentage_error(df_grp[self.y_var], preds))\n",
    "                self.group_models_[grp] = model\n",
    "                self.group_metrics_[grp] = {\n",
    "                    \"r2\": float(model.rsquared),\n",
    "                    \"rmse\": rmse,\n",
    "                    \"mae\": mae,\n",
    "                    \"mape\": mape,\n",
    "                    \"n_obs\": int(n),\n",
    "                }\n",
    "\n",
    "        if not self._fitted_groups:\n",
    "            raise ValueError(\"No valid groups found for fitting.\")\n",
    "        return self\n",
    "\n",
    "    def transform(self, X: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Apply groupwise OLS and compute marginal effects ME_t.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : pd.DataFrame\n",
    "            Must contain y_var, x_vars, fe_vars, and group_col.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        pd.DataFrame\n",
    "            Original rows plus 'alpha1', 'alpha2', and 'ME'.\n",
    "\n",
    "        Raises\n",
    "        ------\n",
    "        TypeError\n",
    "            If X is not a pandas DataFrame.\n",
    "        ValueError\n",
    "            If required columns missing or no group qualifies.\n",
    "        \"\"\"\n",
    "        if not getattr(self, \"group_models_\", None):\n",
    "            raise RuntimeError(\"GroupwiseRegressor must be fit before transform/predict.\")\n",
    "        if not isinstance(X, pd.DataFrame):\n",
    "            raise TypeError(\"Input X must be a pandas DataFrame\")\n",
    "\n",
    "        df = X.copy()\n",
    "\n",
    "        # keep FE category casting consistent with fit\n",
    "        if \"month\" in self.fe_vars:\n",
    "            df[\"month\"] = pd.Categorical(df[\"month\"].astype(int), categories=range(1, 13), ordered=True)\n",
    "        if \"hour\" in self.fe_vars:\n",
    "            df[\"hour\"] = pd.Categorical(df[\"hour\"].astype(int), categories=range(24), ordered=True)\n",
    "        if \"day_of_week\" in self.fe_vars:\n",
    "            df[\"day_of_week\"] = pd.Categorical(df[\"day_of_week\"].astype(int), categories=range(1, 8), ordered=True)\n",
    "        if \"week_of_year\" in self.fe_vars:\n",
    "            df[\"week_of_year\"] = pd.Categorical(df[\"week_of_year\"].astype(int), categories=range(1, 54), ordered=True)\n",
    "        if \"half_hour\" in self.fe_vars:\n",
    "            df[\"half_hour\"] = pd.Categorical(df[\"half_hour\"].astype(int), categories=range(0, 48), ordered=True)\n",
    "\n",
    "        df[\"alpha1\"] = np.nan\n",
    "        df[\"alpha2\"] = np.nan\n",
    "        df[\"ME\"] = np.nan\n",
    "\n",
    "        for grp, df_grp in df.groupby(self.group_col, sort=True):\n",
    "            model = self.group_models_.get(grp)\n",
    "            if model is None:\n",
    "                continue\n",
    "            a1 = model.params.get(self.x_vars[0], np.nan)\n",
    "            a2 = model.params.get(self.x_vars[1], 0.0)\n",
    "            idx = df_grp.index\n",
    "\n",
    "            df.loc[idx, \"alpha1\"] = a1\n",
    "            df.loc[idx, \"alpha2\"] = a2\n",
    "            df.loc[idx, \"ME\"] = a1 + 2.0 * a2 * df_grp[self.x_vars[0]]\n",
    "\n",
    "        return df\n",
    "\n",
    "\n",
    "    def predict(self, X: pd.DataFrame, predict_type: str = \"ME\") -> pd.Series:\n",
    "        \"\"\"\n",
    "        Predict marginal effects (default) or CO2 for each row in X using fitted group models.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : pd.DataFrame\n",
    "            Must contain x_vars, fe_vars, and group_col.\n",
    "        predict_type : {\"ME\",\"y\"}, default \"ME\"\n",
    "            \"ME\": return α1 + 2*α2*Q\n",
    "            \"y\" : return model.predict(...) (CO2)\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        pd.Series\n",
    "            Predictions aligned to X.index.\n",
    "        \"\"\"\n",
    "\n",
    "        if not getattr(self, \"group_models_\", None):\n",
    "            raise RuntimeError(\"GroupwiseRegressor must be fit before predict().\")\n",
    "        if not isinstance(X, pd.DataFrame):\n",
    "            raise TypeError(\"X must be a pandas DataFrame\")\n",
    "\n",
    "        required = self.x_vars + self.fe_vars + [self.group_col]\n",
    "        missing = [c for c in required if c not in X.columns]\n",
    "        if missing:\n",
    "            raise ValueError(f\"Missing columns in input DataFrame: {missing}\")\n",
    "\n",
    "        df = X.copy()\n",
    "        # consistent FE casting\n",
    "        if \"month\" in self.fe_vars:\n",
    "            df[\"month\"] = pd.Categorical(df[\"month\"].astype(int), categories=range(1, 13), ordered=True)\n",
    "        if \"hour\" in self.fe_vars:\n",
    "            df[\"hour\"] = pd.Categorical(df[\"hour\"].astype(int), categories=range(24), ordered=True)\n",
    "        if \"day_of_week\" in self.fe_vars:\n",
    "            df[\"day_of_week\"] = pd.Categorical(df[\"day_of_week\"].astype(int), categories=range(1, 8), ordered=True)\n",
    "        if \"week_of_year\" in self.fe_vars:\n",
    "            df[\"week_of_year\"] = pd.Categorical(df[\"week_of_year\"].astype(int), categories=range(1, 54), ordered=True)\n",
    "        if \"half_hour\" in self.fe_vars:\n",
    "            df[\"half_hour\"] = pd.Categorical(df[\"half_hour\"].astype(int), categories=range(0, 48), ordered=True)\n",
    "\n",
    "        out = pd.Series(index=df.index, dtype=float)\n",
    "\n",
    "        for grp, df_grp in df.groupby(self.group_col, sort=True):\n",
    "            model = self.group_models_.get(grp)\n",
    "            if model is None:\n",
    "                continue\n",
    "\n",
    "            if predict_type == \"y\":\n",
    "                preds = model.predict(df_grp)\n",
    "            else:\n",
    "                a1 = model.params.get(self.x_vars[0], np.nan)\n",
    "                a2 = model.params.get(self.x_vars[1], 0.0)\n",
    "                Q = df_grp[self.x_vars[0]]\n",
    "                preds = a1 + 2.0 * a2 * Q\n",
    "\n",
    "            out.loc[df_grp.index] = preds\n",
    "\n",
    "        return out\n",
    "\n",
    "    def get_metrics(self, summarise: bool = True) -> Union[dict, pd.DataFrame]:\n",
    "        \"\"\"\n",
    "        Get the metrics for each group.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        summarise : bool, default=True\n",
    "            If True, return a summary DataFrame; otherwise return raw metrics dict.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        dict or pd.DataFrame\n",
    "            If summarise=True, returns a DataFrame with group metrics.\n",
    "            If False, returns the raw metrics dictionary.\n",
    "\n",
    "        Raises\n",
    "        ------\n",
    "        RuntimeError\n",
    "            If track_metrics was not set to True during initialization.\n",
    "        \"\"\"\n",
    "        if not self.track_metrics:\n",
    "            raise RuntimeError(\"Metrics tracking is disabled. Set track_metrics=True to enable.\")\n",
    "        if summarise:\n",
    "            df = pd.DataFrame.from_dict(self.group_metrics_, orient=\"index\")\n",
    "            df.index.name = self.group_col\n",
    "            df.reset_index(inplace=True)\n",
    "            return df\n",
    "        return self.group_metrics_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea0e710f",
   "metadata": {},
   "source": [
    "### Running Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3eabbda",
   "metadata": {},
   "source": [
    "#### Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "a511a2d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _apply_fitted_preprocessing(user_pipeline: Pipeline, X: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Apply all *already-fitted* steps in a pipeline except the final estimator,\n",
    "    without constructing a new sklearn Pipeline (avoids 'Pipeline not fitted' warnings).\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    user_pipeline : Pipeline\n",
    "        A pipeline that has already been fitted (on train) and whose final step\n",
    "        is the estimator (e.g., GroupwiseRegressor).\n",
    "    X : pd.DataFrame\n",
    "        Raw features to transform through the fitted preprocessing steps.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pd.DataFrame\n",
    "        The transformed features as a DataFrame. If a transformer returns a numpy array,\n",
    "        we try to retrieve column names via `get_feature_names_out()`; otherwise we fall\n",
    "        back to the original column names.\n",
    "    \"\"\"\n",
    "    Z = X\n",
    "    last_transformer = None\n",
    "\n",
    "    for _, step in user_pipeline.steps[:-1]:\n",
    "        if hasattr(step, \"transform\"):\n",
    "            Z = step.transform(Z)\n",
    "            last_transformer = step\n",
    "\n",
    "    if isinstance(Z, pd.DataFrame):\n",
    "        return Z\n",
    "\n",
    "    # Try to recover column names\n",
    "    cols = None\n",
    "    try:\n",
    "        cols = user_pipeline[:-1].get_feature_names_out()  # type: ignore[index]\n",
    "    except Exception:\n",
    "        try:\n",
    "            if last_transformer is not None and hasattr(last_transformer, \"get_feature_names_out\"):\n",
    "                cols = last_transformer.get_feature_names_out()  # type: ignore[assignment]\n",
    "        except Exception:\n",
    "            cols = None\n",
    "\n",
    "    if cols is None:\n",
    "        cols = X.columns\n",
    "    return pd.DataFrame(Z, index=X.index, columns=list(cols))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "7fa207f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_me_for_split(\n",
    "    fitted_pipeline: Pipeline,\n",
    "    X: pd.DataFrame,\n",
    "    split_name: str | None = None,\n",
    "    id_cols: list[str] = (\"timestamp\", \"city\"),\n",
    "    include_params: bool = True,\n",
    "    keep_cols: list[str] = (\"demand_met\", \"tons_co2\"),\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Use a FITTED pipeline to compute marginal emissions (ME) for a single features DataFrame.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    fitted_pipeline : Pipeline\n",
    "        A pipeline that has already been fit on the training data. Its final step must be\n",
    "        GroupwiseRegressor, whose transform adds 'ME' (and 'alpha1','alpha2').\n",
    "    X : pd.DataFrame\n",
    "        Feature table to transform. Must include the columns required by the pipeline’s\n",
    "        feature steps and binner (e.g., weather vars), plus any IDs you want to keep.\n",
    "    split_name : str, optional\n",
    "        If provided, a 'split' column is added with this value ('train'/'validation'/'test'/etc).\n",
    "    id_cols : list[str], default ('timestamp','city')\n",
    "        Identifier columns to carry into the output if present in `X` after transform.\n",
    "    include_params : bool, default True\n",
    "        If True, also include 'alpha1' and 'alpha2' in the output.\n",
    "    keep_cols : list[str], default ('demand_met','tons_co2')\n",
    "        Additional columns to include if present (useful for diagnostics).\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pd.DataFrame\n",
    "        One row per input row with at least: id_cols ∩ columns, 'ME', and optionally\n",
    "        'alpha1','alpha2', the regressor’s group column, keep_cols, and 'split'.\n",
    "    \"\"\"\n",
    "    # Transform through all steps → last step (GroupwiseRegressor) computes ME\n",
    "    out = fitted_pipeline.transform(X)\n",
    "\n",
    "    # Final estimator for group column name\n",
    "    reg = getattr(fitted_pipeline, \"_final_estimator\", None)\n",
    "    gcol = getattr(reg, \"group_col\", None)\n",
    "\n",
    "    # Build column list in a safe, present-only way\n",
    "    cols: list[str] = [c for c in id_cols if c in out.columns]\n",
    "    if \"ME\" not in out.columns:\n",
    "        raise RuntimeError(\"Pipeline transform did not produce 'ME'. Was the final estimator fitted?\")\n",
    "    cols.append(\"ME\")\n",
    "\n",
    "    if include_params:\n",
    "        for c in (\"alpha1\", \"alpha2\"):\n",
    "            if c in out.columns:\n",
    "                cols.append(c)\n",
    "\n",
    "    if gcol and gcol in out.columns:\n",
    "        cols.append(gcol)\n",
    "\n",
    "    for c in keep_cols:\n",
    "        if c in out.columns and c not in cols:\n",
    "            cols.append(c)\n",
    "\n",
    "    result = out[cols].copy()\n",
    "    if split_name is not None:\n",
    "        result[\"split\"] = split_name\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "3d9c6a96",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_on_split(\n",
    "        regression_model: GroupwiseRegressor,\n",
    "        full_df: pd.DataFrame\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    After pipeline.transform → full_df with group IDs & original y_var,\n",
    "    compute per‑group r2/rmse/mae/n_obs using reg.group_models_.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    reg : GroupwiseRegressor\n",
    "        Fitted GroupwiseRegressor instance with group_models_ populated.\n",
    "    full_df : pd.DataFrame\n",
    "        DataFrame containing the original y_var and group_col.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pd.DataFrame\n",
    "        DataFrame with group metrics: r2, rmse, mae, n_obs.\n",
    "    \"\"\"\n",
    "    df = full_df.copy()\n",
    "    gcol = regression_model.group_col\n",
    "    yname = regression_model.y_var\n",
    "\n",
    "    if gcol not in df.columns or yname not in df.columns:\n",
    "        missing = [c for c in (gcol, yname) if c not in df.columns]\n",
    "        raise KeyError(f\"Required columns missing: {missing}\")\n",
    "\n",
    "    # Use the regressor's predict to ensure FE category handling is consistent\n",
    "    y_true = df[yname]\n",
    "    y_pred = regression_model.predict(df, predict_type=\"y\")\n",
    "\n",
    "    rows = []\n",
    "    for grp, idx in df.groupby(gcol).groups.items():\n",
    "        yt = y_true.loc[idx]\n",
    "        yp = y_pred.loc[idx].dropna()\n",
    "        # align just in case\n",
    "        yt = yt.loc[yp.index]\n",
    "        if len(yt) == 0:\n",
    "            continue\n",
    "        rows.append({\n",
    "            \"group\": grp,\n",
    "            \"r2\": r2_score(yt, yp),\n",
    "            \"rmse\": root_mean_squared_error(yt, yp),\n",
    "            \"mae\": mean_absolute_error(yt, yp),\n",
    "            \"mape\": mean_absolute_percentage_error(yt, yp),\n",
    "            \"n_obs\": int(len(yt)),\n",
    "        })\n",
    "\n",
    "    mdf = pd.DataFrame(rows)\n",
    "    if mdf.empty:\n",
    "        # return empty with expected columns\n",
    "        mdf = pd.DataFrame(columns=[\"group\",\"r2\",\"rmse\",\"mae\",\"mape\",\"n_obs\"])\n",
    "    return mdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "715edc52",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_and_export_marginal_emissions(\n",
    "    pipeline: Pipeline,\n",
    "    x_splits: dict,\n",
    "    y_splits: dict,\n",
    "    out_parquet_path: str,\n",
    "    *,\n",
    "    id_cols: list[str] = (\"timestamp\", \"city\"),\n",
    "    include_params: bool = True,\n",
    "    keep_cols: list[str] = (\"demand_met\", \"tons_co2\"),\n",
    "    order_splits: list[str] = (\"train\", \"validation\", \"test\"),\n",
    "    save_mode: str = \"single\",              # \"single\" | \"per_split\"\n",
    "    compression: str | None = \"snappy\",     # passed to pandas.to_parquet\n",
    "    return_df: bool = True,                 # set False on huge runs\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Fit the pipeline on the train split, compute marginal emissions (ME) for each split,\n",
    "    concatenate, and save to a single Parquet file.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    pipeline : Pipeline\n",
    "        Your full pipeline: [FeatureAddition → Binner → GroupwiseRegressor].\n",
    "    x_splits : dict\n",
    "        Feature splits, e.g. {\"train\": X_train, \"validation\": X_val, \"test\": X_test}.\n",
    "    y_splits : dict\n",
    "        Target splits, e.g. {\"train\": y_train, \"validation\": y_val, \"test\": y_test}.\n",
    "        Only the train target is used for fitting; others are not needed for transform.\n",
    "    out_parquet_path : str\n",
    "        File path for the output Parquet dataset.\n",
    "    id_cols : list[str], default ('timestamp','city')\n",
    "        Identifier columns to include if present.\n",
    "    include_params : bool, default True\n",
    "        Include 'alpha1' and 'alpha2' in the export.\n",
    "    keep_cols : list[str], default ('demand_met','tons_co2')\n",
    "        Additional useful columns to include if present.\n",
    "    order_splits : list[str], default ('train','validation','test')\n",
    "        Order in which to compute and stack splits.\n",
    "    save_mode : {\"single\",\"per_split\"}, default \"single\"\n",
    "        Use \"per_split\" on HPC/MPI (let rank 0 write or give each rank a different path).\n",
    "    compression : str or None, default \"snappy\"\n",
    "        Parquet compression codec (requires pyarrow/fastparquet support).\n",
    "    return_df : bool, default True\n",
    "        If False, skip building the concatenated DataFrame in memory.\n",
    "\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pd.DataFrame or None\n",
    "        Concatenated results if return_df=True and save_mode=\"single\"; otherwise None.\n",
    "\n",
    "\n",
    "    Notes\n",
    "    -----\n",
    "    - Binner quantile edges and groupwise OLS coefficients are learned on TRAIN only.\n",
    "    - Validation and test are transformed using those learned edges/coefficients.\n",
    "    - In MPI jobs, prefer save_mode=\"per_split\" or call this only on rank 0.\n",
    "    - Binner edges and group OLS coefs are learned on train only.\n",
    "    \"\"\"\n",
    "    out_parquet_path = Path(out_parquet_path)\n",
    "\n",
    "    # Fit on train\n",
    "    X_tr = x_splits[\"train\"]\n",
    "    y_tr = y_splits[\"train\"]\n",
    "    _ = pipeline.fit_transform(X_tr, y_tr)\n",
    "\n",
    "    if save_mode not in {\"single\", \"per_split\"}:\n",
    "        raise ValueError(\"save_mode must be 'single' or 'per_split'.\")\n",
    "\n",
    "    # Compute ME for each requested split\n",
    "    parts: list[pd.DataFrame] = []\n",
    "    for split in order_splits:\n",
    "        if split not in x_splits:\n",
    "            continue\n",
    "        df_me = compute_me_for_split(\n",
    "            fitted_pipeline=pipeline,\n",
    "            X=x_splits[split],\n",
    "            split_name=split,\n",
    "            id_cols=id_cols,\n",
    "            include_params=include_params,\n",
    "            keep_cols=keep_cols,\n",
    "        )\n",
    "\n",
    "        if save_mode == \"per_split\":\n",
    "            split_path = out_parquet_path.with_name(\n",
    "                f\"{out_parquet_path.stem}__{split}{out_parquet_path.suffix or '.parquet'}\"\n",
    "            )\n",
    "            split_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "            df_me.to_parquet(split_path, index=False, compression=compression)\n",
    "            # optionally avoid keeping in memory on huge runs\n",
    "            if return_df:\n",
    "                parts.append(df_me)\n",
    "        else:\n",
    "            parts.append(df_me)\n",
    "\n",
    "    if save_mode == \"single\":\n",
    "        final = pd.concat(parts, ignore_index=True) if parts else pd.DataFrame()\n",
    "        out_parquet_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "        final.to_parquet(out_parquet_path, index=False, compression=compression)\n",
    "        print(f\"[SAVE] Wrote marginal emissions to {out_parquet_path} (rows={len(final):,})\")\n",
    "        return final if return_df else None\n",
    "    else:\n",
    "        print(f\"[SAVE] Wrote per-split Parquet files next to {out_parquet_path}\")\n",
    "        if return_df:\n",
    "            return pd.concat(parts, ignore_index=True) if parts else pd.DataFrame()\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f70dc699",
   "metadata": {},
   "source": [
    "#### Runners & Orchestrators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "41cecb8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_regressor_model(\n",
    "    user_pipeline: Pipeline,\n",
    "    x_df: pd.DataFrame,\n",
    "    y_df: pd.Series | pd.DataFrame,\n",
    "    split_name: str,\n",
    "    extra_info: dict | None = None,\n",
    "    return_model: bool = False,\n",
    "    random_state: int = 12,\n",
    "    interval_hours: float = 0.5,\n",
    "    *,\n",
    " model_id_hash: str | None = None,\n",
    "    params_json_str: str | None = None,\n",
    ") -> tuple[pd.DataFrame, list[str], GroupwiseRegressor | dict]:\n",
    "    \"\"\"\n",
    "    Run a pipeline on one split, compute per-group metrics, attach energy weights,\n",
    "    and compute diagnostics (pooled CO₂ fit + finite-difference ME checks).\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    user_pipeline : Pipeline\n",
    "        Full pipeline [FeatureAddition → (Binner) → GroupwiseRegressor].\n",
    "    x_df : pd.DataFrame\n",
    "        Features for the split.\n",
    "    y_df : pd.Series or single-column pd.DataFrame\n",
    "        Target for the split.\n",
    "    split_name : {\"train\",\"validation\",\"test\"}\n",
    "        Which split to run.\n",
    "    extra_info : dict, optional\n",
    "        Extra metadata to stamp onto the output rows.\n",
    "    return_model : bool, default False\n",
    "        If True, returns the final estimator as the 3rd tuple item; otherwise returns extras dict.\n",
    "    random_state : int, default 12\n",
    "        Random seed for reproducibility.\n",
    "    interval_hours : float, default 0.5\n",
    "        Duration represented by each row (half-hourly = 0.5).\n",
    "    model_id_hash : str, optional\n",
    "        If provided, stamp this precomputed run-level hash (recommended).\n",
    "        If None, a local signature is computed (useful for ad-hoc calls).\n",
    "    params_json_str : str, optional\n",
    "        Pre-rendered pipeline params JSON to stamp; if None, it is computed.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    metrics_df : pd.DataFrame\n",
    "        Per-group metrics with added 'energy_MWh' and metadata columns.\n",
    "    x_cols_used : list[str]\n",
    "        Regressor feature names used by the GroupwiseRegressor (x_vars + fe_vars).\n",
    "    model_or_extras : GroupwiseRegressor | dict\n",
    "        If return_model=True → the fitted final estimator; else a dict of diagnostics.\n",
    "    \"\"\"\n",
    "    np.random.seed(random_state)\n",
    "\n",
    "    for col in x_df.columns:\n",
    "        dt = x_df[col].dtype\n",
    "        if str(dt).startswith((\"uint\", \"UInt\")):\n",
    "            x_df[col] = x_df[col].astype(\"int64\")\n",
    "\n",
    "    if split_name not in (\"train\", \"validation\", \"test\"):\n",
    "        raise ValueError(f\"split_name must be 'train', 'validation', or 'test' (got {split_name!r})\")\n",
    "\n",
    "    X = x_df.copy()\n",
    "    if isinstance(y_df, pd.DataFrame):\n",
    "        if y_df.shape[1] != 1:\n",
    "            raise ValueError(\"y_df must be a Series or single-column DataFrame.\")\n",
    "        y_ser = y_df.iloc[:, 0]\n",
    "    else:\n",
    "        y_ser = y_df\n",
    "\n",
    "    # Use provided model_id_hash (from orchestrator) or compute a local one\n",
    "    if model_id_hash is None:\n",
    "        model_id_hash, _ = signature_for_run(\n",
    "            user_pipeline,\n",
    "            x_columns=list(X.columns),\n",
    "            y=y_ser,\n",
    "            random_state=random_state,\n",
    "            eval_splits=(split_name,),   # local call; orchestrator passes a shared hash\n",
    "            compute_test=False,\n",
    "            extra_info=extra_info,\n",
    "        )\n",
    "\n",
    "    if params_json_str is None:\n",
    "        params_json_str = json.dumps(\n",
    "            user_pipeline.get_params(deep=True),\n",
    "            sort_keys=True, separators=(\",\", \":\"), default=str\n",
    "        )\n",
    "\n",
    "    extras: dict[str, Any] = {}\n",
    "\n",
    "    if split_name == \"train\":\n",
    "        # Fit → metrics from regressor\n",
    "        _ = user_pipeline.fit_transform(X, y_ser)\n",
    "        model = user_pipeline._final_estimator  # type: ignore[attr-defined]\n",
    "        metrics_df = model.get_metrics(summarise=True).reset_index(drop=True)\n",
    "\n",
    "        # Canonicalize group col to \"group\"\n",
    "        if model.group_col in metrics_df.columns:\n",
    "            metrics_df = metrics_df.rename(columns={model.group_col: \"group\"})\n",
    "        elif \"group\" not in metrics_df.columns:\n",
    "            metrics_df = metrics_df.rename(columns={metrics_df.columns[0]: \"group\"})\n",
    "\n",
    "        # Preprocessed rows for weights & diagnostics\n",
    "        x_tr = _apply_fitted_preprocessing(user_pipeline, X)\n",
    "        x_tr[model.y_var] = np.asarray(y_ser, dtype=float)\n",
    "\n",
    "        # Energy weights\n",
    "        w = _compute_group_energy_weights(\n",
    "            df=x_tr, group_col=model.group_col, q_col=model.x_vars[0], interval_hours=interval_hours\n",
    "        ).rename(columns={model.group_col: \"group\"})\n",
    "        metrics_df = metrics_df.merge(w, on=\"group\", how=\"left\")\n",
    "\n",
    "        # Diagnostics (in-sample)\n",
    "        extras[\"pooled_co2\"] = pooled_co2_metrics(\n",
    "            model, x_tr, y_col=model.y_var, group_col=model.group_col\n",
    "        )\n",
    "        me_df = model.transform(x_tr)\n",
    "        fd_df = finite_difference_me_metrics(\n",
    "            df=me_df,\n",
    "            time_col=\"timestamp\" if \"timestamp\" in me_df.columns else \"time_id\",\n",
    "            q_col=model.x_vars[0],\n",
    "            y_col=model.y_var,\n",
    "            me_col=\"ME\",\n",
    "            group_keys=[k for k in (\"city\",) if k in me_df.columns],\n",
    "        )\n",
    "        extras[\"fd_me_by_city\"] = fd_df.to_dict(orient=\"records\") if not fd_df.empty else []\n",
    "        extras[\"fd_me_pooled\"] = (\n",
    "            fd_df.loc[fd_df[\"city\"] == \"ALL\"].iloc[0].to_dict()\n",
    "            if (not fd_df.empty and \"city\" in fd_df.columns and \"ALL\" in fd_df[\"city\"].values)\n",
    "            else (fd_df.sort_values(\"n_pairs\", ascending=False).iloc[0].to_dict() if not fd_df.empty else {})\n",
    "        )\n",
    "\n",
    "    else:\n",
    "        # Use fitted preprocessing + regressor\n",
    "        model = user_pipeline._final_estimator  # type: ignore[attr-defined]\n",
    "        x_tr = _apply_fitted_preprocessing(user_pipeline, X)\n",
    "\n",
    "        if model.group_col not in x_tr.columns:\n",
    "            raise KeyError(\n",
    "                f\"Group column '{model.group_col}' is missing after transform. \"\n",
    "                \"Ensure your binner outputs it.\"\n",
    "            )\n",
    "\n",
    "        x_tr[model.y_var] = np.asarray(y_ser, dtype=float)\n",
    "\n",
    "        # Per-group metrics\n",
    "        metrics_df = evaluate_on_split(model, x_tr)\n",
    "\n",
    "        # Energy weights\n",
    "        w = _compute_group_energy_weights(\n",
    "            df=x_tr, group_col=model.group_col, q_col=model.x_vars[0], interval_hours=interval_hours\n",
    "        ).rename(columns={model.group_col: \"group\"})\n",
    "        metrics_df = metrics_df.merge(w, on=\"group\", how=\"left\")\n",
    "\n",
    "        # Out-of-sample diagnostics\n",
    "        extras[\"pooled_co2\"] = pooled_co2_metrics(\n",
    "            model, x_tr, y_col=model.y_var, group_col=model.group_col\n",
    "        )\n",
    "        me_df = model.transform(x_tr)\n",
    "        fd_df = finite_difference_me_metrics(\n",
    "            df=me_df,\n",
    "            time_col=\"timestamp\" if \"timestamp\" in me_df.columns else \"time_id\",\n",
    "            q_col=model.x_vars[0],\n",
    "            y_col=model.y_var,\n",
    "            me_col=\"ME\",\n",
    "            group_keys=[k for k in (\"city\",) if k in me_df.columns],\n",
    "        )\n",
    "        extras[\"fd_me_by_city\"] = fd_df.to_dict(orient=\"records\") if not fd_df.empty else []\n",
    "        extras[\"fd_me_pooled\"] = (\n",
    "            fd_df.loc[fd_df[\"city\"] == \"ALL\"].iloc[0].to_dict()\n",
    "            if (not fd_df.empty and \"city\" in fd_df.columns and \"ALL\" in fd_df[\"city\"].values)\n",
    "            else (fd_df.sort_values(\"n_pairs\", ascending=False).iloc[0].to_dict() if not fd_df.empty else {})\n",
    "        )\n",
    "\n",
    "    # Stamp metadata\n",
    "    metrics_df[\"data_split\"] = split_name\n",
    "    metrics_df[\"model_id_hash\"] = model_id_hash\n",
    "    metrics_df[\"random_state\"] = random_state\n",
    "    metrics_df[\"pipeline_params_json\"] = params_json_str\n",
    "    metrics_df[\"log_time\"] = datetime.now().isoformat()\n",
    "\n",
    "    model = user_pipeline._final_estimator  # type: ignore[attr-defined]\n",
    "    metrics_df[\"x_columns_used\"] = \",\".join(model.x_vars + model.fe_vars)\n",
    "    for k, v in (extra_info or {}).items():\n",
    "        metrics_df[k] = v\n",
    "\n",
    "    x_cols_used = model.x_vars + model.fe_vars\n",
    "    print(f\"[LOG] {len(metrics_df)} rows for split={split_name}, model_id={model_id_hash}, random_state={random_state}\")\n",
    "\n",
    "    return (metrics_df, x_cols_used, model) if return_model else (metrics_df, x_cols_used, extras)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "44cd5394",
   "metadata": {},
   "outputs": [],
   "source": [
    "def regressor_orchestrator(\n",
    "        user_pipeline: Pipeline,\n",
    "        x_splits: dict,\n",
    "        y_splits: dict,\n",
    "        log_csv_path: str | None = \"marginal_emissions_log.csv\",   # legacy\n",
    "        extra_info: dict | None = None,\n",
    "        force_run: bool = False,\n",
    "        force_overwrite: bool = False,\n",
    "        random_state: int = 12,\n",
    "        group_col_name: str = \"group\",\n",
    "        interval_hours: float = 0.5,\n",
    "        eval_splits: tuple[str, ...] | None = None,\n",
    "        compute_test: bool = False,\n",
    "        # rotating CSV\n",
    "        results_dir: str | None = None,\n",
    "        file_prefix: str | None = None,\n",
    "        max_log_mb: int = 95,\n",
    "        fsync: bool = True,\n",
    ") -> pd.DataFrame | None:\n",
    "    \"\"\"\n",
    "    Fit/evaluate a pipeline on train/validation/test, summarise metrics, and append to a CSV log.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    user_pipeline : Pipeline\n",
    "        Full pipeline, typically [FeatureAddition → Binner → GroupwiseRegressor].\n",
    "    x_splits : dict\n",
    "        Must include \"train\" and \"validation\". Include \"test\" iff compute_test=True.\n",
    "    y_splits : dict\n",
    "        Target splits with the same keys as x_splits.  Must include \"train\" and \"validation\". Include \"test\" iff compute_test=True.\n",
    "     log_csv_path : str, optional\n",
    "        Legacy path; used only to infer default results_dir/file_prefix if those are None.\n",
    "    extra_info : dict, optional\n",
    "        Extra metadata to stamp onto per-split logs (propagates into `run_regressor_model`).\n",
    "    force_run : bool, default=False\n",
    "        If False and an identical model signature was previously logged, skip this run.\n",
    "    force_overwrite : bool, default=False\n",
    "        If True, allows re-logging the same model_id_hash (previous rows are NOT removed here;\n",
    "        use `save_summary_to_csv(..., force_overwrite=True)` for row replacement).\n",
    "    random_state : int, default=12\n",
    "        Random seed recorded in the model signature and summary.\n",
    "    group_col_name : str, default=\"group\"\n",
    "        Canonical group column name used by `summarise_metrics_logs` for nested metrics.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pd.DataFrame or None\n",
    "        One-row summary DataFrame if the run executes; None if skipped due to prior identical log.\n",
    "\n",
    "    Notes\n",
    "    -----\n",
    "    - The model signature (hash) is computed from pipeline parameters, feature columns, target name(s),\n",
    "      random_state, and any `extra_info`. If unchanged and `force_run=False`, the run is skipped.\n",
    "    - `x_columns` recorded in the summary are taken from the **train** split’s evaluation result.\n",
    "    \"\"\"\n",
    "    # in regressor_orchestrator before signature_for_run(...)\n",
    "    if eval_splits is None:\n",
    "        eval_splits = (\"train\",\"validation\",\"test\") if compute_test else (\"train\",\"validation\")\n",
    "    compute_test = (\"test\" in eval_splits)  # ← keep hash consistent with actual splits\n",
    "\n",
    "    # One signature for the whole run (based on TRAIN)\n",
    "    model_key, sig = signature_for_run(\n",
    "        user_pipeline,\n",
    "        x_columns=list(x_splits[\"train\"].columns),\n",
    "        y=y_splits[\"train\"],\n",
    "        random_state=random_state,\n",
    "        eval_splits=eval_splits,\n",
    "        compute_test=compute_test,\n",
    "        extra_info=extra_info,\n",
    "    )\n",
    "\n",
    "    # Resolve dir + prefix (fallback to legacy path)\n",
    "    if results_dir is None or file_prefix is None:\n",
    "        base = Path(log_csv_path or \"marginal_emissions_log.csv\")\n",
    "        inferred_dir = base.parent if str(base.parent) != \"\" else Path(\".\")\n",
    "        inferred_prefix = base.stem\n",
    "        results_dir = results_dir or str(inferred_dir)\n",
    "        file_prefix = file_prefix or inferred_prefix\n",
    "\n",
    "    # De-dupe via index\n",
    "    if not force_run and not force_overwrite:\n",
    "        if is_model_logged_rotating_csv(model_key, results_dir, file_prefix):\n",
    "            print(f\"[SKIP] Model already logged (hash: {model_key})\")\n",
    "            return None\n",
    "\n",
    "    # Precompute params JSON once (consistent across splits)\n",
    "    params_json_str = json.dumps(\n",
    "        user_pipeline.get_params(deep=True),\n",
    "        sort_keys=True, separators=(\",\", \":\"), default=str\n",
    "    )\n",
    "\n",
    "    logs, pooled_extras, fd_extras = {}, {}, {}\n",
    "    x_cols_used: list[str] | None = None\n",
    "\n",
    "    for split in eval_splits:\n",
    "        metrics_df, x_cols_used, extras = run_regressor_model(\n",
    "            user_pipeline=user_pipeline,\n",
    "            x_df=x_splits[split],\n",
    "            y_df=y_splits[split],\n",
    "            split_name=split,\n",
    "            extra_info=extra_info,\n",
    "            return_model=False,\n",
    "            random_state=random_state,\n",
    "            interval_hours=interval_hours,\n",
    "            model_id_hash=model_key,          # shared ID across splits\n",
    "            params_json_str=params_json_str,  # shared params JSON\n",
    "        )\n",
    "        logs[split] = metrics_df\n",
    "        pooled_extras[split] = extras.get(\"pooled_co2\", {})\n",
    "        fd_extras[split] = extras.get(\"fd_me_pooled\", {})\n",
    "\n",
    "    summary_df = summarise_metrics_logs(\n",
    "        train_logs=logs[\"train\"],\n",
    "        val_logs=logs[\"validation\"],\n",
    "        test_logs=logs.get(\"test\"),\n",
    "        user_pipeline=user_pipeline,\n",
    "        x_columns=x_cols_used or [],\n",
    "        random_state=random_state,\n",
    "        group_col_name=group_col_name,           # <- use the parameter you accept\n",
    "        pooled_metrics_by_split=pooled_extras,\n",
    "        fd_me_metrics_by_split=fd_extras,\n",
    "    )\n",
    "\n",
    "    save_summary_to_rotating_csv(\n",
    "        summary_df,\n",
    "        results_dir=results_dir,\n",
    "        file_prefix=file_prefix,\n",
    "        max_mb=max_log_mb,\n",
    "        force_overwrite=force_overwrite,\n",
    "        fsync=fsync,\n",
    "    )\n",
    "    return summary_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2f96d8d",
   "metadata": {},
   "source": [
    "#### Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "4dcb9c36",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_grid_search(\n",
    "        base_feature_pipeline: Pipeline,\n",
    "        regressor_cls,\n",
    "        regressor_kwargs: dict,\n",
    "        grid_config: list[dict],\n",
    "        x_splits: dict,\n",
    "        y_splits: dict,\n",
    "        log_path: str | None,  # legacy; optional now\n",
    "        global_extra_info: dict | None = None,\n",
    "        force_run: bool = False,\n",
    "        force_overwrite: bool = False,\n",
    "        base_feature_pipeline_name: str = \"BaseFeaturePipeline\",\n",
    "        eval_splits: tuple[str, ...] = (\"train\",\"validation\"),\n",
    "        results_dir: str | None = None,\n",
    "        file_prefix: str | None = None,\n",
    "        max_log_mb: int = 95,\n",
    "        fsync: bool = True,\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Execute a series of [features → binner → regressor] runs and log one summary row per config.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    base_feature_pipeline : Pipeline\n",
    "        Preprocessing steps applied before binning. This object is cloned per run to avoid state leakage.\n",
    "    regressor_cls : type\n",
    "        Estimator class to instantiate for the final step (e.g., GroupwiseRegressor).\n",
    "    regressor_kwargs : dict\n",
    "        Baseline kwargs for the regressor. Per-config overrides from `grid_config` are merged on top.\n",
    "        IMPORTANT: This function will not mutate the caller's dict.\n",
    "    grid_config : list of dict\n",
    "        Each item should contain:\n",
    "            - \"binner_class\": class (e.g., MultiQuantileBinner or MultiMedianBinner)\n",
    "            - \"binner_kwargs\": dict of init args for the binner\n",
    "            - \"label\": str label for printing/logging (optional)\n",
    "            - Optional: \"x_vars\", \"fe_vars\" to override the regressor’s predictors per-config\n",
    "            - Optional: anything else you want echoed into `extra_info`\n",
    "    x_splits, y_splits : dict\n",
    "        Dicts keyed by {\"train\",\"validation\",\"test\"} with DataFrames/Series for each split.\n",
    "    log_path : str\n",
    "        CSV path where each successful config appends one summary row.\n",
    "    global_extra_info : dict, optional\n",
    "        Extra metadata stamped into each run’s logs.\n",
    "    force_run, force_overwrite : bool\n",
    "        Passed through to `regressor_orchestrator`.\n",
    "    base_feature_pipeline_name : str, default \"BaseFeaturePipeline\"\n",
    "        Step name used for the features sub-pipeline.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    None\n",
    "        Prints progress and writes rows to `log_path`. Skips silently (with a message) if a config\n",
    "        is already logged and `force_run=False`.\n",
    "\n",
    "    Notes\n",
    "    -----\n",
    "    - We clone `base_feature_pipeline` per run to avoid cross-config state sharing.\n",
    "    - If a binner provides `group_col_name` and the regressor does not specify `group_col`,\n",
    "      we set the regressor’s `group_col` to match.\n",
    "    - If a config provides `x_vars`/`fe_vars`, they override the baseline `regressor_kwargs`.\n",
    "    \"\"\"\n",
    "    missing_x = [s for s in eval_splits if s not in x_splits]\n",
    "    missing_y = [s for s in eval_splits if s not in y_splits]\n",
    "    if missing_x or missing_y:\n",
    "        raise KeyError(f\"Missing splits: X{missing_x} Y{missing_y}\")\n",
    "\n",
    "    total = len(grid_config)\n",
    "    for i, raw_config in enumerate(grid_config, start=1):\n",
    "        config = dict(raw_config)\n",
    "        binner_class = config[\"binner_class\"]\n",
    "        binner_kwargs = dict(config.get(\"binner_kwargs\", {}))\n",
    "        label = config.get(\"label\", binner_class.__name__)\n",
    "\n",
    "        reg_kwargs = dict(regressor_kwargs)\n",
    "        if \"x_vars\" in config:\n",
    "            reg_kwargs[\"x_vars\"] = list(config[\"x_vars\"])\n",
    "        if \"fe_vars\" in config:\n",
    "            reg_kwargs[\"fe_vars\"] = list(config[\"fe_vars\"])\n",
    "        reg_kwargs[\"random_state\"] = reg_kwargs.get(\"random_state\", 12)\n",
    "\n",
    "        binner_group_col = binner_kwargs.get(\"group_col_name\")\n",
    "        if binner_group_col and \"group_col\" not in reg_kwargs:\n",
    "            reg_kwargs[\"group_col\"] = binner_group_col\n",
    "\n",
    "        try:\n",
    "            features_step = clone(base_feature_pipeline)\n",
    "        except Exception:\n",
    "            features_step = base_feature_pipeline\n",
    "\n",
    "        binner = binner_class(**binner_kwargs)\n",
    "        regressor = regressor_cls(**reg_kwargs)\n",
    "\n",
    "        full_pipeline = Pipeline([\n",
    "            (base_feature_pipeline_name, features_step),\n",
    "            (binner_class.__name__, binner),\n",
    "            (regressor_cls.__name__, regressor),\n",
    "        ])\n",
    "\n",
    "        extra_info = {\n",
    "            \"binner_class\": binner_class.__name__,\n",
    "            \"binner_params\": binner_kwargs,\n",
    "            \"regressor_params\": reg_kwargs,\n",
    "            \"grid_label\": label,\n",
    "            **(global_extra_info or {}),\n",
    "        }\n",
    "\n",
    "        rank_tag = \"\"\n",
    "        try:\n",
    "            _, rank, size = _mpi_context()\n",
    "            rank_tag = f\"[R{rank}/{max(size-1,0)}] \"\n",
    "        except Exception:\n",
    "            pass\n",
    "        print(f\"\\n{rank_tag}[GRID {i}/{total}] {label}\")\n",
    "\n",
    "        try:\n",
    "            summary_df = regressor_orchestrator(\n",
    "                user_pipeline=full_pipeline,\n",
    "                x_splits=x_splits,\n",
    "                y_splits=y_splits,\n",
    "                log_csv_path=log_path,            # legacy OK\n",
    "                extra_info=extra_info,\n",
    "                force_run=force_run,\n",
    "                force_overwrite=force_overwrite,\n",
    "                random_state=reg_kwargs[\"random_state\"],\n",
    "                eval_splits=eval_splits,\n",
    "                # NEW\n",
    "                results_dir=results_dir,\n",
    "                file_prefix=file_prefix,\n",
    "                max_log_mb=max_log_mb,\n",
    "                fsync=fsync,\n",
    "                )\n",
    "            if summary_df is not None:\n",
    "                print(f\"[GRID] Logged: {label}\")\n",
    "            else:\n",
    "                print(f\"[GRID] Skipped (already logged): {label}\")\n",
    "        except Exception as e:\n",
    "            print(f\"[GRID] ERROR in '{label}': {type(e).__name__}: {e}\")\n",
    "            continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "3c452581",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_grid_search_auto(\n",
    "        base_feature_pipeline,\n",
    "        regressor_cls,\n",
    "        regressor_kwargs: dict,\n",
    "        grid_config: list[dict],\n",
    "        x_splits: dict,\n",
    "        y_splits: dict,\n",
    "        *,\n",
    "        # logging/rotation knobs\n",
    "        results_dir: str,\n",
    "        file_prefix: str,\n",
    "        max_log_mb: int = 95,\n",
    "        fsync: bool = False,              # set True on HPC if you want durable writes\n",
    "        # orchestration\n",
    "        base_feature_pipeline_name: str = \"FeatureAdditionPipeline\",\n",
    "        eval_splits: tuple[str, ...] = (\"train\",\"validation\"),\n",
    "        force_run: bool = False,\n",
    "        force_overwrite: bool = False,\n",
    "        distribute: str = \"auto\",         # \"auto\" | \"mpi\" | \"single\"\n",
    "        dist_mode: str = \"stride\",        # \"stride\" | \"chunked\"\n",
    "        seed: int = 12,\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Single-node or MPI-parallel grid search runner.\n",
    "\n",
    "    - Auto-detects MPI and splits `grid_config` across ranks.\n",
    "    - Ensures per-rank deterministic RNG via `seed + rank`.\n",
    "    - Uses rotating CSV logging with per-file & index locks.\n",
    "\n",
    "    Parameters are passed straight to `run_grid_search`, except we slice `grid_config`.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    base_feature_pipeline: Pipeline\n",
    "        The base feature pipeline to use for each config.\n",
    "    regressor_cls: Type[BaseEstimator]\n",
    "        The regression model class to use.\n",
    "    regressor_kwargs: dict\n",
    "        Keyword arguments to pass to the regression model.\n",
    "    grid_config: list[dict]\n",
    "        The grid search configuration to use.\n",
    "    x_splits: dict\n",
    "        The input feature splits.\n",
    "    y_splits: dict\n",
    "        The target variable splits.\n",
    "    results_dir: str\n",
    "        The directory to save results.\n",
    "    file_prefix: str\n",
    "        The prefix for result files.\n",
    "    max_log_mb: int\n",
    "        The maximum log file size in MB.\n",
    "    naming: PartNaming | None\n",
    "        Optional naming scheme for output files.\n",
    "    fsync: bool\n",
    "        Whether to fsync log files (for durability).\n",
    "    base_feature_pipeline_name: str\n",
    "        The name of the base feature pipeline.\n",
    "    eval_splits: tuple[str, ...]\n",
    "        The evaluation splits to use.\n",
    "    force_run: bool\n",
    "        Whether to force re-running of existing configs.\n",
    "    force_overwrite: bool\n",
    "        Whether to force overwriting of existing results.\n",
    "    distribute: str\n",
    "        The distribution strategy to use.\n",
    "    dist_mode: str\n",
    "        The distribution mode to use.\n",
    "    seed: int\n",
    "        The random seed to use.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    None\n",
    "        Logs the results of the grid search.\n",
    "    \"\"\"\n",
    "    comm, rank, size = _mpi_context()\n",
    "    if distribute == \"auto\":\n",
    "        distribute = \"mpi\" if size > 1 else \"single\"\n",
    "\n",
    "    # Partition the configs\n",
    "    local_configs = _distribute_configs(grid_config, rank=rank, size=size, mode=dist_mode) \\\n",
    "                    if distribute == \"mpi\" else grid_config\n",
    "    if not local_configs:\n",
    "        if rank == 0:\n",
    "            print(\"[GRID] No configs assigned (empty grid or partition).\")\n",
    "        return\n",
    "\n",
    "    # Per-rank RNG — override/augment existing random_state\n",
    "    local_reg_kwargs = dict(regressor_kwargs)\n",
    "    local_reg_kwargs[\"random_state\"] = int(local_reg_kwargs.get(\"random_state\", seed))\n",
    "\n",
    "    if rank == 0 and distribute == \"mpi\":\n",
    "        print(f\"[MPI] size={size} → ~{len(grid_config)/max(size,1):.1f} configs per rank\")\n",
    "    else:\n",
    "        if distribute == \"mpi\":\n",
    "            print(f\"[MPI] rank={rank}/{size-1} assigned {len(local_configs)} configs\")\n",
    "\n",
    "    run_grid_search(\n",
    "        base_feature_pipeline=base_feature_pipeline,\n",
    "        regressor_cls=regressor_cls,\n",
    "        regressor_kwargs=local_reg_kwargs,\n",
    "        grid_config=local_configs,\n",
    "        x_splits=x_splits,\n",
    "        y_splits=y_splits,\n",
    "        log_path=None,  # legacy path unused when using rotating logs\n",
    "        global_extra_info={\"runner_rank\": rank, \"runner_size\": size},\n",
    "        force_run=force_run,\n",
    "        force_overwrite=force_overwrite,\n",
    "        base_feature_pipeline_name=base_feature_pipeline_name,\n",
    "        eval_splits=eval_splits,\n",
    "        results_dir=results_dir,\n",
    "        file_prefix=file_prefix,\n",
    "        max_log_mb=max_log_mb,\n",
    "        fsync=fsync,\n",
    "    )\n",
    "\n",
    "    # Optional barrier for neat logs\n",
    "    try:\n",
    "        comm.Barrier()\n",
    "    except Exception:\n",
    "        pass\n",
    "    if rank == 0:\n",
    "        print(\"[GRID] Completed (all ranks).\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4002a4a",
   "metadata": {},
   "source": [
    "#### Parameter Grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "26223e11",
   "metadata": {},
   "outputs": [],
   "source": [
    "def all_nonempty_subsets(columns: list[str]) -> list[list[str]]:\n",
    "    \"\"\"All non-empty subsets preserving input order.\"\"\"\n",
    "    return [list(c) for i in range(1, len(columns) + 1) for c in combinations(columns, i)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "f3142ef6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_fe_vars(all_cols: list[str], x_vars: list[str]) -> list[str]:\n",
    "    \"\"\"Complement of x_vars within all_cols.\"\"\"\n",
    "    xset = set(x_vars)\n",
    "    return [c for c in all_cols if c not in xset]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "f4694214",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_x_fe_combinations_disjoint(\n",
    "    candidate_x_vars: list[str],\n",
    "    candidate_fe_vars: list[str],\n",
    "    x_var_length: int = 2,\n",
    "    max_fe_len: int | None = None,\n",
    "    *,\n",
    "    allow_empty_fe: bool = False,\n",
    ") -> list[dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Generate all disjoint non-empty combinations of x_vars and fe_vars.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    candidate_x_vars : list of str\n",
    "        Columns eligible to be used as predictors (x_vars).\n",
    "    candidate_fe_vars : list of str\n",
    "        Columns eligible to be used as fixed effects (fe_vars).\n",
    "    x_var_length : int\n",
    "        Number of x_vars to include in each combination.\n",
    "    max_fe_len : int | None\n",
    "        Maximum number of fe_vars to include in each combination.\n",
    "    allow_empty_fe : bool\n",
    "        Whether to allow empty fe_vars in the combinations.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    list of dicts\n",
    "        Each dict has keys: {'x_vars': [...], 'fe_vars': [...]}\n",
    "    \"\"\"\n",
    "    if x_var_length < 1:\n",
    "        raise ValueError(\"x_var_length must be >= 1\")\n",
    "    if len(candidate_x_vars) < x_var_length:\n",
    "        raise ValueError(\"Not enough candidate_x_vars for requested x_var_length\")\n",
    "\n",
    "    results: list[dict[str, Any]] = []\n",
    "\n",
    "    x_subsets = [list(c) for c in combinations(candidate_x_vars, x_var_length)]\n",
    "    fe_pool = [list(c) for i in range(0 if allow_empty_fe else 1, len(candidate_fe_vars) + 1)\n",
    "               for c in combinations(candidate_fe_vars, i)]\n",
    "\n",
    "    for x_vars in x_subsets:\n",
    "        for fe_vars in fe_pool:\n",
    "            if max_fe_len is not None and len(fe_vars) > max_fe_len:\n",
    "                continue\n",
    "            if set(x_vars).isdisjoint(fe_vars):\n",
    "                results.append({\"x_vars\": x_vars, \"fe_vars\": list(fe_vars)})\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "6ec79c73",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_quantile_grid_configs(\n",
    "        candidate_binning_vars: list[str],\n",
    "        candidate_bin_counts: list[int],\n",
    "        candidate_x_vars: list[str],\n",
    "        candidate_fe_vars: list[str],\n",
    "        x_var_length: int = 2,\n",
    "        binner_extra_grid: dict | list[dict] | None = None,\n",
    ") -> list[dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Produce configs for MultiQuantileBinner sweeping:\n",
    "      - which vars to bin on\n",
    "      - how many bins\n",
    "      - x/fe combinations (disjoint from binned vars)\n",
    "      - optional extra binner kwargs via dict-of-lists or list-of-dicts\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    candidate_binning_vars : list[str]\n",
    "        Variables to be binned.\n",
    "    candidate_bin_counts : list[int]\n",
    "        Number of bins to create for each variable.\n",
    "    candidate_x_vars : list[str]\n",
    "        Variables to use as predictors (x_vars).\n",
    "    candidate_fe_vars : list[str]\n",
    "        Variables to use as fixed effects (fe_vars).\n",
    "    x_var_length : int\n",
    "        Number of x_vars to include in each combination.\n",
    "    binner_extra_grid : dict | list[dict] | None\n",
    "        Optional extra parameters for the binner.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    list[dict[str, Any]]\n",
    "        A list of configuration dictionaries for the binner.\n",
    "    \"\"\"\n",
    "    if not candidate_binning_vars:\n",
    "        return []\n",
    "    if not candidate_bin_counts:\n",
    "        return []\n",
    "\n",
    "    def _expand(grid):\n",
    "        if grid is None:\n",
    "            return [dict()]\n",
    "        if isinstance(grid, list):\n",
    "            return [dict(d) for d in grid]\n",
    "        if isinstance(grid, dict):\n",
    "            keys = list(grid.keys())\n",
    "            vals = [list(v) if isinstance(v, (list, tuple, set)) else [v] for v in (grid[k] for k in keys)]\n",
    "            return [dict(zip(keys, combo)) for combo in product(*vals)]\n",
    "        raise TypeError(\"binner_extra_grid must be a dict or list of dicts\")\n",
    "\n",
    "    extra_list = _expand(binner_extra_grid)\n",
    "    configs: list[dict[str, Any]] = []\n",
    "\n",
    "    # compute once (perf)\n",
    "    x_fe_grid = build_x_fe_combinations_disjoint(\n",
    "        candidate_x_vars, candidate_fe_vars, x_var_length=x_var_length\n",
    "    )\n",
    "\n",
    "    for bin_vars in all_nonempty_subsets(candidate_binning_vars):\n",
    "        bset = set(bin_vars)\n",
    "        for bin_count in candidate_bin_counts:\n",
    "            if int(bin_count) < 2:\n",
    "                continue\n",
    "            bin_spec = {v: int(bin_count) for v in bin_vars}\n",
    "\n",
    "            for combo in x_fe_grid:\n",
    "                if not set(combo[\"x_vars\"]).isdisjoint(bset):\n",
    "                    continue\n",
    "                for extra in extra_list:\n",
    "                    binner_kwargs = {\"bin_specs\": bin_spec, **extra}\n",
    "\n",
    "                    # label suffix for clarity in logs\n",
    "                    tag_bits = []\n",
    "                    pol = extra.get(\"oob_policy\")\n",
    "                    if pol: tag_bits.append(f\"oob{pol}\")\n",
    "                    rate = extra.get(\"max_oob_rate\")\n",
    "                    if rate is not None: tag_bits.append(f\"rate{float(rate):g}\")\n",
    "                    tag = f\"__{'_'.join(tag_bits)}\" if tag_bits else \"\"\n",
    "\n",
    "                    configs.append({\n",
    "                        \"binner_class\": MultiQuantileBinner,\n",
    "                        \"binner_kwargs\": binner_kwargs,\n",
    "                        \"label\": (\n",
    "                            f\"qbin_{bin_count}_{'-'.join(bin_vars)}\"\n",
    "                            f\"__x_{'-'.join(combo['x_vars'])}\"\n",
    "                            f\"__fe_{'-'.join(combo['fe_vars'])}{tag}\"\n",
    "                        ),\n",
    "                        \"x_vars\": combo[\"x_vars\"],\n",
    "                        \"fe_vars\": combo[\"fe_vars\"],\n",
    "                    })\n",
    "    return configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "e844b32b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_median_binner_configs(\n",
    "    candidate_binning_vars: list[str],\n",
    "    candidate_x_vars: list[str],\n",
    "    candidate_fe_vars: list[str],\n",
    "    x_var_length: int = 2,\n",
    "    max_fe_len: int | None = None,\n",
    "    binner_extra_grid: dict | list[dict] | None = None,\n",
    ") -> list[dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Produce configs for MultiMedianBinner sweeping subsets of variables and x/fe combos.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    candidate_binning_vars : list[str]\n",
    "        Variables to be binned.\n",
    "    candidate_x_vars : list[str]\n",
    "        Variables to use as predictors (x_vars).\n",
    "    candidate_fe_vars : list[str]\n",
    "        Variables to use as fixed effects (fe_vars).\n",
    "    x_var_length : int\n",
    "        Number of x_vars to include in each combination.\n",
    "    max_fe_len : int | None\n",
    "        Maximum number of fixed effects to include in each combination.\n",
    "    binner_extra_grid : dict | list[dict] | None\n",
    "        Optional extra parameters for the binner.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    list[dict[str, Any]]\n",
    "        A list of configuration dictionaries for the binner.\n",
    "    \"\"\"\n",
    "    if not candidate_binning_vars:\n",
    "        return []\n",
    "\n",
    "    def _expand(grid):\n",
    "        if grid is None:\n",
    "            return [dict()]\n",
    "        if isinstance(grid, list):\n",
    "            return [dict(d) for d in grid]\n",
    "        if isinstance(grid, dict):\n",
    "            keys = list(grid.keys())\n",
    "            vals = [ (v if isinstance(v, (list, tuple, set)) else [v]) for v in grid.values() ]\n",
    "            return [dict(zip(keys, combo)) for combo in product(*vals)]\n",
    "        raise TypeError(\"binner_extra_grid must be a dict or list of dicts\")\n",
    "\n",
    "    extra_list = _expand(binner_extra_grid)\n",
    "\n",
    "    configs: list[dict[str, Any]] = []\n",
    "    x_fe_grid = build_x_fe_combinations_disjoint(\n",
    "        candidate_x_vars, candidate_fe_vars, x_var_length=x_var_length, max_fe_len=max_fe_len\n",
    "    )\n",
    "\n",
    "    for bin_vars in all_nonempty_subsets(candidate_binning_vars):\n",
    "        bset = set(bin_vars)\n",
    "        for combo in x_fe_grid:\n",
    "            if not set(combo[\"x_vars\"]).isdisjoint(bset):\n",
    "                continue\n",
    "            for extra in extra_list:\n",
    "                binner_kwargs = {\n",
    "                    \"variables\": bin_vars,\n",
    "                    \"group_col_name\": \"median_group_id\",\n",
    "                    \"retain_flags\": True,\n",
    "                    **extra,\n",
    "                }\n",
    "                tag_bits = []\n",
    "                if \"retain_flags\" in extra:\n",
    "                    tag_bits.append(f\"rf{int(bool(extra['retain_flags']))}\")\n",
    "                for k, v in extra.items():\n",
    "                    if k == \"retain_flags\":\n",
    "                        continue\n",
    "                    tag_bits.append(f\"{k}{v}\")\n",
    "                tag = f\"__{'_'.join(tag_bits)}\" if tag_bits else \"\"\n",
    "\n",
    "                configs.append({\n",
    "                    \"binner_class\": MultiMedianBinner,\n",
    "                    \"binner_kwargs\": binner_kwargs,\n",
    "                    \"label\": (\n",
    "                        f\"median_{'-'.join(bin_vars)}\"\n",
    "                        f\"__x_{'-'.join(combo['x_vars'])}\"\n",
    "                        f\"__fe_{'-'.join(combo['fe_vars'])}{tag}\"\n",
    "                    ),\n",
    "                    \"x_vars\": combo[\"x_vars\"],\n",
    "                    \"fe_vars\": combo[\"fe_vars\"],\n",
    "                })\n",
    "    return configs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "755e0565",
   "metadata": {},
   "source": [
    "### New Development"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00283ee1",
   "metadata": {},
   "source": [
    "#### Feature Transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "b6afc5f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class WindDirToCyclic(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, dir_col: str = \"wind_direction_meteorological\",\n",
    "                 out_sin: str = \"wind_dir_sin\", out_cos: str = \"wind_dir_cos\",\n",
    "                 drop_original: bool = True):\n",
    "        self.dir_col = dir_col\n",
    "        self.out_sin = out_sin\n",
    "        self.out_cos = out_cos\n",
    "        self.drop_original = drop_original\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        if self.dir_col not in X.columns:\n",
    "            raise KeyError(f\"'{self.dir_col}' not in input\")\n",
    "        return self\n",
    "\n",
    "    def transform(self, X: pd.DataFrame) -> pd.DataFrame:\n",
    "        df = X.copy()\n",
    "        theta = np.deg2rad(df[self.dir_col].astype(float))\n",
    "        df[self.out_sin] = np.sin(theta)\n",
    "        df[self.out_cos] = np.cos(theta)\n",
    "        if self.drop_original:\n",
    "            df = df.drop(columns=[self.dir_col])\n",
    "        return df\n",
    "\n",
    "    def get_feature_names_out(self, input_features=None):\n",
    "        base = [] if input_features is None else list(input_features)\n",
    "        if self.drop_original and self.dir_col in base:\n",
    "            base.remove(self.dir_col)\n",
    "        return np.array(base + [self.out_sin, self.out_cos])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "6ae6066f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class WindDirCyclical(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, col=\"wind_direction_meteorological\"):\n",
    "        self.col = col\n",
    "    def fit(self, X, y=None):\n",
    "        if self.col not in X.columns:\n",
    "            raise KeyError(f\"{self.col} not in columns\")\n",
    "        return self\n",
    "    def transform(self, X):\n",
    "        df = X.copy()\n",
    "        rad = np.deg2rad(df[self.col].astype(float))\n",
    "        df[\"wind_dir_sin\"] = np.sin(rad)\n",
    "        df[\"wind_dir_cos\"] = np.cos(rad)\n",
    "        return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "71470ae9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Log1pTransform(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, columns: Sequence[str]):\n",
    "        self.columns = list(columns)\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        for c in self.columns:\n",
    "            if c not in X.columns:\n",
    "                raise KeyError(f\"'{c}' not found\")\n",
    "        return self\n",
    "\n",
    "    def transform(self, X: pd.DataFrame) -> pd.DataFrame:\n",
    "        df = X.copy()\n",
    "        for c in self.columns:\n",
    "            df[c] = np.log1p(df[c].astype(float))\n",
    "        return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "f6347b08",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Winsorize(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, columns: Sequence[str], lower: float = 0.0, upper: float = 0.995):\n",
    "        self.columns = list(columns)\n",
    "        self.lower = float(lower)\n",
    "        self.upper = float(upper)\n",
    "        self.bounds_: dict[str, tuple[float, float]] = {}\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        self.bounds_.clear()\n",
    "        for c in self.columns:\n",
    "            if c not in X.columns:\n",
    "                raise KeyError(f\"'{c}' not found\")\n",
    "            arr = X[c].astype(float).to_numpy()\n",
    "            lo = np.nanpercentile(arr, self.lower * 100.0)\n",
    "            hi = np.nanpercentile(arr, self.upper * 100.0)\n",
    "            self.bounds_[c] = (lo, hi)\n",
    "        return self\n",
    "\n",
    "    def transform(self, X: pd.DataFrame) -> pd.DataFrame:\n",
    "        df = X.copy()\n",
    "        for c, (lo, hi) in self.bounds_.items():\n",
    "            df[c] = df[c].clip(lo, hi)\n",
    "        return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "091a965d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class StandardizeContinuous(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, columns: Sequence[str], suffix: str = \"_std\", with_mean: bool = True, with_std: bool = True):\n",
    "        self.columns = list(columns)\n",
    "        self.suffix = suffix\n",
    "        self.with_mean = with_mean\n",
    "        self.with_std = with_std\n",
    "        self.means_: dict[str, float] = {}\n",
    "        self.stds_: dict[str, float] = {}\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        self.means_.clear()\n",
    "        self.stds_.clear()\n",
    "        for c in self.columns:\n",
    "            if c not in X.columns:\n",
    "                raise KeyError(f\"'{c}' not found\")\n",
    "            vals = X[c].astype(float)\n",
    "            mu = float(vals.mean()) if self.with_mean else 0.0\n",
    "            sd = float(vals.std(ddof=0)) if self.with_std else 1.0\n",
    "            if sd == 0.0:\n",
    "                sd = 1.0\n",
    "            self.means_[c] = mu\n",
    "            self.stds_[c] = sd\n",
    "        return self\n",
    "\n",
    "    def transform(self, X: pd.DataFrame) -> pd.DataFrame:\n",
    "        df = X.copy()\n",
    "        for c in self.columns:\n",
    "            mu, sd = self.means_[c], self.stds_[c]\n",
    "            df[f\"{c}{self.suffix}\"] = (df[c].astype(float) - mu) / sd\n",
    "        return df\n",
    "\n",
    "    # helpers for invertibility\n",
    "    def original_from_std(self, std_values: pd.Series, col: str) -> pd.Series:\n",
    "        mu, sd = self.means_[col], self.stds_[col]\n",
    "        return std_values * sd + mu\n",
    "\n",
    "    def get_feature_names_out(self, input_features=None):\n",
    "        base = [] if input_features is None else list(input_features)\n",
    "        return np.array(base + [f\"{c}{self.suffix}\" for c in self.columns])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "6919b7bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class OneHotTimeFE(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, columns: Sequence[str] = (\"month\",\"hour\"), drop_first: bool = True, dtype=\"int8\"):\n",
    "        self.columns = list(columns)\n",
    "        self.drop_first = drop_first\n",
    "        self.dtype = dtype\n",
    "        self.out_cols_: list[str] = []\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        for c in self.columns:\n",
    "            if c not in X.columns:\n",
    "                raise KeyError(f\"'{c}' not found\")\n",
    "        dummies = pd.get_dummies(X[self.columns].astype(\"Int64\"), columns=self.columns,\n",
    "                                 drop_first=self.drop_first, dtype=self.dtype)\n",
    "        self.out_cols_ = list(dummies.columns)\n",
    "        return self\n",
    "\n",
    "    def transform(self, X: pd.DataFrame) -> pd.DataFrame:\n",
    "        df = X.copy()\n",
    "        dummies = pd.get_dummies(df[self.columns].astype(\"Int64\"), columns=self.columns,\n",
    "                                 drop_first=self.drop_first, dtype=self.dtype)\n",
    "        # align to training columns\n",
    "        for c in self.out_cols_:\n",
    "            if c not in dummies.columns:\n",
    "                dummies[c] = 0\n",
    "        dummies = dummies[self.out_cols_]\n",
    "        df = pd.concat([df, dummies], axis=1)\n",
    "        return df\n",
    "\n",
    "    def get_feature_names_out(self, input_features=None):\n",
    "        base = [] if input_features is None else list(input_features)\n",
    "        return np.array(base + self.out_cols_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8715ed91",
   "metadata": {},
   "source": [
    "#### Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "33a72f9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class _BaseGAMEstimator(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"\n",
    "    Base: handles fit/transform/predict, builds X-matrix, and rescales ME using chain rule.\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 y_var: str = \"tons_co2\",\n",
    "                 q_col: str = \"demand_met\",\n",
    "                 q_std_col: str = \"demand_met_std\",         # provided by StandardizeContinuous\n",
    "                 weather_linear_cols: Optional[Sequence[str]] = None,\n",
    "                 fe_cols: Optional[Sequence[str]] = None,   # one-hot time FE columns\n",
    "                 n_splines_q: int = 15,\n",
    "                 lam_q: float = 10.0,\n",
    "                 random_state: Optional[int] = 12):\n",
    "        self.y_var = y_var\n",
    "        self.q_col = q_col\n",
    "        self.q_std_col = q_std_col\n",
    "        self.weather_linear_cols = list(weather_linear_cols or [])\n",
    "        self.fe_cols = list(fe_cols or [])\n",
    "        self.n_splines_q = int(n_splines_q)\n",
    "        self.lam_q = float(lam_q)\n",
    "        self.random_state = random_state\n",
    "\n",
    "        # learned\n",
    "        self.gam_: Optional[LinearGAM] = None\n",
    "        self._x_cols_: list[str] = []\n",
    "        self._q_index_: Optional[int] = None\n",
    "        self._q_std_: Optional[float] = None   # for chain rule\n",
    "        self._q_mean_: Optional[float] = None  # not needed but stored for completeness\n",
    "\n",
    "    # to be implemented by subclasses\n",
    "    def _gam_terms(self) -> list:\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def _build_matrix(self, df: pd.DataFrame) -> np.ndarray:\n",
    "        cols = [self.q_std_col] + self.weather_linear_cols + self.fe_cols\n",
    "        self._x_cols_ = cols\n",
    "        X = df[cols].to_numpy(dtype=float)\n",
    "        # remember where q_std is located for derivative\n",
    "        self._q_index_ = 0\n",
    "        return X\n",
    "\n",
    "    def fit(self, X: pd.DataFrame, y: pd.Series | pd.DataFrame | None = None):\n",
    "        if y is None:\n",
    "            if self.y_var not in X.columns:\n",
    "                raise KeyError(f\"y_var '{self.y_var}' not in X\")\n",
    "            y_arr = X[self.y_var].astype(float).to_numpy()\n",
    "        else:\n",
    "            y_arr = (y.iloc[:,0] if isinstance(y, pd.DataFrame) else y).astype(float).to_numpy()\n",
    "\n",
    "        # chain rule scale: we need std of the ORIGINAL q to convert d/dQ_std -> d/dQ\n",
    "        # Expect that a StandardizeContinuous step created '<q_col>_std' and stored stats.\n",
    "        # We fetch std and mean from columns if present as attributes on a fitted StandardizeContinuous,\n",
    "        # but since we're inside the estimator, require caller to provide q_std_col and pass the raw q_col too.\n",
    "        if self.q_col not in X.columns or self.q_std_col not in X.columns:\n",
    "            raise KeyError(f\"Expect both '{self.q_col}' and '{self.q_std_col}' in the DataFrame\")\n",
    "\n",
    "        # compute std directly from train (same as preprocessor stored); we keep it here for ME scaling\n",
    "        q_series = X[self.q_col].astype(float)\n",
    "        self._q_mean_ = float(q_series.mean())\n",
    "        q_std = float(q_series.std(ddof=0))\n",
    "        self._q_std_ = 1.0 if q_std == 0.0 else q_std\n",
    "\n",
    "        Xmat = self._build_matrix(X)\n",
    "\n",
    "        # build GAM with specified terms\n",
    "        terms = self._gam_terms()\n",
    "        np.random.seed(self.random_state)  # ensures reproducible smoothers\n",
    "        gam = LinearGAM(terms, fit_intercept=True, lam=self.lam_q)\n",
    "        gam.n_splines[self._q_index_] = int(self.n_splines_q)  # set for Q spline\n",
    "        self.gam_ = gam.fit(Xmat, y_arr)\n",
    "        return self\n",
    "\n",
    "    def _derivative_q_std_df(self, df: pd.DataFrame, h: float = 1e-4) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Version-agnostic derivative: central finite difference on the standardized demand feature.\n",
    "        Returns dY/d(Q_std) for each row.\n",
    "        \"\"\"\n",
    "        assert self.gam_ is not None, \"Model not fitted\"\n",
    "\n",
    "        df_p = df.copy()\n",
    "        df_m = df.copy()\n",
    "        df_p[self.q_std_col] = df_p[self.q_std_col].astype(float) + h\n",
    "        df_m[self.q_std_col] = df_m[self.q_std_col].astype(float) - h\n",
    "\n",
    "        Xp = self._build_matrix(df_p)\n",
    "        Xm = self._build_matrix(df_m)\n",
    "\n",
    "        # predict is vectorized; two passes only\n",
    "        y_p = self.gam_.predict(Xp)\n",
    "        y_m = self.gam_.predict(Xm)\n",
    "        return (y_p - y_m) / (2.0 * h)\n",
    "\n",
    "    def predict(self, X: pd.DataFrame, predict_type: str = \"ME\") -> np.ndarray:\n",
    "        if self.gam_ is None:\n",
    "            raise RuntimeError(\"Model not fitted\")\n",
    "\n",
    "        if predict_type.lower() == \"y\":\n",
    "            Xmat = self._build_matrix(X)\n",
    "            return self.gam_.predict(Xmat)\n",
    "\n",
    "        # dY/d(Q_std) via central differences, then chain rule to original Q units\n",
    "        d_y_d_qstd = self._derivative_q_std_df(X)              # length n\n",
    "        q_sd = float(self._q_std_ or 1.0)                      # from fit()\n",
    "        return d_y_d_qstd * (1.0 / q_sd)                       # dY/dQ = dY/dQstd * (1/std(Q))\n",
    "\n",
    "    def transform(self, X: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Returns X plus columns:\n",
    "            - 'y_pred' : predicted tons_co2\n",
    "            - 'ME'     : marginal emissions in original Q units\n",
    "        \"\"\"\n",
    "        df = X.copy()\n",
    "        yhat = self.predict(df, predict_type=\"y\")\n",
    "        me = self.predict(df, predict_type=\"ME\")\n",
    "        df[\"y_pred\"] = yhat\n",
    "        df[\"ME\"] = me\n",
    "        return df\n",
    "\n",
    "    def get_params(self, deep=True):\n",
    "        return {\n",
    "            \"y_var\": self.y_var,\n",
    "            \"q_col\": self.q_col,\n",
    "            \"q_std_col\": self.q_std_col,\n",
    "            \"weather_linear_cols\": list(self.weather_linear_cols),\n",
    "            \"fe_cols\": list(self.fe_cols),\n",
    "            \"n_splines_q\": self.n_splines_q,\n",
    "            \"lam_q\": self.lam_q,\n",
    "            \"random_state\": self.random_state,\n",
    "        }\n",
    "\n",
    "    def set_params(self, **params):\n",
    "        for k, v in params.items():\n",
    "            setattr(self, k, v)\n",
    "        return self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "575a3348",
   "metadata": {},
   "outputs": [],
   "source": [
    "class QSplineRegressorPyGAM(_BaseGAMEstimator):\n",
    "    def _gam_terms(self):\n",
    "            lin_count = len(self.weather_linear_cols) + len(self.fe_cols)\n",
    "            terms_list = [s(0)] + [l(i+1) for i in range(lin_count)]\n",
    "            return _sum_terms(terms_list)  # <-- return a TermList, not a list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "abaf718c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class QGAMRegressorPyGAM(_BaseGAMEstimator):\n",
    "    def __init__(self,\n",
    "                 y_var: str = \"tons_co2\",\n",
    "                 q_col: str = \"demand_met\",\n",
    "                 q_std_col: str = \"demand_met_std\",\n",
    "                 # which of the linear cols should be smoothed (pass their *_std names)\n",
    "                 smooth_cols: Optional[Sequence[str]] = None,   # e.g., [\"surface_net_solar_radiation_joules_per_m2_std\",\"wind_speed_mps_std\",\"temperature_celsius_std\"]\n",
    "                 weather_linear_cols: Optional[Sequence[str]] = None,\n",
    "                 fe_cols: Optional[Sequence[str]] = None,\n",
    "                 n_splines_q: int = 15, lam_q: float = 10.0,\n",
    "                 n_splines_other: int = 15, lam_other: float = 10.0,\n",
    "                 random_state: Optional[int] = 12):\n",
    "        super().__init__(y_var=y_var, q_col=q_col, q_std_col=q_std_col,\n",
    "                         weather_linear_cols=weather_linear_cols, fe_cols=fe_cols,\n",
    "                         n_splines_q=n_splines_q, lam_q=lam_q, random_state=random_state)\n",
    "        self.smooth_cols = list(smooth_cols or [])\n",
    "        self.n_splines_other = int(n_splines_other)\n",
    "        self.lam_other = float(lam_other)\n",
    "\n",
    "    def _gam_terms(self):\n",
    "        terms_list = [s(0)]\n",
    "        offset = 1\n",
    "        for i, col in enumerate(self.weather_linear_cols + self.fe_cols):\n",
    "            j = offset + i\n",
    "            if col in self.smooth_cols:\n",
    "                terms_list.append(s(j))\n",
    "            else:\n",
    "                terms_list.append(l(j))\n",
    "        return _sum_terms(terms_list)\n",
    "\n",
    "    def fit(self, X: pd.DataFrame, y: pd.Series | pd.DataFrame | None = None):\n",
    "        # need to override to set different n_splines / lam for \"other\" smooth terms\n",
    "        super().fit(X, y)  # builds self.gam_, self._x_cols_, etc.\n",
    "        # Refit using desired lam per-term\n",
    "        if self.gam_ is None:\n",
    "            return self\n",
    "\n",
    "        Xmat = self._build_matrix(X)\n",
    "        # Recreate terms and assign lam/n_splines per term\n",
    "        terms = self._gam_terms()\n",
    "        np.random.seed(self.random_state)  # ensures reproducible smoothers\n",
    "        gam = LinearGAM(terms, fit_intercept=True)\n",
    "        # lam vector length = number of terms\n",
    "        lam_vec = []\n",
    "        for idx, term in enumerate(terms):\n",
    "            if idx == 0:  # Q spline\n",
    "                lam_vec.append(self.lam_q)\n",
    "            else:\n",
    "                # check which column\n",
    "                colname = (self.weather_linear_cols + self.fe_cols)[idx - 1]\n",
    "                lam_vec.append(self.lam_other if colname in self.smooth_cols else 0.0)\n",
    "        gam.lam = lam_vec\n",
    "\n",
    "        # set n_splines for smooth terms\n",
    "        # term indices align with features order; check types via isinstance on term\n",
    "        from pygam.terms import SplineTerm, LinearTerm\n",
    "        for idx, term in enumerate(gam.terms):\n",
    "            if isinstance(term, SplineTerm):\n",
    "                if idx == 0:  # Q\n",
    "                    gam.n_splines[idx] = int(self.n_splines_q)\n",
    "                else:\n",
    "                    gam.n_splines[idx] = int(self.n_splines_other)\n",
    "\n",
    "        y_arr = (X[self.y_var].astype(float).to_numpy()\n",
    "                 if y is None else (y.iloc[:,0] if isinstance(y, pd.DataFrame) else y).astype(float).to_numpy())\n",
    "        np.random.seed(self.random_state)          # for reproducibility\n",
    "        self.gam_ = gam.fit(Xmat, y_arr)\n",
    "        return self"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a27e9b7",
   "metadata": {},
   "source": [
    "#### Pipelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "6136704d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_mef_preprocess_pipeline(\n",
    "    timestamp_col: str = \"timestamp\",\n",
    "    use_winsorize_precip: bool = True,\n",
    "    winsor_upper: float = 0.995,\n",
    "):\n",
    "    \"\"\"\n",
    "    Returns a sklearn Pipeline that:\n",
    "      - adds month/hour (using your DateTimeFeatureAdder)\n",
    "      - cyclic-encodes wind direction\n",
    "      - log1p precipitation (winsorize optional)\n",
    "      - standardizes continuous columns (adds *_std, keeps originals)\n",
    "      - one-hot month/hour\n",
    "    \"\"\"\n",
    "    # you already have DateTimeFeatureAdder in your codebase; reuse it here\n",
    "    dt = DateTimeFeatureAdder(timestamp_col=timestamp_col, drop_original=False)\n",
    "\n",
    "    # columns from your reduced set\n",
    "    cont_cols = [\n",
    "        \"demand_met\",\n",
    "        \"wind_speed_mps\",\n",
    "        \"temperature_celsius\",\n",
    "        \"precipitation_mm\",   # will be log1p'ed first\n",
    "        \"surface_net_solar_radiation_joules_per_m2\",\n",
    "        \"total_cloud_cover\",\n",
    "    ]\n",
    "\n",
    "    steps = [\n",
    "        (\"DateTime\", dt),\n",
    "        (\"WindDirCyclic\", WindDirToCyclic(dir_col=\"wind_direction_meteorological\",\n",
    "                                          out_sin=\"wind_dir_sin\", out_cos=\"wind_dir_cos\", drop_original=True)),\n",
    "        (\"Log1pPrecip\", Log1pTransform(columns=[\"precipitation_mm\"])),\n",
    "    ]\n",
    "    if use_winsorize_precip:\n",
    "        steps.append((\"WinsorizePrecip\", Winsorize(columns=[\"precipitation_mm\"], lower=0.0, upper=winsor_upper)))\n",
    "\n",
    "    steps += [\n",
    "        (\"Standardize\", StandardizeContinuous(columns=cont_cols, suffix=\"_std\")),\n",
    "        (\"TimeFE\", OneHotTimeFE(columns=(\"month\",\"hour\"), drop_first=True, dtype=\"int8\")),\n",
    "    ]\n",
    "    return Pipeline(steps, verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "315ced5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_mef_preprocess_pipeline_2(\n",
    "    timestamp_col: str = \"timestamp\",\n",
    "    use_winsorize_precip: bool = True,\n",
    "    winsor_upper: float = 0.995,\n",
    "):\n",
    "    cont_cols = [\n",
    "        \"demand_met\",\n",
    "        \"wind_speed_mps\",\n",
    "        \"temperature_celsius\",\n",
    "        \"precipitation_mm_log1p\",  # <- we’ll create this\n",
    "        \"surface_net_solar_radiation_joules_per_m2\",\n",
    "        \"total_cloud_cover\",\n",
    "    ]\n",
    "\n",
    "    steps = [\n",
    "        (\"Add_Datetime_Features\", DateTimeFeatureAdder(timestamp_col=timestamp_col, drop_original=False)),\n",
    "        (\"Add_Fourier_Time\", TimeFourierAdder(timestamp_col=timestamp_col)),        # hour/dow/doy sin/cos\n",
    "        (\"WindDirCyc\", WindDirCyclical(col=\"wind_direction_meteorological\")),      # sin/cos of wind dir\n",
    "        (\"Add_Original\", AnalysisFeatureAdder(timestamp_col=timestamp_col, demand_met_col=\"demand_met\", co2_col=\"tons_co2\")),\n",
    "    ]\n",
    "\n",
    "    if use_winsorize_precip:\n",
    "        steps.append((\"WinsorizePrecip\", Winsorize(columns=[\"precipitation_mm\"], lower=0.0, upper=winsor_upper)))\n",
    "    steps.append((\"Log1pPrecip\", Log1pColumns(columns=[\"precipitation_mm\"])))\n",
    "\n",
    "    steps += [\n",
    "        (\"Standardize\", StandardizeContinuous(columns=cont_cols, suffix=\"_std\")),  # adds *_std, keeps originals\n",
    "        # No month/hour one-hots here — Fourier + is_weekend are enough\n",
    "    ]\n",
    "\n",
    "    return Pipeline(steps, verbose=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "d759d857",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_qspline_pipeline(\n",
    "    preprocess: Pipeline,\n",
    "    fe_cols_prefixes: Iterable[str] = (\"month_\", \"hour_\"),\n",
    "    y_var: str = \"tons_co2\",\n",
    "    q_col: str = \"demand_met\",\n",
    "    q_std_col: str = \"demand_met_std\",\n",
    "    weather_linear_base: Optional[Sequence[str]] = None,\n",
    "    n_splines_q: int = 15,\n",
    "    lam_q: float = 10.0,\n",
    "    random_state: int = 12,\n",
    "):\n",
    "    \"\"\"\n",
    "    Full pipeline: [preprocess] -> QSplineRegressorPyGAM\n",
    "    \"\"\"\n",
    "    weather_linear_base = list(weather_linear_base or [])\n",
    "    # auto-discover FE one-hots produced by OneHotTimeFE\n",
    "    def _fe_names_after_fit(df_pre: pd.DataFrame) -> list[str]:\n",
    "        return [c for c in df_pre.columns if c.startswith(fe_cols_prefixes)]\n",
    "\n",
    "    # placeholder estimator, fe_cols filled at fit-time\n",
    "    est = QSplineRegressorPyGAM(\n",
    "        y_var=y_var, q_col=q_col, q_std_col=q_std_col,\n",
    "        weather_linear_cols=weather_linear_base + [\"wind_dir_sin\", \"wind_dir_cos\"],\n",
    "        fe_cols=[], n_splines_q=n_splines_q, lam_q=lam_q, random_state=random_state,\n",
    "    )\n",
    "\n",
    "    class _AttachFENames(BaseEstimator, TransformerMixin):\n",
    "        \"\"\"Small meta-transformer to attach FE names right before fitting the estimator inside Pipeline.\"\"\"\n",
    "        def __init__(self, estimator: QSplineRegressorPyGAM):\n",
    "            self.estimator = estimator\n",
    "        def fit(self, X, y=None):\n",
    "            # X here is already preprocessed; capture FE names\n",
    "            fe_cols = _fe_names_after_fit(X)\n",
    "            self.estimator.fe_cols = fe_cols\n",
    "            return self\n",
    "        def transform(self, X):\n",
    "            return X\n",
    "\n",
    "    return Pipeline([\n",
    "        (\"PreprocessMEF\", preprocess),\n",
    "        (\"AttachFE\", _AttachFENames(est)),\n",
    "        (\"QSplineRegressorPyGAM\", est),\n",
    "    ])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "d851fb2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_qgam_pipeline(\n",
    "    preprocess: Pipeline,\n",
    "    fe_cols_prefixes: Iterable[str] = (\"month_\", \"hour_\"),\n",
    "    y_var: str = \"tons_co2\",\n",
    "    q_col: str = \"demand_met\",\n",
    "    q_std_col: str = \"demand_met_std\",\n",
    "    smooth_cols: Optional[Sequence[str]] = None,  # e.g., [\"surface_net_solar_radiation_joules_per_m2_std\",\"wind_speed_mps_std\",\"temperature_celsius_std\"]\n",
    "    linear_extra: Optional[Sequence[str]] = None, # e.g., [\"precipitation_mm_std\",\"total_cloud_cover_std\",\"wind_dir_sin\",\"wind_dir_cos\"]\n",
    "    n_splines_q: int = 15, lam_q: float = 10.0,\n",
    "    n_splines_other: int = 15, lam_other: float = 10.0,\n",
    "    random_state: int = 12,\n",
    "):\n",
    "    smooth_cols = list(smooth_cols or [])\n",
    "    linear_extra = list(linear_extra or [\"precipitation_mm_std\", \"total_cloud_cover_std\", \"wind_dir_sin\", \"wind_dir_cos\"])\n",
    "\n",
    "    def _fe_names_after_fit(df_pre: pd.DataFrame) -> list[str]:\n",
    "        return [c for c in df_pre.columns if c.startswith(fe_cols_prefixes)]\n",
    "\n",
    "    est = QGAMRegressorPyGAM(\n",
    "        y_var=y_var, q_col=q_col, q_std_col=q_std_col,\n",
    "        smooth_cols=smooth_cols,\n",
    "        weather_linear_cols=smooth_cols + linear_extra,   # order matters; smooth subset will be detected inside\n",
    "        fe_cols=[],\n",
    "        n_splines_q=n_splines_q, lam_q=lam_q,\n",
    "        n_splines_other=n_splines_other, lam_other=lam_other,\n",
    "        random_state=random_state,\n",
    "    )\n",
    "\n",
    "    class _AttachFENames(BaseEstimator, TransformerMixin):\n",
    "        def __init__(self, estimator: QGAMRegressorPyGAM):\n",
    "            self.estimator = estimator\n",
    "        def fit(self, X, y=None):\n",
    "            self.estimator.fe_cols = _fe_names_after_fit(X)\n",
    "            return self\n",
    "        def transform(self, X):\n",
    "            return X\n",
    "\n",
    "    return Pipeline([\n",
    "        (\"PreprocessMEF\", preprocess),\n",
    "        (\"AttachFE\", _AttachFENames(est)),\n",
    "        (\"QGAMRegressorPyGAM\", est),\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "6a54a813",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TimeFourierAdder(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"\n",
    "    Add smooth cyclic time features:\n",
    "      - hour_sin, hour_cos   (24h cycle using hour + minute)\n",
    "      - dow_sin,  dow_cos    (weekly cycle, 0=Mon..6=Sun)\n",
    "      - doy_sin,  doy_cos    (annual cycle, 1..365/366)\n",
    "    Assumes a column `timestamp` (or override via timestamp_col).\n",
    "    \"\"\"\n",
    "    def __init__(self, timestamp_col=\"timestamp\",\n",
    "                 add_hour=True, add_week=True, add_doy=True):\n",
    "        self.timestamp_col = timestamp_col\n",
    "        self.add_hour = add_hour\n",
    "        self.add_week = add_week\n",
    "        self.add_doy = add_doy\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        if self.timestamp_col not in X.columns:\n",
    "            raise KeyError(f\"{self.timestamp_col} not in columns\")\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        df = X.copy()\n",
    "        t = pd.to_datetime(df[self.timestamp_col], errors=\"raise\")\n",
    "\n",
    "        if self.add_hour:\n",
    "            h = t.dt.hour + t.dt.minute/60.0\n",
    "            df[\"hour_sin\"] = np.sin(2*np.pi*h/24.0)\n",
    "            df[\"hour_cos\"] = np.cos(2*np.pi*h/24.0)\n",
    "\n",
    "        if self.add_week:\n",
    "            dow = t.dt.dayofweek.astype(float)  # 0=Mon\n",
    "            df[\"dow_sin\"] = np.sin(2*np.pi*dow/7.0)\n",
    "            df[\"dow_cos\"] = np.cos(2*np.pi*dow/7.0)\n",
    "\n",
    "        if self.add_doy:\n",
    "            doy = t.dt.dayofyear.astype(float)\n",
    "            df[\"doy_sin\"] = np.sin(2*np.pi*doy/365.0)   # 365 is fine for our purpose\n",
    "            df[\"doy_cos\"] = np.cos(2*np.pi*doy/365.0)\n",
    "\n",
    "        return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "9f2b13be",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_gam_pipeline(fitted_pipeline: Pipeline, X: pd.DataFrame, y: pd.Series,\n",
    "                          id_cols: Sequence[str] = (\"timestamp\",\"city\")) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Apply a fitted [preprocess -> (Q)GAM] and return a tidy frame with:\n",
    "        id_cols ∩ cols, y_true, y_pred, ME\n",
    "    \"\"\"\n",
    "    # push through whole pipeline (model's transform attaches y_pred, ME)\n",
    "    Z = fitted_pipeline.transform(pd.concat([X, y.rename(\"tons_co2\")], axis=1))\n",
    "    cols = [c for c in id_cols if c in Z.columns] + [\"y_pred\",\"ME\"]\n",
    "    out = pd.DataFrame(index=Z.index)\n",
    "    for c in cols:\n",
    "        out[c] = Z[c]\n",
    "    out[\"y_true\"] = y.values\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "fed0f521",
   "metadata": {},
   "outputs": [],
   "source": [
    "def metrics_from_frame(df: pd.DataFrame) -> dict:\n",
    "    y_true = df[\"y_true\"].to_numpy(float)\n",
    "    y_pred = df[\"y_pred\"].to_numpy(float)\n",
    "    return {\n",
    "        \"r2\": float(r2_score(y_true, y_pred)),\n",
    "        \"rmse\": float(root_mean_squared_error(y_true, y_pred)),\n",
    "        \"mae\": float(mean_absolute_error(y_true, y_pred)),\n",
    "        \"n_obs\": int(len(df)),\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "40be9b7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper\n",
    "def _sum_terms(terms):\n",
    "    # terms: list like [s(0), l(1), ...]\n",
    "    return reduce(add, terms)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "acc458dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Log1pColumns(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, columns):\n",
    "        self.columns = list(columns)\n",
    "    def fit(self, X, y=None):\n",
    "        missing = [c for c in self.columns if c not in X.columns]\n",
    "        if missing: raise KeyError(f\"Missing for log1p: {missing}\")\n",
    "        return self\n",
    "    def transform(self, X):\n",
    "        df = X.copy()\n",
    "        for c in self.columns:\n",
    "            df[f\"{c}_log1p\"] = np.log1p(df[c].astype(float).clip(lower=0))\n",
    "        return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c14bcc0",
   "metadata": {},
   "source": [
    "## Loading Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1c30f6b",
   "metadata": {},
   "source": [
    "#### Directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "ba754c05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DIRECTORIES AND PATHS\n",
    "# This is a redundant code block, but it is included as a reminder of the directory variables.\n",
    "base_data_directory = \"data\"  # Base directory where the dataframes will be saved\n",
    "hitachi_data_directory = os.path.join(base_data_directory, \"hitachi_copy\")  # Directory where the dataframes will be saved\n",
    "marginal_emissions_development_directory = os.path.join(base_data_directory, \"marginal_emissions_development\")  # Directory for marginal emissions development data\n",
    "marginal_emissions_results_directory = os.path.join(marginal_emissions_development_directory, \"results\")\n",
    "marginal_emissions_logs_directory = os.path.join(marginal_emissions_development_directory, \"logs\")\n",
    "\n",
    "marginal_emissions_prefix = \"marginal_emissions_results\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "4e6996bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "Contents of 'data/hitachi_copy' and subdirectories:\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "  - .DS_Store\n",
      "  - customers_20250714_1401.parquet\n",
      "  - grid_readings_20250714_1401.parquet\n",
      "  - grid_readings_20250714_1401_processed.parquet\n",
      "  - grid_readings_20250714_1401_processed_half_hourly.parquet\n",
      "  - weather_20250714_1401.parquet\n",
      "  - weather_20250714_1401_processed.parquet\n",
      "  - weather_and_grid_data_half-hourly_20250714_1401.parquet\n",
      "  - weather_data_combined_20250714_1401.parquet\n",
      "  - meter_primary_files/.DS_Store\n",
      "  - meter_primary_files/meter_readings_2021_20250714_2015.parquet\n",
      "  - meter_primary_files/meter_readings_2021_20250714_2015_formatted.parquet\n",
      "  - meter_primary_files/meter_readings_2021_Q4_20250714_2015_formatted.parquet\n",
      "  - meter_primary_files/meter_readings_2022_20250714_2324.parquet\n",
      "  - meter_primary_files/meter_readings_2022_20250714_2324_formatted.parquet\n",
      "  - meter_primary_files/meter_readings_2022_Q1_20250714_2324_formatted.parquet\n",
      "  - meter_primary_files/meter_readings_2022_Q2_20250714_2324_formatted.parquet\n",
      "  - meter_primary_files/meter_readings_2022_Q3_20250714_2324_formatted.parquet\n",
      "  - meter_primary_files/meter_readings_2022_Q4_20250714_2324_formatted.parquet\n",
      "  - meter_primary_files/meter_readings_2023_20250714_2039.parquet\n",
      "  - meter_primary_files/meter_readings_2023_20250714_2039_formatted.parquet\n",
      "  - meter_primary_files/meter_readings_2023_Q1_20250714_2039_formatted.parquet\n",
      "  - meter_primary_files/meter_readings_all_years_20250714_formatted.parquet\n",
      "  - meter_primary_files/meter_readings_delhi_2021_20250714_2015_formatted.parquet\n",
      "  - meter_primary_files/meter_readings_delhi_2021_Q4_20250714_2015_formatted.parquet\n",
      "  - meter_primary_files/meter_readings_delhi_2022_20250714_2324_formatted.parquet\n",
      "  - meter_primary_files/meter_readings_delhi_2022_Q1_20250714_2324_formatted.parquet\n",
      "  - meter_primary_files/meter_readings_delhi_2022_Q2_20250714_2324_formatted.parquet\n",
      "  - meter_primary_files/meter_readings_delhi_2022_Q3_20250714_2324_formatted.parquet\n",
      "  - meter_primary_files/meter_readings_mumbai_2022_20250714_2324_formatted.parquet\n",
      "  - meter_primary_files/meter_readings_mumbai_2022_Q3_20250714_2324_formatted.parquet\n",
      "  - meter_primary_files/meter_readings_mumbai_2022_Q4_20250714_2324_formatted.parquet\n",
      "  - meter_primary_files/meter_readings_mumbai_2023_20250714_2039_formatted.parquet\n",
      "  - meter_primary_files/meter_readings_mumbai_2023_Q1_20250714_2039_formatted.parquet\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"-\" * 120)\n",
    "print(f\"Contents of '{hitachi_data_directory}' and subdirectories:\\n\" + \"-\" * 120)\n",
    "for root, dirs, files in os.walk(hitachi_data_directory):\n",
    "    for f in sorted(files):\n",
    "        rel_dir = os.path.relpath(root, hitachi_data_directory)\n",
    "        rel_file = os.path.join(rel_dir, f) if rel_dir != \".\" else f\n",
    "        print(f\"  - {rel_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b38bf5a",
   "metadata": {},
   "source": [
    "#### File Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "8269b795",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cleaned weather data\n",
    "base_file = \"weather_and_grid_data_half-hourly_20250714_1401\"\n",
    "\n",
    "train_file = \"marginal_emissions_estimation_20250714_1401_train_data\"\n",
    "validation_file = \"marginal_emissions_estimation_20250714_1401_validation_data\"\n",
    "test_file = \"marginal_emissions_estimation_20250714_1401_test_data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "90515dbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_filepath = os.path.join(hitachi_data_directory, base_file + \".parquet\")\n",
    "\n",
    "train_filepath = os.path.join(marginal_emissions_development_directory, train_file + \".parquet\")\n",
    "validation_filepath = os.path.join(marginal_emissions_development_directory, validation_file + \".parquet\")\n",
    "test_filepath = os.path.join(marginal_emissions_development_directory, test_file + \".parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a9e2c01",
   "metadata": {},
   "source": [
    "#### Load and Look at Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "0da9f9c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_pldf = pl.read_parquet(base_filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "fc6df3d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_pldf = pl.read_parquet(train_filepath)\n",
    "validation_pldf = pl.read_parquet(validation_filepath)\n",
    "test_pldf = pl.read_parquet(test_filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "70726a07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "Sample rows of prepared dataset [train_pldf]:\n",
      "------------------------------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (8, 30)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>timestamp</th><th>city</th><th>land_latitude</th><th>land_longitude</th><th>wind_speed_mps</th><th>wind_direction_meteorological</th><th>temperature_celsius</th><th>precipitation_mm</th><th>surface_net_solar_radiation_kWh_per_m2</th><th>surface_solar_radiation_downwards_kWh_per_m2</th><th>surface_net_solar_radiation_joules_per_m2</th><th>surface_solar_radiation_downwards_joules_per_m2</th><th>total_cloud_cover</th><th>high_cloud_cover</th><th>medium_cloud_cover</th><th>low_cloud_cover</th><th>thermal_generation</th><th>gas_generation</th><th>hydro_generation</th><th>nuclear_generation</th><th>renewable_generation</th><th>total_generation</th><th>demand_met</th><th>non_renewable_generation</th><th>tons_co2</th><th>g_co2_per_kwh</th><th>tons_co2_per_mwh</th><th>wind_dir_cardinal_8</th><th>wind_dir_cardinal_16</th><th>wind_dir_cardinal_4</th></tr><tr><td>datetime[μs, Asia/Kolkata]</td><td>cat</td><td>f64</td><td>f64</td><td>f32</td><td>f32</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>str</td><td>str</td><td>str</td></tr></thead><tbody><tr><td>2021-10-08 15:00:00 IST</td><td>&quot;delhi&quot;</td><td>28.6</td><td>76.94</td><td>2.140953</td><td>25.835144</td><td>26.379761</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>114385.583333</td><td>7599.0</td><td>20366.416667</td><td>4383.75</td><td>23636.666667</td><td>170371.416667</td><td>169058.166667</td><td>146734.75</td><td>57438.4312</td><td>674.281609</td><td>0.674282</td><td>&quot;NE&quot;</td><td>&quot;NNE&quot;</td><td>&quot;N&quot;</td></tr><tr><td>2022-04-02 07:30:00 IST</td><td>&quot;delhi&quot;</td><td>28.5</td><td>76.94</td><td>1.252711</td><td>269.7388</td><td>37.496246</td><td>0.0</td><td>379635.698365</td><td>478384.166353</td><td>1.3667e6</td><td>1.7222e6</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>148525.25</td><td>3716.583333</td><td>17398.083333</td><td>4775.583333</td><td>7029.416667</td><td>181444.916667</td><td>179586.333333</td><td>174415.5</td><td>73197.73305</td><td>806.829505</td><td>0.80683</td><td>&quot;W&quot;</td><td>&quot;W&quot;</td><td>&quot;W&quot;</td></tr><tr><td>2023-06-03 02:00:00 IST</td><td>&quot;delhi&quot;</td><td>28.5</td><td>77.24</td><td>1.250598</td><td>4.888458</td><td>26.310791</td><td>0.0</td><td>31047.207561</td><td>37781.735594</td><td>111762.140077</td><td>136002.537948</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>152663.333333</td><td>2826.083333</td><td>18549.166667</td><td>4306.75</td><td>14895.0</td><td>193240.333333</td><td>191291.5</td><td>178345.333333</td><td>75013.44475</td><td>776.375407</td><td>0.776375</td><td>&quot;N&quot;</td><td>&quot;N&quot;</td><td>&quot;N&quot;</td></tr><tr><td>2021-12-22 03:30:00 IST</td><td>&quot;delhi&quot;</td><td>28.5</td><td>77.04</td><td>0.308357</td><td>305.025024</td><td>12.031494</td><td>0.0</td><td>31733.148774</td><td>38559.853892</td><td>114239.335587</td><td>138815.474012</td><td>0.477142</td><td>0.477142</td><td>0.0</td><td>0.0</td><td>107928.25</td><td>2750.333333</td><td>7460.0</td><td>5760.916667</td><td>496.5</td><td>124396.0</td><td>122301.916667</td><td>123899.5</td><td>53201.4684</td><td>855.356809</td><td>0.855357</td><td>&quot;NW&quot;</td><td>&quot;NW&quot;</td><td>&quot;W&quot;</td></tr><tr><td>2022-09-01 18:00:00 IST</td><td>&quot;delhi&quot;</td><td>28.8</td><td>77.24</td><td>1.668718</td><td>167.790649</td><td>28.773071</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.998016</td><td>0.998016</td><td>0.085846</td><td>0.035889</td><td>131903.5</td><td>3079.083333</td><td>33746.333333</td><td>4913.25</td><td>8871.166667</td><td>182513.333333</td><td>180045.666667</td><td>173642.166667</td><td>64956.17895</td><td>711.801991</td><td>0.711802</td><td>&quot;S&quot;</td><td>&quot;SSE&quot;</td><td>&quot;S&quot;</td></tr><tr><td>2021-02-04 21:00:00 IST</td><td>&quot;delhi&quot;</td><td>28.6</td><td>77.04</td><td>1.573774</td><td>337.806122</td><td>9.36554</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.895966</td><td>0.0</td><td>0.0</td><td>0.895966</td><td>128295.166667</td><td>4127.333333</td><td>9463.166667</td><td>3943.333333</td><td>6764.333333</td><td>152593.333333</td><td>153224.166667</td><td>145829.0</td><td>63434.0552</td><td>831.422076</td><td>0.831422</td><td>&quot;N&quot;</td><td>&quot;NNW&quot;</td><td>&quot;N&quot;</td></tr><tr><td>2022-07-02 19:00:00 IST</td><td>&quot;mumbai&quot;</td><td>18.6</td><td>72.97</td><td>5.224999</td><td>224.165146</td><td>25.368011</td><td>0.262992</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.986664</td><td>0.949738</td><td>0.348267</td><td>0.412567</td><td>124265.416667</td><td>2436.416667</td><td>25907.166667</td><td>5394.5</td><td>17885.5</td><td>175889.0</td><td>173530.916667</td><td>158003.5</td><td>61090.30475</td><td>694.634387</td><td>0.694634</td><td>&quot;SW&quot;</td><td>&quot;SW&quot;</td><td>&quot;S&quot;</td></tr><tr><td>2023-10-31 04:00:00 IST</td><td>&quot;mumbai&quot;</td><td>19.2</td><td>72.77</td><td>1.930534</td><td>101.093933</td><td>28.272888</td><td>0.0</td><td>121951.73787</td><td>135923.404761</td><td>439026.256331</td><td>489324.257141</td><td>0.016663</td><td>0.0</td><td>0.016663</td><td>0.0</td><td>139990.416667</td><td>2156.25</td><td>8066.916667</td><td>5397.833333</td><td>4847.5</td><td>160458.916667</td><td>158748.5</td><td>155611.416667</td><td>68688.48725</td><td>856.150478</td><td>0.85615</td><td>&quot;E&quot;</td><td>&quot;E&quot;</td><td>&quot;E&quot;</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (8, 30)\n",
       "┌────────────┬────────┬────────────┬───────────┬───┬───────────┬───────────┬───────────┬───────────┐\n",
       "│ timestamp  ┆ city   ┆ land_latit ┆ land_long ┆ … ┆ tons_co2_ ┆ wind_dir_ ┆ wind_dir_ ┆ wind_dir_ │\n",
       "│ ---        ┆ ---    ┆ ude        ┆ itude     ┆   ┆ per_mwh   ┆ cardinal_ ┆ cardinal_ ┆ cardinal_ │\n",
       "│ datetime[μ ┆ cat    ┆ ---        ┆ ---       ┆   ┆ ---       ┆ 8         ┆ 16        ┆ 4         │\n",
       "│ s, Asia/Ko ┆        ┆ f64        ┆ f64       ┆   ┆ f64       ┆ ---       ┆ ---       ┆ ---       │\n",
       "│ lkata]     ┆        ┆            ┆           ┆   ┆           ┆ str       ┆ str       ┆ str       │\n",
       "╞════════════╪════════╪════════════╪═══════════╪═══╪═══════════╪═══════════╪═══════════╪═══════════╡\n",
       "│ 2021-10-08 ┆ delhi  ┆ 28.6       ┆ 76.94     ┆ … ┆ 0.674282  ┆ NE        ┆ NNE       ┆ N         │\n",
       "│ 15:00:00   ┆        ┆            ┆           ┆   ┆           ┆           ┆           ┆           │\n",
       "│ IST        ┆        ┆            ┆           ┆   ┆           ┆           ┆           ┆           │\n",
       "│ 2022-04-02 ┆ delhi  ┆ 28.5       ┆ 76.94     ┆ … ┆ 0.80683   ┆ W         ┆ W         ┆ W         │\n",
       "│ 07:30:00   ┆        ┆            ┆           ┆   ┆           ┆           ┆           ┆           │\n",
       "│ IST        ┆        ┆            ┆           ┆   ┆           ┆           ┆           ┆           │\n",
       "│ 2023-06-03 ┆ delhi  ┆ 28.5       ┆ 77.24     ┆ … ┆ 0.776375  ┆ N         ┆ N         ┆ N         │\n",
       "│ 02:00:00   ┆        ┆            ┆           ┆   ┆           ┆           ┆           ┆           │\n",
       "│ IST        ┆        ┆            ┆           ┆   ┆           ┆           ┆           ┆           │\n",
       "│ 2021-12-22 ┆ delhi  ┆ 28.5       ┆ 77.04     ┆ … ┆ 0.855357  ┆ NW        ┆ NW        ┆ W         │\n",
       "│ 03:30:00   ┆        ┆            ┆           ┆   ┆           ┆           ┆           ┆           │\n",
       "│ IST        ┆        ┆            ┆           ┆   ┆           ┆           ┆           ┆           │\n",
       "│ 2022-09-01 ┆ delhi  ┆ 28.8       ┆ 77.24     ┆ … ┆ 0.711802  ┆ S         ┆ SSE       ┆ S         │\n",
       "│ 18:00:00   ┆        ┆            ┆           ┆   ┆           ┆           ┆           ┆           │\n",
       "│ IST        ┆        ┆            ┆           ┆   ┆           ┆           ┆           ┆           │\n",
       "│ 2021-02-04 ┆ delhi  ┆ 28.6       ┆ 77.04     ┆ … ┆ 0.831422  ┆ N         ┆ NNW       ┆ N         │\n",
       "│ 21:00:00   ┆        ┆            ┆           ┆   ┆           ┆           ┆           ┆           │\n",
       "│ IST        ┆        ┆            ┆           ┆   ┆           ┆           ┆           ┆           │\n",
       "│ 2022-07-02 ┆ mumbai ┆ 18.6       ┆ 72.97     ┆ … ┆ 0.694634  ┆ SW        ┆ SW        ┆ S         │\n",
       "│ 19:00:00   ┆        ┆            ┆           ┆   ┆           ┆           ┆           ┆           │\n",
       "│ IST        ┆        ┆            ┆           ┆   ┆           ┆           ┆           ┆           │\n",
       "│ 2023-10-31 ┆ mumbai ┆ 19.2       ┆ 72.77     ┆ … ┆ 0.85615   ┆ E         ┆ E         ┆ E         │\n",
       "│ 04:00:00   ┆        ┆            ┆           ┆   ┆           ┆           ┆           ┆           │\n",
       "│ IST        ┆        ┆            ┆           ┆   ┆           ┆           ┆           ┆           │\n",
       "└────────────┴────────┴────────────┴───────────┴───┴───────────┴───────────┴───────────┴───────────┘"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Schema([('timestamp', Datetime(time_unit='us', time_zone='Asia/Kolkata')),\n",
       "        ('city', Categorical(ordering='physical')),\n",
       "        ('land_latitude', Float64),\n",
       "        ('land_longitude', Float64),\n",
       "        ('wind_speed_mps', Float32),\n",
       "        ('wind_direction_meteorological', Float32),\n",
       "        ('temperature_celsius', Float64),\n",
       "        ('precipitation_mm', Float64),\n",
       "        ('surface_net_solar_radiation_kWh_per_m2', Float64),\n",
       "        ('surface_solar_radiation_downwards_kWh_per_m2', Float64),\n",
       "        ('surface_net_solar_radiation_joules_per_m2', Float64),\n",
       "        ('surface_solar_radiation_downwards_joules_per_m2', Float64),\n",
       "        ('total_cloud_cover', Float32),\n",
       "        ('high_cloud_cover', Float32),\n",
       "        ('medium_cloud_cover', Float32),\n",
       "        ('low_cloud_cover', Float32),\n",
       "        ('thermal_generation', Float64),\n",
       "        ('gas_generation', Float64),\n",
       "        ('hydro_generation', Float64),\n",
       "        ('nuclear_generation', Float64),\n",
       "        ('renewable_generation', Float64),\n",
       "        ('total_generation', Float64),\n",
       "        ('demand_met', Float64),\n",
       "        ('non_renewable_generation', Float64),\n",
       "        ('tons_co2', Float64),\n",
       "        ('g_co2_per_kwh', Float64),\n",
       "        ('tons_co2_per_mwh', Float64),\n",
       "        ('wind_dir_cardinal_8', String),\n",
       "        ('wind_dir_cardinal_16', String),\n",
       "        ('wind_dir_cardinal_4', String)])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Sample Rows of the DataFrame\n",
    "print(\"\\n\" + \"-\" * 120)\n",
    "print(f\"Sample rows of prepared dataset [train_pldf]:\\n\" + \"-\" * 120)\n",
    "display(train_pldf.sample(8))\n",
    "display(train_pldf.schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "5b217bb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Time Boundaries:\n",
      "\tStart Time: 2021-01-01 00:00:00+05:30\n",
      "\tEnd Time: 2023-12-31 23:30:00+05:30\n",
      "\tTotal Duration: 1094 days, 23:30:00\n",
      "\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# reminder of time boundaries\n",
    "print(\"Train Time Boundaries:\")\n",
    "print(f\"\\tStart Time: {train_pldf['timestamp'].min()}\")\n",
    "print(f\"\\tEnd Time: {train_pldf['timestamp'].max()}\")\n",
    "print(f\"\\tTotal Duration: {train_pldf['timestamp'].max() - train_pldf['timestamp'].min()}\")\n",
    "print(\"\\n\" + \"-\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "6f24f372",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Time Boundaries:\n",
      "\tStart Time: 2024-01-01 00:00:00+05:30\n",
      "\tEnd Time: 2024-05-31 23:30:00+05:30\n",
      "\tTotal Duration: 151 days, 23:30:00\n",
      "\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# reminder of time boundaries\n",
    "print(\"Validation Time Boundaries:\")\n",
    "print(f\"\\tStart Time: {validation_pldf['timestamp'].min()}\")\n",
    "print(f\"\\tEnd Time: {validation_pldf['timestamp'].max()}\")\n",
    "print(f\"\\tTotal Duration: {validation_pldf['timestamp'].max() - validation_pldf['timestamp'].min()}\")\n",
    "print(\"\\n\" + \"-\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "be74c5ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Time Boundaries:\n",
      "\tStart Time: 2024-06-01 00:00:00+05:30\n",
      "\tEnd Time: 2025-05-31 23:30:00+05:30\n",
      "\tTotal Duration: 364 days, 23:30:00\n",
      "\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# reminder of time boundaries\n",
    "print(\"Test Time Boundaries:\")\n",
    "print(f\"\\tStart Time: {test_pldf['timestamp'].min()}\")\n",
    "print(f\"\\tEnd Time: {test_pldf['timestamp'].max()}\")\n",
    "print(f\"\\tTotal Duration: {test_pldf['timestamp'].max() - test_pldf['timestamp'].min()}\")\n",
    "print(\"\\n\" + \"-\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "852bfc50",
   "metadata": {},
   "source": [
    "## Data Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53f57e2f",
   "metadata": {},
   "source": [
    "Core demand variable (for MEF)\n",
    "demand_met → spline/GAM smooth term.\n",
    "\n",
    "Weather variables\n",
    "wind_speed_mps → standardized.\n",
    "\n",
    "wind_direction_meteorological → cyclic transform:\n",
    "\n",
    "wind_dir_sin = sin(2π × wind_dir / 360)\n",
    "\n",
    "wind_dir_cos = cos(2π × wind_dir / 360)\n",
    "\n",
    "temperature_celsius → standardized.\n",
    "\n",
    "precipitation_mm → log1p transform → standardized (possible winsorize at 99.5% after log1p).\n",
    "\n",
    "surface_net_solar_radiation_joules_per_m2 → standardized (convert to kWh/m² if needed for interpretability).\n",
    "\n",
    "total_cloud_cover → standardized.\n",
    "\n",
    "Transform\n",
    "\n",
    "precipitation_mm → log1p\n",
    "\n",
    "wind_direction_meteorological → sin & cos cyclic encoding\n",
    "\n",
    "Optional Winsorize\n",
    "\n",
    "Precipitation after log1p (99.5% cutoff) to tame extreme monsoon spikes.\n",
    "\n",
    "Standardize continuous predictors (mean 0, sd 1).\n",
    "\n",
    "Drop any constant or NA-heavy columns (shouldn’t be an issue now).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6558533c",
   "metadata": {},
   "source": [
    "📐 Model setup\n",
    "Spline model\n",
    "Formula:\n",
    "\n",
    "CO₂_t = s(Q_t) + \\beta^\\top W_t + \\text{FE}_{month,hour} + \\epsilon_t\n",
    "Where:\n",
    "\n",
    "s(Q_t) = cubic spline on standardized demand (demand_met), 4–6 interior knots at demand quantiles.\n",
    "\n",
    "W_t = weather predictors (linear terms after scaling).\n",
    "\n",
    "Fixed effects for month & hour handle seasonality & diurnal cycles.\n",
    "\n",
    "MEF: derivative of spline w.r.t Q."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce3e79b3",
   "metadata": {},
   "source": [
    "GAM\n",
    "Formula:\n",
    "\n",
    "CO₂_t = s(Q_t) + s(\\text{solar}) + s(\\text{wind_speed}) + s(\\text{temperature}) + \\beta^\\top W_{\\text{linear}} + \\text{FE}_{month,hour} + \\epsilon_t\n",
    "Where:\n",
    "\n",
    "Smooth terms: demand_met, surface_net_solar_radiation_joules_per_m2, wind_speed_mps, temperature_celsius.\n",
    "\n",
    "Linear terms: cyclic wind dir (sin, cos), log1p_precip, total_cloud_cover.\n",
    "\n",
    "Same fixed effects as spline model.\n",
    "\n",
    "MEF: derivative of Q-smooth from GAM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "8b3f0e44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conversion to Pandas DataFrame for compatibility with existing code\n",
    "train_df = train_pldf.to_pandas()\n",
    "validation_df = validation_pldf.to_pandas()\n",
    "test_df = test_pldf.to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "47c177ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns in Train DataFrame:\n",
      "['timestamp', 'city', 'land_latitude', 'land_longitude', 'wind_speed_mps', 'wind_direction_meteorological', 'temperature_celsius', 'precipitation_mm', 'surface_net_solar_radiation_kWh_per_m2', 'surface_solar_radiation_downwards_kWh_per_m2', 'surface_net_solar_radiation_joules_per_m2', 'surface_solar_radiation_downwards_joules_per_m2', 'total_cloud_cover', 'high_cloud_cover', 'medium_cloud_cover', 'low_cloud_cover', 'thermal_generation', 'gas_generation', 'hydro_generation', 'nuclear_generation', 'renewable_generation', 'total_generation', 'demand_met', 'non_renewable_generation', 'tons_co2', 'g_co2_per_kwh', 'tons_co2_per_mwh', 'wind_dir_cardinal_8', 'wind_dir_cardinal_16', 'wind_dir_cardinal_4']\n",
      "\n",
      "Columns in Validation DataFrame:\n",
      "['timestamp', 'city', 'land_latitude', 'land_longitude', 'wind_speed_mps', 'wind_direction_meteorological', 'temperature_celsius', 'precipitation_mm', 'surface_net_solar_radiation_kWh_per_m2', 'surface_solar_radiation_downwards_kWh_per_m2', 'surface_net_solar_radiation_joules_per_m2', 'surface_solar_radiation_downwards_joules_per_m2', 'total_cloud_cover', 'high_cloud_cover', 'medium_cloud_cover', 'low_cloud_cover', 'thermal_generation', 'gas_generation', 'hydro_generation', 'nuclear_generation', 'renewable_generation', 'total_generation', 'demand_met', 'non_renewable_generation', 'tons_co2', 'g_co2_per_kwh', 'tons_co2_per_mwh', 'wind_dir_cardinal_8', 'wind_dir_cardinal_16', 'wind_dir_cardinal_4']\n",
      "\n",
      "Columns in Test DataFrame:\n",
      "['timestamp', 'city', 'land_latitude', 'land_longitude', 'wind_speed_mps', 'wind_direction_meteorological', 'temperature_celsius', 'precipitation_mm', 'surface_net_solar_radiation_kWh_per_m2', 'surface_solar_radiation_downwards_kWh_per_m2', 'surface_net_solar_radiation_joules_per_m2', 'surface_solar_radiation_downwards_joules_per_m2', 'total_cloud_cover', 'high_cloud_cover', 'medium_cloud_cover', 'low_cloud_cover', 'thermal_generation', 'gas_generation', 'hydro_generation', 'nuclear_generation', 'renewable_generation', 'total_generation', 'demand_met', 'non_renewable_generation', 'tons_co2', 'g_co2_per_kwh', 'tons_co2_per_mwh', 'wind_dir_cardinal_8', 'wind_dir_cardinal_16', 'wind_dir_cardinal_4']\n"
     ]
    }
   ],
   "source": [
    "print(\"Columns in Train DataFrame:\")\n",
    "print(train_df.columns.tolist())\n",
    "print(\"\\nColumns in Validation DataFrame:\")\n",
    "print(validation_df.columns.tolist())\n",
    "print(\"\\nColumns in Test DataFrame:\")\n",
    "print(test_df.columns.tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fb0e5e3",
   "metadata": {},
   "source": [
    "### Original Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "8f6dfe84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# marginal_emissions_results_directory\n",
    "# marginal_emissions_logs_directory\n",
    "\n",
    "# marginal_emissions_prefix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "f792b5ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = train_df.drop(columns=[\"tons_co2\"])\n",
    "y_train = train_df[\"tons_co2\"]\n",
    "X_val = validation_df.drop(columns=[\"tons_co2\"])\n",
    "y_val = validation_df[\"tons_co2\"]\n",
    "X_test = test_df.drop(columns=[\"tons_co2\"])\n",
    "y_test = test_df[\"tons_co2\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "53771b11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# New: a plain OLS regressor that builds q^1..q^4 and q×(weather) interactions,\n",
    "# then exposes predict(..., predict_type=\"ME\") for the exact derivative wrt q.\n",
    "\n",
    "\n",
    "class Poly4MERegressor(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self,\n",
    "                 y_var=\"tons_co2\",\n",
    "                 q_col=\"demand_met\",                # use RAW demand (not std)\n",
    "                 weather_cols_std=(\"wind_speed_mps_std\",\n",
    "                                   \"temperature_celsius_std\",\n",
    "                                   \"surface_net_solar_radiation_joules_per_m2_std\"),\n",
    "                 wind_dir_cols=(\"wind_dir_sin\",\"wind_dir_cos\"),\n",
    "                 extra_linear_cols=(\"precipitation_mm_log1p_std\",\n",
    "                                    \"total_cloud_cover_std\",\n",
    "                                    \"hour_sin\",\"hour_cos\",\n",
    "                                    \"dow_sin\",\"dow_cos\",\n",
    "                                    \"doy_sin\",\"doy_cos\",\n",
    "                                    \"is_weekend\"),\n",
    "                 add_interactions=True):\n",
    "        self.y_var = y_var\n",
    "        self.q_col = q_col\n",
    "        self.weather_cols_std = list(weather_cols_std)\n",
    "        self.wind_dir_cols = list(wind_dir_cols)\n",
    "        self.extra_linear_cols = list(extra_linear_cols)\n",
    "        self.add_interactions = bool(add_interactions)\n",
    "\n",
    "        self.model_ = None\n",
    "        self.term_names_ = None  # for convenience / introspection\n",
    "\n",
    "    def _design(self, X: pd.DataFrame) -> pd.DataFrame:\n",
    "        df = X.copy()\n",
    "        q = self.q_col\n",
    "\n",
    "        # polynomial in RAW q\n",
    "        df[f\"{q}_p1\"] = df[q].astype(float)\n",
    "        df[f\"{q}_p2\"] = df[q].astype(float)**2\n",
    "        df[f\"{q}_p3\"] = df[q].astype(float)**3\n",
    "        df[f\"{q}_p4\"] = df[q].astype(float)**4\n",
    "\n",
    "        # interactions q × standardized weather / wind_dir\n",
    "        inter_cols = []\n",
    "        if self.add_interactions:\n",
    "            for c in list(self.weather_cols_std) + list(self.wind_dir_cols):\n",
    "                name = f\"{q}_x_{c}\"\n",
    "                df[name] = df[q].astype(float) * df[c].astype(float)\n",
    "                inter_cols.append(name)\n",
    "\n",
    "        # stash names for predict/ME\n",
    "        self._poly_cols = [f\"{q}_p1\", f\"{q}_p2\", f\"{q}_p3\", f\"{q}_p4\"]\n",
    "        self._inter_cols = inter_cols\n",
    "        self._lin_cols = list(self.weather_cols_std) + list(self.wind_dir_cols) + list(self.extra_linear_cols)\n",
    "\n",
    "        return df\n",
    "\n",
    "    def fit(self, X: pd.DataFrame, y=None):\n",
    "        if y is None:\n",
    "            if self.y_var not in X.columns:\n",
    "                raise KeyError(f\"'{self.y_var}' missing in X\")\n",
    "            y_ser = X[self.y_var]\n",
    "        else:\n",
    "            y_ser = (y if isinstance(y, pd.Series) else y.iloc[:,0])\n",
    "        df = self._design(X)\n",
    "\n",
    "        # Build formula: y ~ q_p1 + q_p2 + q_p3 + q_p4 + (q×weather) + linear controls\n",
    "        rhs = self._poly_cols + self._inter_cols + self._lin_cols\n",
    "        # keep only columns that actually exist (e.g. some Fourier flags might be missing)\n",
    "        rhs = [c for c in rhs if c in df.columns]\n",
    "        formula = f\"{self.y_var} ~ \" + \" + \".join(rhs)\n",
    "\n",
    "        self.model_ = smf.ols(formula, data=df.assign(**{self.y_var: y_ser})).fit()\n",
    "        self.term_names_ = rhs\n",
    "        return self\n",
    "\n",
    "    def transform(self, X: pd.DataFrame) -> pd.DataFrame:\n",
    "        # Attach y_pred and ME (exact derivative wrt q)\n",
    "        if self.model_ is None:\n",
    "            raise RuntimeError(\"Model not fitted\")\n",
    "        df = self._design(X)\n",
    "        yhat = self.model_.predict(df)\n",
    "        me = self._me_from_df(df)\n",
    "        out = df.copy()\n",
    "        out[\"y_pred\"] = np.asarray(yhat, dtype=float)\n",
    "        out[\"ME\"] = np.asarray(me, dtype=float)\n",
    "        return out\n",
    "\n",
    "    def predict(self, X: pd.DataFrame, predict_type=\"ME\"):\n",
    "        df = self._design(X)\n",
    "        if predict_type.lower() == \"y\":\n",
    "            return self.model_.predict(df)\n",
    "        return self._me_from_df(df)\n",
    "\n",
    "    # --- exact derivative wrt q ---\n",
    "    def _me_from_df(self, df: pd.DataFrame) -> np.ndarray:\n",
    "        q = df[self.q_col].astype(float).to_numpy()\n",
    "        coefs = self.model_.params  # pandas Series (includes intercept)\n",
    "\n",
    "        # coefficients (missing defaults to 0.0 if regularization or dropped)\n",
    "        def c(name): return float(coefs[name]) if name in coefs.index else 0.0\n",
    "\n",
    "        q1 = c(f\"{self.q_col}_p1\")\n",
    "        q2 = c(f\"{self.q_col}_p2\")\n",
    "        q3 = c(f\"{self.q_col}_p3\")\n",
    "        q4 = c(f\"{self.q_col}_p4\")\n",
    "\n",
    "        # base polynomial derivative: β1 + 2β2 q + 3β3 q^2 + 4β4 q^3\n",
    "        me = (q1\n",
    "              + 2.0 * q2 * q\n",
    "              + 3.0 * q3 * (q**2)\n",
    "              + 4.0 * q4 * (q**3))\n",
    "\n",
    "        # + interactions: sum γ_c * feature_c\n",
    "        for cfeat in list(self.weather_cols_std) + list(self.wind_dir_cols):\n",
    "            γ = c(f\"{self.q_col}_x_{cfeat}\")\n",
    "            if γ != 0.0 and cfeat in df.columns:\n",
    "                me = me + γ * df[cfeat].astype(float).to_numpy()\n",
    "\n",
    "        return me\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "f4db1cab",
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocess = build_mef_preprocess_pipeline_2(timestamp_col=\"timestamp\", use_winsorize_precip=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "e42c7873",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "poly4 = Poly4MERegressor(\n",
    "    y_var=\"tons_co2\",\n",
    "    q_col=\"demand_met\",  # raw q\n",
    "    weather_cols_std=(\"wind_speed_mps_std\",\n",
    "                      \"temperature_celsius_std\",\n",
    "                      \"surface_net_solar_radiation_joules_per_m2_std\"),\n",
    "    wind_dir_cols=(\"wind_dir_sin\",\"wind_dir_cos\"),\n",
    "    extra_linear_cols=(\"precipitation_mm_log1p_std\",\n",
    "                       \"total_cloud_cover_std\",\n",
    "                       \"hour_sin\",\"hour_cos\",\n",
    "                       \"dow_sin\",\"dow_cos\",\n",
    "                       \"doy_sin\",\"doy_cos\",\n",
    "                       \"is_weekend\"),\n",
    "    add_interactions=True\n",
    ")\n",
    "\n",
    "poly_pipeline = Pipeline([\n",
    "    (\"PreprocessMEF\", preprocess),\n",
    "    (\"Poly4ME\", poly4),\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "fc8052d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN: {'r2': 0.4823478103208152, 'rmse': 5632.8721326194045, 'mae': 4563.779819447094, 'n_obs': 2365200}\n",
      "VAL: {'r2': -1.004314431223058, 'rmse': 8447.794490421793, 'mae': 7504.033985986517, 'n_obs': 328320}\n"
     ]
    }
   ],
   "source": [
    "# Fit on train (must pass y so the estimator can build and fit OLS)\n",
    "poly_pipeline.fit(pd.concat([X_train, y_train.rename(\"tons_co2\")], axis=1), y_train)\n",
    "\n",
    "# Evaluate\n",
    "train_eval = evaluate_gam_pipeline(poly_pipeline, X_train, y_train)\n",
    "val_eval  = evaluate_gam_pipeline(poly_pipeline, X_val,  y_val)\n",
    "\n",
    "\n",
    "print(\"TRAIN:\", metrics_from_frame(train_eval))\n",
    "print(\"VAL:\",  metrics_from_frame(val_eval))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c143bbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:                      y   R-squared:                       0.482\n",
      "Model:                            OLS   Adj. R-squared:                  0.482\n",
      "Method:                 Least Squares   F-statistic:                 1.102e+06\n",
      "Date:                Sun, 17 Aug 2025   Prob (F-statistic):               0.00\n",
      "Time:                        12:25:58   Log-Likelihood:            -2.3783e+07\n",
      "No. Observations:             2365200   AIC:                         4.757e+07\n",
      "Df Residuals:                 2365197   BIC:                         4.757e+07\n",
      "Df Model:                           2                                         \n",
      "Covariance Type:            nonrobust                                         \n",
      "==============================================================================\n",
      "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "const       1.631e-15   1.47e-18   1110.902      0.000    1.63e-15    1.63e-15\n",
      "x1         -2.369e-10   2.13e-13  -1110.902      0.000   -2.37e-10   -2.36e-10\n",
      "x2          7.732e-06   6.96e-09   1110.902      0.000    7.72e-06    7.75e-06\n",
      "x3          -4.77e-11   7.87e-14   -606.405      0.000   -4.79e-11   -4.75e-11\n",
      "x4          8.891e-17   2.21e-19    402.278      0.000    8.85e-17    8.93e-17\n",
      "x5         -1.893e-10    1.7e-13  -1110.902      0.000    -1.9e-10   -1.89e-10\n",
      "x6         -4.253e-10   3.83e-13  -1110.902      0.000   -4.26e-10   -4.25e-10\n",
      "x7         -1.861e-10   1.68e-13  -1110.902      0.000   -1.86e-10   -1.86e-10\n",
      "x8          1.102e-11   9.92e-15   1110.902      0.000     1.1e-11     1.1e-11\n",
      "x9           4.63e-11   4.17e-14   1110.902      0.000    4.62e-11    4.64e-11\n",
      "x10        -1.388e-15   1.25e-18  -1110.902      0.000   -1.39e-15   -1.39e-15\n",
      "x11        -3.481e-15   3.13e-18  -1110.902      0.000   -3.49e-15   -3.47e-15\n",
      "x12        -1.237e-15   1.11e-18  -1110.902      0.000   -1.24e-15   -1.23e-15\n",
      "x13         9.316e-17   8.39e-20   1110.902      0.000     9.3e-17    9.33e-17\n",
      "x14         5.499e-16   4.95e-19   1110.902      0.000    5.49e-16    5.51e-16\n",
      "x15        -9.589e-16   8.63e-19  -1110.902      0.000   -9.61e-16   -9.57e-16\n",
      "x16        -3.848e-16   3.46e-19  -1110.902      0.000   -3.85e-16   -3.84e-16\n",
      "x17         1.716e-15   1.54e-18   1110.902      0.000    1.71e-15    1.72e-15\n",
      "x18         1.981e-15   1.78e-18   1110.902      0.000    1.98e-15    1.98e-15\n",
      "x19        -1.546e-16   1.39e-19  -1110.902      0.000   -1.55e-16   -1.54e-16\n",
      "x20         2.256e-16   2.03e-19   1110.902      0.000    2.25e-16    2.26e-16\n",
      "x21        -1.481e-15   1.33e-18  -1110.902      0.000   -1.48e-15   -1.48e-15\n",
      "x22          2.33e-15    2.1e-18   1110.902      0.000    2.33e-15    2.33e-15\n",
      "x23         5.358e-16   4.82e-19   1110.902      0.000    5.35e-16    5.37e-16\n",
      "==============================================================================\n",
      "Omnibus:                   112072.790   Durbin-Watson:                   0.000\n",
      "Prob(Omnibus):                  0.000   Jarque-Bera (JB):           128879.411\n",
      "Skew:                          -0.571   Prob(JB):                         0.00\n",
      "Kurtosis:                       2.960   Cond. No.                     2.09e+24\n",
      "==============================================================================\n",
      "\n",
      "Notes:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
      "[2] The condition number is large, 2.09e+24. This might indicate that there are\n",
      "strong multicollinearity or other numerical problems.\n",
      "TRAIN | r2=0.482 rmse=5632.9 mae=4563.8 n=2,365,200\n",
      "VAL   | r2=-1.004 rmse=8447.8 mae=7504.0 n=328,320\n"
     ]
    }
   ],
   "source": [
    "# --- 1) time features (Fourier) + weekend ---\n",
    "def add_time_features(df, ts_col=\"timestamp\"):\n",
    "    out = df.copy()\n",
    "    t = pd.to_datetime(out[ts_col], errors=\"raise\")\n",
    "    # hour-of-day as fractional hour\n",
    "    h = t.dt.hour + t.dt.minute/60.0\n",
    "    out[\"hour_sin\"] = np.sin(2*np.pi*h/24.0)\n",
    "    out[\"hour_cos\"] = np.cos(2*np.pi*h/24.0)\n",
    "    # day-of-week: 0=Mon..6=Sun\n",
    "    dow = t.dt.dayofweek.astype(float)\n",
    "    out[\"dow_sin\"] = np.sin(2*np.pi*dow/7.0)\n",
    "    out[\"dow_cos\"] = np.cos(2*np.pi*dow/7.0)\n",
    "    # day-of-year\n",
    "    doy = t.dt.dayofyear.astype(float)\n",
    "    out[\"doy_sin\"] = np.sin(2*np.pi*doy/365.0)\n",
    "    out[\"doy_cos\"] = np.cos(2*np.pi*doy/365.0)\n",
    "    # weekend flag\n",
    "    out[\"is_weekend\"] = (t.dt.dayofweek >= 5).astype(int)\n",
    "    return out\n",
    "\n",
    "X_train_f = add_time_features(X_train)\n",
    "X_val_f   = add_time_features(X_val)\n",
    "\n",
    "# --- 2) wind direction cyclic encoding ---\n",
    "def add_wind_dir_cyclic(df, col=\"wind_direction_meteorological\"):\n",
    "    out = df.copy()\n",
    "    theta = np.deg2rad(out[col].astype(float))\n",
    "    out[\"wind_dir_sin\"] = np.sin(theta)\n",
    "    out[\"wind_dir_cos\"] = np.cos(theta)\n",
    "    return out\n",
    "\n",
    "X_train_f = add_wind_dir_cyclic(X_train_f)\n",
    "X_val_f   = add_wind_dir_cyclic(X_val_f)\n",
    "\n",
    "# --- 3) winsorize precip on TRAIN, then log1p everywhere ---\n",
    "precip_col = \"precipitation_mm\"\n",
    "winsor_upper = 0.995  # 99.5th pct on train\n",
    "cap_hi = np.nanpercentile(X_train_f[precip_col].astype(float), winsor_upper*100.0)\n",
    "cap_lo = 0.0  # keep >= 0\n",
    "\n",
    "def clip_and_log1p_precip(df):\n",
    "    out = df.copy()\n",
    "    out[precip_col] = out[precip_col].astype(float).clip(lower=cap_lo, upper=cap_hi)\n",
    "    out[\"precipitation_mm_log1p\"] = np.log1p(out[precip_col])\n",
    "    return out\n",
    "\n",
    "X_train_f = clip_and_log1p_precip(X_train_f)\n",
    "X_val_f   = clip_and_log1p_precip(X_val_f)\n",
    "\n",
    "# --- 4) standardize continuous *controls* on TRAIN; keep RAW demand for polynomial ---\n",
    "to_std = [\n",
    "    \"wind_speed_mps\",\n",
    "    \"temperature_celsius\",\n",
    "    \"surface_net_solar_radiation_joules_per_m2\",\n",
    "    \"precipitation_mm_log1p\",\n",
    "    \"total_cloud_cover\",\n",
    "]\n",
    "mu = {c: float(X_train_f[c].mean()) for c in to_std}\n",
    "sd = {c: float(X_train_f[c].std(ddof=0) or 1.0) for c in to_std}  # avoid /0\n",
    "\n",
    "def add_std(df):\n",
    "    out = df.copy()\n",
    "    for c in to_std:\n",
    "        out[f\"{c}_std\"] = (out[c].astype(float) - mu[c]) / sd[c]\n",
    "    return out\n",
    "\n",
    "X_train_f = add_std(X_train_f)\n",
    "X_val_f   = add_std(X_val_f)\n",
    "\n",
    "# --- 5) polynomial in RAW demand q + 6) interactions with selected controls ---\n",
    "q = \"demand_met\"\n",
    "\n",
    "inter_features = [\n",
    "    \"wind_speed_mps_std\",\n",
    "    \"temperature_celsius_std\",\n",
    "    \"surface_net_solar_radiation_joules_per_m2_std\",\n",
    "    \"wind_dir_sin\",\n",
    "    \"wind_dir_cos\",\n",
    "]\n",
    "\n",
    "lin_controls = inter_features + [\n",
    "    \"precipitation_mm_log1p_std\",\n",
    "    \"total_cloud_cover_std\",\n",
    "    \"hour_sin\",\"hour_cos\",\"dow_sin\",\"dow_cos\",\"doy_sin\",\"doy_cos\",\"is_weekend\",\n",
    "]\n",
    "\n",
    "def build_design(df):\n",
    "    out = df.copy()\n",
    "    # poly in raw q\n",
    "    out[f\"{q}_p1\"] = out[q].astype(float)\n",
    "    out[f\"{q}_p2\"] = out[q].astype(float)**2\n",
    "    out[f\"{q}_p3\"] = out[q].astype(float)**3\n",
    "    out[f\"{q}_p4\"] = out[q].astype(float)**4\n",
    "    # interactions q × feature\n",
    "    for c in inter_features:\n",
    "        if c in out.columns:\n",
    "            out[f\"{q}_x_{c}\"] = out[q].astype(float) * out[c].astype(float)\n",
    "    return out\n",
    "\n",
    "D_train = build_design(X_train_f)\n",
    "D_val   = build_design(X_val_f)\n",
    "\n",
    "# columns to feed OLS (only those present)\n",
    "poly_cols = [f\"{q}_p1\", f\"{q}_p2\", f\"{q}_p3\", f\"{q}_p4\"]\n",
    "inter_cols = [c for c in [f\"{q}_x_{c0}\" for c0 in inter_features] if c in D_train.columns]\n",
    "rhs_cols = [c for c in (poly_cols + inter_cols + lin_controls) if c in D_train.columns]\n",
    "\n",
    "# --- 7) fit OLS with named design matrix ---\n",
    "Xmat_train = sm.add_constant(D_train[rhs_cols], has_constant=\"add\")\n",
    "ols_model = sm.OLS(y_train.to_numpy(float), Xmat_train.to_numpy(float)).fit()\n",
    "coef = pd.Series(ols_model.params, index=Xmat_train.columns)\n",
    "print(ols_model.summary())\n",
    "\n",
    "# --- helper: exact ME from coefficients ---\n",
    "def me_from_df(df, coef_series):\n",
    "    qv = df[q].astype(float).to_numpy()\n",
    "    # pull safely (0 if absent)\n",
    "    def c(name): return float(coef_series.get(name, 0.0))\n",
    "    me = (\n",
    "        c(f\"{q}_p1\")\n",
    "        + 2.0*c(f\"{q}_p2\")*qv\n",
    "        + 3.0*c(f\"{q}_p3\")*(qv**2)\n",
    "        + 4.0*c(f\"{q}_p4\")*(qv**3)\n",
    "    )\n",
    "    for cfeat in inter_features:\n",
    "        col = f\"{q}_x_{cfeat}\"\n",
    "        if col in df.columns:\n",
    "            me = me + c(col)*df[cfeat].astype(float).to_numpy()\n",
    "    return me\n",
    "\n",
    "# --- 8) predict ŷ and ME on each split ---\n",
    "def predict_split(D, y_true=None, name=\"split\"):\n",
    "    Xm = sm.add_constant(D[rhs_cols], has_constant=\"add\")\n",
    "    y_pred = np.asarray(Xm.to_numpy(float) @ coef.values, dtype=float)\n",
    "    me = me_from_df(D, coef)\n",
    "    out = pd.DataFrame({\n",
    "        \"y_pred\": y_pred,\n",
    "        \"ME\": me,\n",
    "    }, index=D.index)\n",
    "    if y_true is not None:\n",
    "        out[\"y_true\"] = y_true.values\n",
    "        print(\n",
    "            f\"{name.upper():5s} | r2={r2_score(out['y_true'], out['y_pred']):.3f} \"\n",
    "            f\"rmse={root_mean_squared_error(out['y_true'], out['y_pred']):.1f} \"\n",
    "            f\"mae={mean_absolute_error(out['y_true'], out['y_pred']):.1f} \"\n",
    "            f\"n={len(out):,}\"\n",
    "        )\n",
    "    return out\n",
    "\n",
    "train_out = predict_split(D_train, y_train, name=\"train\")\n",
    "val_out   = predict_split(D_val,   y_val,   name=\"val\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "da957219",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:                      y   R-squared:                       0.569\n",
      "Model:                            OLS   Adj. R-squared:                  0.569\n",
      "Method:                 Least Squares   F-statistic:                 5.215e+05\n",
      "Date:                Sun, 17 Aug 2025   Prob (F-statistic):               0.00\n",
      "Time:                        12:26:01   Log-Likelihood:            -2.3565e+07\n",
      "No. Observations:             2365200   AIC:                         4.713e+07\n",
      "Df Residuals:                 2365193   BIC:                         4.713e+07\n",
      "Df Model:                           6                                         \n",
      "Covariance Type:            nonrobust                                         \n",
      "==============================================================================\n",
      "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "const       7.335e-06   1.85e-08    396.719      0.000     7.3e-06    7.37e-06\n",
      "x1             0.4049      0.001    396.718      0.000       0.403       0.407\n",
      "x2          1.072e-08   1.18e-08      0.907      0.364   -1.24e-08    3.39e-08\n",
      "x3         -1.225e-12    3.4e-14    -35.990      0.000   -1.29e-12   -1.16e-12\n",
      "x4            -0.0153   2.65e-05   -576.906      0.000      -0.015      -0.015\n",
      "x5             0.0076   2.38e-05    317.916      0.000       0.008       0.008\n",
      "x6             0.0014   2.79e-05     49.757      0.000       0.001       0.001\n",
      "x7             0.0072    3.2e-05    225.951      0.000       0.007       0.007\n",
      "x8         -8.582e-06   2.14e-08   -401.325      0.000   -8.62e-06   -8.54e-06\n",
      "x9         -1.198e-06   3.13e-09   -383.190      0.000    -1.2e-06   -1.19e-06\n",
      "x10         8.323e-07   2.09e-09    398.582      0.000    8.28e-07    8.36e-07\n",
      "x11         2.028e-06   5.01e-09    404.710      0.000    2.02e-06    2.04e-06\n",
      "x12        -8.178e-06   2.06e-08   -397.940      0.000   -8.22e-06   -8.14e-06\n",
      "x13        -2.114e-07   4.29e-10   -492.519      0.000   -2.12e-07   -2.11e-07\n",
      "x14         1.791e-05   4.51e-08    397.068      0.000    1.78e-05     1.8e-05\n",
      "x15         1.101e-05   2.78e-08    396.168      0.000     1.1e-05    1.11e-05\n",
      "x16        -4.543e-08   1.22e-10   -372.081      0.000   -4.57e-08   -4.52e-08\n",
      "x17         1.342e-06    3.4e-09    395.261      0.000    1.34e-06    1.35e-06\n",
      "x18        -1.191e-05      3e-08   -396.328      0.000    -1.2e-05   -1.18e-05\n",
      "x19          9.95e-06   2.49e-08    399.922      0.000     9.9e-06       1e-05\n",
      "x20          1.72e-06   4.34e-09    396.093      0.000    1.71e-06    1.73e-06\n",
      "==============================================================================\n",
      "Omnibus:                    47435.604   Durbin-Watson:                   0.012\n",
      "Prob(Omnibus):                  0.000   Jarque-Bera (JB):            50334.713\n",
      "Skew:                          -0.356   Prob(JB):                         0.00\n",
      "Kurtosis:                       3.070   Cond. No.                     1.42e+18\n",
      "==============================================================================\n",
      "\n",
      "Notes:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
      "[2] The condition number is large, 1.42e+18. This might indicate that there are\n",
      "strong multicollinearity or other numerical problems.\n",
      "TRAIN | r2=0.569 rmse=5136.9 mae=4085.4 n=2,365,200\n",
      "VAL   | r2=-0.434 rmse=7144.6 mae=6022.7 n=328,320\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# --- 1) time features (Fourier) + weekend ---\n",
    "def add_time_features(df, ts_col=\"timestamp\"):\n",
    "    out = df.copy()\n",
    "    t = pd.to_datetime(out[ts_col], errors=\"raise\")\n",
    "    # hour-of-day as fractional hour\n",
    "    h = t.dt.hour + t.dt.minute/60.0\n",
    "    out[\"hour_sin\"] = np.sin(2*np.pi*h/24.0)\n",
    "    out[\"hour_cos\"] = np.cos(2*np.pi*h/24.0)\n",
    "    # day-of-week: 0=Mon..6=Sun\n",
    "    dow = t.dt.dayofweek.astype(float)\n",
    "    out[\"dow_sin\"] = np.sin(2*np.pi*dow/7.0)\n",
    "    out[\"dow_cos\"] = np.cos(2*np.pi*dow/7.0)\n",
    "    # day-of-year\n",
    "    doy = t.dt.dayofyear.astype(float)\n",
    "    out[\"doy_sin\"] = np.sin(2*np.pi*doy/365.0)\n",
    "    out[\"doy_cos\"] = np.cos(2*np.pi*doy/365.0)\n",
    "    # weekend flag\n",
    "    out[\"is_weekend\"] = (t.dt.dayofweek >= 5).astype(int)\n",
    "    return out\n",
    "\n",
    "X_train_f = add_time_features(X_train)\n",
    "X_val_f   = add_time_features(X_val)\n",
    "\n",
    "# --- 2) wind direction cyclic encoding ---\n",
    "def add_wind_dir_cyclic(df, col=\"wind_direction_meteorological\"):\n",
    "    out = df.copy()\n",
    "    theta = np.deg2rad(out[col].astype(float))\n",
    "    out[\"wind_dir_sin\"] = np.sin(theta)\n",
    "    out[\"wind_dir_cos\"] = np.cos(theta)\n",
    "    return out\n",
    "\n",
    "X_train_f = add_wind_dir_cyclic(X_train_f)\n",
    "X_val_f   = add_wind_dir_cyclic(X_val_f)\n",
    "\n",
    "# --- 3) winsorize precip on TRAIN, then log1p everywhere ---\n",
    "precip_col = \"precipitation_mm\"\n",
    "winsor_upper = 0.995  # 99.5th pct on train\n",
    "cap_hi = np.nanpercentile(X_train_f[precip_col].astype(float), winsor_upper*100.0)\n",
    "cap_lo = 0.0  # keep >= 0\n",
    "\n",
    "def clip_and_log1p_precip(df):\n",
    "    out = df.copy()\n",
    "    out[precip_col] = out[precip_col].astype(float).clip(lower=cap_lo, upper=cap_hi)\n",
    "    out[\"precipitation_mm_log1p\"] = np.log1p(out[precip_col])\n",
    "    return out\n",
    "\n",
    "X_train_f = clip_and_log1p_precip(X_train_f)\n",
    "X_val_f   = clip_and_log1p_precip(X_val_f)\n",
    "\n",
    "# --- 4) standardize continuous *controls* on TRAIN; keep RAW demand for polynomial ---\n",
    "to_std = [\n",
    "    \"wind_speed_mps\",\n",
    "    \"temperature_celsius\",\n",
    "    \"surface_net_solar_radiation_joules_per_m2\",\n",
    "    \"precipitation_mm_log1p\",\n",
    "    \"total_cloud_cover\",\n",
    "]\n",
    "mu = {c: float(X_train_f[c].mean()) for c in to_std}\n",
    "sd = {c: float(X_train_f[c].std(ddof=0) or 1.0) for c in to_std}  # avoid /0\n",
    "\n",
    "def add_std(df):\n",
    "    out = df.copy()\n",
    "    for c in to_std:\n",
    "        out[f\"{c}_std\"] = (out[c].astype(float) - mu[c]) / sd[c]\n",
    "    return out\n",
    "\n",
    "X_train_f = add_std(X_train_f)\n",
    "X_val_f   = add_std(X_val_f)\n",
    "\n",
    "# --- 5) polynomial in RAW demand q + 6) interactions with selected controls ---\n",
    "q = \"demand_met\"\n",
    "\n",
    "# --- interactions: now without wind speed ---\n",
    "inter_features = [\n",
    "    \"temperature_celsius_std\",\n",
    "    \"surface_net_solar_radiation_joules_per_m2_std\",\n",
    "    \"wind_dir_sin\",\n",
    "    \"wind_dir_cos\",\n",
    "]\n",
    "\n",
    "lin_controls = inter_features + [\n",
    "    \"precipitation_mm_log1p_std\",\n",
    "    \"total_cloud_cover_std\",\n",
    "    \"hour_sin\",\"hour_cos\",\"dow_sin\",\"dow_cos\",\"doy_sin\",\"doy_cos\",\"is_weekend\",\n",
    "]\n",
    "\n",
    "# --- degree 3 polynomial in raw q ---\n",
    "def build_design(df):\n",
    "    out = df.copy()\n",
    "    out[f\"{q}_p1\"] = out[q].astype(float)\n",
    "    out[f\"{q}_p2\"] = out[q].astype(float)**2\n",
    "    out[f\"{q}_p3\"] = out[q].astype(float)**3\n",
    "    for c in inter_features:\n",
    "        if c in out.columns:\n",
    "            out[f\"{q}_x_{c}\"] = out[q].astype(float) * out[c].astype(float)\n",
    "    return out\n",
    "\n",
    "D_train = build_design(X_train_f)\n",
    "D_val   = build_design(X_val_f)\n",
    "\n",
    "poly_cols = [f\"{q}_p1\", f\"{q}_p2\", f\"{q}_p3\"]\n",
    "inter_cols = [c for c in [f\"{q}_x_{c0}\" for c0 in inter_features] if c in D_train.columns]\n",
    "rhs_cols = [c for c in (poly_cols + inter_cols + lin_controls) if c in D_train.columns]\n",
    "\n",
    "# --- 7) fit OLS with named design matrix ---\n",
    "Xmat_train = sm.add_constant(D_train[rhs_cols], has_constant=\"add\")\n",
    "ols_model = sm.OLS(y_train.to_numpy(float), Xmat_train.to_numpy(float)).fit()\n",
    "coef = pd.Series(ols_model.params, index=Xmat_train.columns)\n",
    "print(ols_model.summary())\n",
    "\n",
    "# --- exact ME formula now stops at cubic ---\n",
    "def me_from_df(df, coef_series):\n",
    "    qv = df[q].astype(float).to_numpy()\n",
    "    def c(name): return float(coef_series.get(name, 0.0))\n",
    "    me = (\n",
    "        c(f\"{q}_p1\")\n",
    "        + 2.0*c(f\"{q}_p2\")*qv\n",
    "        + 3.0*c(f\"{q}_p3\")*(qv**2)\n",
    "    )\n",
    "    for cfeat in inter_features:\n",
    "        col = f\"{q}_x_{cfeat}\"\n",
    "        if col in df.columns:\n",
    "            me = me + c(col)*df[cfeat].astype(float).to_numpy()\n",
    "    return me\n",
    "\n",
    "# --- 8) predict ŷ and ME on each split ---\n",
    "def predict_split(D, y_true=None, name=\"split\"):\n",
    "    Xm = sm.add_constant(D[rhs_cols], has_constant=\"add\")\n",
    "    y_pred = np.asarray(Xm.to_numpy(float) @ coef.values, dtype=float)\n",
    "    me = me_from_df(D, coef)\n",
    "    out = pd.DataFrame({\n",
    "        \"y_pred\": y_pred,\n",
    "        \"ME\": me,\n",
    "    }, index=D.index)\n",
    "    if y_true is not None:\n",
    "        out[\"y_true\"] = y_true.values\n",
    "        print(\n",
    "            f\"{name.upper():5s} | r2={r2_score(out['y_true'], out['y_pred']):.3f} \"\n",
    "            f\"rmse={root_mean_squared_error(out['y_true'], out['y_pred']):.1f} \"\n",
    "            f\"mae={mean_absolute_error(out['y_true'], out['y_pred']):.1f} \"\n",
    "            f\"n={len(out):,}\"\n",
    "        )\n",
    "    return out\n",
    "\n",
    "train_out = predict_split(D_train, y_train, name=\"train\")\n",
    "val_out   = predict_split(D_val,   y_val,   name=\"val\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "4a11ed44",
   "metadata": {},
   "outputs": [],
   "source": [
    "random_state = 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "88e66a74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN (spline): {'r2': 0.9007229178402355, 'rmse': 2466.808108215663, 'mae': 1895.805220643691, 'n_obs': 2365200}\n",
      "TRAIN (gam)   : {'r2': 0.902350014367182, 'rmse': 2446.509784035914, 'mae': 1876.2662291433633, 'n_obs': 2365200}\n",
      "VAL   (spline): {'r2': 0.7607804587011959, 'rmse': 2918.4935655969402, 'mae': 2355.146831214452, 'n_obs': 328320}\n",
      "VAL   (gam)   : {'r2': 0.7635884458554831, 'rmse': 2901.314192780919, 'mae': 2349.7995390459955, 'n_obs': 328320}\n"
     ]
    }
   ],
   "source": [
    "preprocess = build_mef_preprocess_pipeline(timestamp_col=\"timestamp\", use_winsorize_precip=True)\n",
    "\n",
    "qspline = build_qspline_pipeline(\n",
    "    preprocess,\n",
    "    weather_linear_base=[\"wind_speed_mps_std\",\"temperature_celsius_std\"],\n",
    "    n_splines_q=15, lam_q=10.0\n",
    ")\n",
    "\n",
    "qgam = build_qgam_pipeline(\n",
    "    preprocess,\n",
    "    smooth_cols=[\"surface_net_solar_radiation_joules_per_m2_std\",\"wind_speed_mps_std\",\"temperature_celsius_std\"],\n",
    "    linear_extra=[\"precipitation_mm_std\",\"total_cloud_cover_std\",\"wind_dir_sin\",\"wind_dir_cos\"],\n",
    "    n_splines_q=15, lam_q=10.0, n_splines_other=12, lam_other=10.0\n",
    ")\n",
    "\n",
    "evaluate_mef_pipeline = evaluate_gam_pipeline\n",
    "\n",
    "# Fit/eval each model separately\n",
    "# Fit/eval each model separately\n",
    "qspline.fit(pd.concat([X_train, y_train.rename(\"tons_co2\")], axis=1), y_train)\n",
    "qgam.fit(pd.concat(objs=[X_train, y_train.rename(\"tons_co2\")], axis=1), y_train)\n",
    "\n",
    "train_spline = evaluate_mef_pipeline(qspline, X_train, y_train)\n",
    "train_gam    = evaluate_mef_pipeline(qgam,    X_train, y_train)\n",
    "val_spline   = evaluate_mef_pipeline(qspline, X_val,   y_val)\n",
    "val_gam      = evaluate_mef_pipeline(qgam,    X_val,   y_val)\n",
    "\n",
    "print(\"TRAIN (spline):\", metrics_from_frame(train_spline))\n",
    "print(\"TRAIN (gam)   :\", metrics_from_frame(train_gam))\n",
    "print(\"VAL   (spline):\", metrics_from_frame(val_spline))\n",
    "print(\"VAL   (gam)   :\", metrics_from_frame(val_gam))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "e5a7df4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN (spline): {'r2': 0.8994856425956893, 'rmse': 2482.132238469184, 'mae': 1911.1260953291799, 'n_obs': 2365200}\n",
      "TRAIN (gam)   : {'r2': 0.9026817135234814, 'rmse': 2442.3510761126913, 'mae': 1871.9748169092968, 'n_obs': 2365200}\n",
      "VAL   (spline): {'r2': 0.7503071750569459, 'rmse': 2981.696564545364, 'mae': 2416.9335131209273, 'n_obs': 328320}\n",
      "VAL   (gam)   : {'r2': 0.764982643506238, 'rmse': 2892.7465346089093, 'mae': 2341.276235681636, 'n_obs': 328320}\n"
     ]
    }
   ],
   "source": [
    "preprocess = build_mef_preprocess_pipeline(timestamp_col=\"timestamp\", use_winsorize_precip=True)\n",
    "\n",
    "qspline = build_qspline_pipeline(\n",
    "    preprocess,\n",
    "    weather_linear_base=[\"total_cloud_cover_std\"],\n",
    "    n_splines_q=15, lam_q=10.0\n",
    ")\n",
    "\n",
    "qgam = build_qgam_pipeline(\n",
    "    preprocess,\n",
    "    smooth_cols=[\"surface_net_solar_radiation_joules_per_m2_std\",\"wind_speed_mps_std\",\"temperature_celsius_std\", \"precipitation_mm_std\",\"wind_dir_sin\",\"wind_dir_cos\"],\n",
    "    linear_extra=[\"total_cloud_cover_std\",],\n",
    "    n_splines_q=15, lam_q=10.0, n_splines_other=12, lam_other=10.0\n",
    ")\n",
    "\n",
    "evaluate_mef_pipeline = evaluate_gam_pipeline\n",
    "\n",
    "# Fit/eval each model separately\n",
    "qspline.fit(pd.concat([X_train, y_train.rename(\"tons_co2\")], axis=1), y_train)\n",
    "qgam.fit(pd.concat(objs=[X_train, y_train.rename(\"tons_co2\")], axis=1), y_train)\n",
    "\n",
    "train_spline = evaluate_mef_pipeline(qspline, X_train, y_train)\n",
    "train_gam    = evaluate_mef_pipeline(qgam,    X_train, y_train)\n",
    "val_spline   = evaluate_mef_pipeline(qspline, X_val,   y_val)\n",
    "val_gam      = evaluate_mef_pipeline(qgam,    X_val,   y_val)\n",
    "\n",
    "print(\"TRAIN (spline):\", metrics_from_frame(train_spline))\n",
    "print(\"TRAIN (gam)   :\", metrics_from_frame(train_gam))\n",
    "print(\"VAL   (spline):\", metrics_from_frame(val_spline))\n",
    "print(\"VAL   (gam)   :\", metrics_from_frame(df=val_gam))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "48a0f8dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- QGAM v2: smooth {Q, wind, temp, solar, precip}; only cloud cover linear (+ wind_dir sin/cos linear)\n",
    "def build_qgam_pipeline_v2(\n",
    "    preprocess: Pipeline,\n",
    "    fe_cols_prefixes=(\"month_\", \"hour_\"),\n",
    "    y_var=\"tons_co2\",\n",
    "    q_col=\"demand_met\",\n",
    "    q_std_col=\"demand_met_std\",\n",
    "    # smooth all meaningful nonlinear drivers\n",
    "    smooth_cols=(\"wind_speed_mps_std\",\n",
    "                 \"temperature_celsius_std\",\n",
    "                 \"surface_net_solar_radiation_joules_per_m2_std\",\n",
    "                 \"precipitation_mm_std\"),\n",
    "    # keep these linear\n",
    "    linear_extra=(\"total_cloud_cover_std\", \"wind_dir_sin\", \"wind_dir_cos\"),\n",
    "    n_splines_q=16, lam_q=10.0,\n",
    "    n_splines_other=20, lam_other=15.0,   # slightly stronger penalty to tame wiggles\n",
    "    random_state=12,\n",
    "):\n",
    "    def _fe_names_after_fit(df_pre: pd.DataFrame) -> list[str]:\n",
    "        return [c for c in df_pre.columns if c.startswith(fe_cols_prefixes)]\n",
    "\n",
    "    est = QGAMRegressorPyGAM(\n",
    "        y_var=y_var, q_col=q_col, q_std_col=q_std_col,\n",
    "        smooth_cols=list(smooth_cols),\n",
    "        weather_linear_cols=list(smooth_cols) + list(linear_extra),\n",
    "        fe_cols=[],\n",
    "        n_splines_q=n_splines_q, lam_q=lam_q,\n",
    "        n_splines_other=n_splines_other, lam_other=lam_other,\n",
    "        random_state=random_state,\n",
    "    )\n",
    "\n",
    "    class _AttachFENames(BaseEstimator, TransformerMixin):\n",
    "        def __init__(self, estimator): self.estimator = estimator\n",
    "        def fit(self, X, y=None):\n",
    "            self.estimator.fe_cols = _fe_names_after_fit(X); return self\n",
    "        def transform(self, X): return X\n",
    "\n",
    "    return Pipeline([\n",
    "        (\"PreprocessMEF\", preprocess),\n",
    "        (\"AttachFE\", _AttachFENames(est)),\n",
    "        (\"QGAMRegressorPyGAM_v2\", est),\n",
    "    ])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "574f8aaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- QSpline v2: Q spline + smooth {wind, temp, solar}; linear {precip, cloud, wind_dir}\n",
    "def build_qspline_pipeline_v2(\n",
    "    preprocess: Pipeline,\n",
    "    fe_cols_prefixes=(\"month_\", \"hour_\"),\n",
    "    y_var=\"tons_co2\",\n",
    "    q_col=\"demand_met\",\n",
    "    q_std_col=\"demand_met_std\",\n",
    "    # smooth these weather terms (std-scaled from preprocess)\n",
    "    smooth_cols=(\"wind_speed_mps_std\",\n",
    "                 \"temperature_celsius_std\",\n",
    "                 \"surface_net_solar_radiation_joules_per_m2_std\"),\n",
    "    # keep these linear\n",
    "    linear_extra=(\"precipitation_mm_std\", \"total_cloud_cover_std\", \"wind_dir_sin\", \"wind_dir_cos\"),\n",
    "    n_splines_q=16, lam_q=10.0,         # a touch more flexibility for Q\n",
    "    n_splines_other=20, lam_other=12.0, # more knots for temp/solar; modest penalty\n",
    "    random_state=12,\n",
    "):\n",
    "    def _fe_names_after_fit(df_pre: pd.DataFrame) -> list[str]:\n",
    "        return [c for c in df_pre.columns if c.startswith(fe_cols_prefixes)]\n",
    "\n",
    "    # We use QGAMRegressorPyGAM under the hood so we can add a few smooth weather terms\n",
    "    est = QGAMRegressorPyGAM(\n",
    "        y_var=y_var, q_col=q_col, q_std_col=q_std_col,\n",
    "        smooth_cols=list(smooth_cols),\n",
    "        weather_linear_cols=list(smooth_cols) + list(linear_extra),\n",
    "        fe_cols=[],  # filled at fit-time\n",
    "        n_splines_q=n_splines_q, lam_q=lam_q,\n",
    "        n_splines_other=n_splines_other, lam_other=lam_other,\n",
    "        random_state=random_state,\n",
    "    )\n",
    "\n",
    "    class _AttachFENames(BaseEstimator, TransformerMixin):\n",
    "        def __init__(self, estimator): self.estimator = estimator\n",
    "        def fit(self, X, y=None):\n",
    "            self.estimator.fe_cols = _fe_names_after_fit(X); return self\n",
    "        def transform(self, X): return X\n",
    "\n",
    "    return Pipeline([\n",
    "        (\"PreprocessMEF\", preprocess),\n",
    "        (\"AttachFE\", _AttachFENames(est)),\n",
    "        (\"QSplineRegressorPyGAM_v2\", est),\n",
    "    ])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "afbfcf11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN (spline): {'r2': 0.9023497516729004, 'rmse': 2446.5130747875396, 'mae': 1876.2670131060654, 'n_obs': 2365200}\n",
      "TRAIN (gam)   : {'r2': 0.9024198021173191, 'rmse': 2445.635401267366, 'mae': 1875.6468955007492, 'n_obs': 2365200}\n",
      "VAL   (spline): {'r2': 0.7634820868875153, 'rmse': 2901.9667541276717, 'mae': 2350.0381272519685, 'n_obs': 328320}\n",
      "VAL   (gam)   : {'r2': 0.7637625589516848, 'rmse': 2900.2456116722146, 'mae': 2347.757980548095, 'n_obs': 328320}\n"
     ]
    }
   ],
   "source": [
    "preprocess = build_mef_preprocess_pipeline(timestamp_col=\"timestamp\", use_winsorize_precip=True)\n",
    "\n",
    "qspline_v2 = build_qspline_pipeline_v2(preprocess)\n",
    "qgam_v2    = build_qgam_pipeline_v2(preprocess)\n",
    "\n",
    "# Fit both on TRAIN (X must include the raw features; y is tons_co2)\n",
    "qspline_v2.fit(pd.concat([X_train, y_train.rename(\"tons_co2\")], axis=1), y_train)\n",
    "qgam_v2.fit(pd.concat([X_train, y_train.rename(\"tons_co2\")], axis=1), y_train)\n",
    "\n",
    "\n",
    "evaluate_mef_pipeline = evaluate_gam_pipeline\n",
    "\n",
    "train_spline = evaluate_mef_pipeline(qspline_v2, X_train, y_train)\n",
    "train_gam    = evaluate_mef_pipeline(qgam_v2,    X_train, y_train)\n",
    "val_spline   = evaluate_mef_pipeline(qspline_v2, X_val,   y_val)\n",
    "val_gam      = evaluate_mef_pipeline(qgam_v2,    X_val,   y_val)\n",
    "\n",
    "print(\"TRAIN (spline):\", metrics_from_frame(train_spline))\n",
    "print(\"TRAIN (gam)   :\", metrics_from_frame(train_gam))\n",
    "print(\"VAL   (spline):\", metrics_from_frame(val_spline))\n",
    "print(\"VAL   (gam)   :\", metrics_from_frame(df=val_gam))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "5684e93f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN (spline): {'r2': 0.8654680128898424, 'rmse': 2871.5980090999483, 'mae': 2251.7032523084335, 'n_obs': 2365200}\n",
      "TRAIN (gam)   : {'r2': 0.8657315569102029, 'rmse': 2868.7839442176482, 'mae': 2249.0375048850465, 'n_obs': 2365200}\n",
      "VAL   (spline): {'r2': 0.6673237518172881, 'rmse': 3441.686521386532, 'mae': 2868.835758898085, 'n_obs': 328320}\n",
      "VAL   (gam)   : {'r2': 0.6693585909478375, 'rmse': 3431.144710326922, 'mae': 2859.9268315418162, 'n_obs': 328320}\n"
     ]
    }
   ],
   "source": [
    "# 1) Preprocess with Fourier time + wind dir + winsor + log1p + standardize\n",
    "preprocess = build_mef_preprocess_pipeline_2(timestamp_col=\"timestamp\", use_winsorize_precip=True)\n",
    "\n",
    "# 2) Build models using the *right* column names from preprocess_2\n",
    "#    QSpline: smooth wind/temp/solar; linear: precip_log1p, cloud, wind_dir, time terms\n",
    "qspline = build_qspline_pipeline_v2(\n",
    "    preprocess,\n",
    "    # smooth weather cols (std)\n",
    "    smooth_cols=(\"wind_speed_mps_std\",\n",
    "                 \"temperature_celsius_std\",\n",
    "                 \"surface_net_solar_radiation_joules_per_m2_std\"),\n",
    "    # linear extras — NOTE precip *_log1p_std* here\n",
    "    linear_extra=(\"precipitation_mm_log1p_std\",\"total_cloud_cover_std\",\"wind_dir_sin\",\"wind_dir_cos\",\n",
    "                  \"hour_sin\",\"hour_cos\",\"dow_sin\",\"dow_cos\",\"doy_sin\",\"doy_cos\",\"is_weekend\"),\n",
    "    n_splines_q=18, lam_q=10.0, n_splines_other=20, lam_other=12.0\n",
    ")\n",
    "\n",
    "#    QGAM: smooth wind/temp/solar/precip_log1p; linear: cloud, wind_dir, time\n",
    "qgam = build_qgam_pipeline_v2(\n",
    "    preprocess,\n",
    "    smooth_cols=(\"wind_speed_mps_std\",\n",
    "                 \"temperature_celsius_std\",\n",
    "                 \"surface_net_solar_radiation_joules_per_m2_std\",\n",
    "                 \"precipitation_mm_log1p_std\"),\n",
    "    linear_extra=(\"total_cloud_cover_std\",\"wind_dir_sin\",\"wind_dir_cos\",\n",
    "                  \"hour_sin\",\"hour_cos\",\"dow_sin\",\"dow_cos\",\"doy_sin\",\"doy_cos\",\"is_weekend\"),\n",
    "    n_splines_q=18, lam_q=10.0, n_splines_other=20, lam_other=15.0\n",
    ")\n",
    "\n",
    "# 3) Fit (X must include tons_co2 for the final step; we concat y)\n",
    "qspline.fit(pd.concat([X_train, y_train.rename(\"tons_co2\")], axis=1), y_train)\n",
    "qgam.fit(pd.concat([X_train, y_train.rename(\"tons_co2\")], axis=1), y_train)\n",
    "\n",
    "# 4) ALWAYS use the evaluator to get a DataFrame with y_true,y_pred,ME\n",
    "evaluate_mef_pipeline = evaluate_gam_pipeline\n",
    "\n",
    "train_spline = evaluate_mef_pipeline(qspline, X_train, y_train)\n",
    "train_gam    = evaluate_mef_pipeline(qgam,    X_train, y_train)\n",
    "val_spline   = evaluate_mef_pipeline(qspline, X_val,   y_val)\n",
    "val_gam      = evaluate_mef_pipeline(qgam,    X_val,   y_val)\n",
    "\n",
    "print(\"TRAIN (spline):\", metrics_from_frame(train_spline))\n",
    "print(\"TRAIN (gam)   :\", metrics_from_frame(train_gam))\n",
    "print(\"VAL   (spline):\", metrics_from_frame(val_spline))\n",
    "print(\"VAL   (gam)   :\", metrics_from_frame(val_gam))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "ecbd6125",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_mef_preprocess_clean(\n",
    "    timestamp_col: str = \"timestamp\",\n",
    "    use_winsorize_precip: bool = True,\n",
    "    winsor_upper: float = 0.995,\n",
    "):\n",
    "    \"\"\"\n",
    "    - Date/time cols + weekend\n",
    "    - Fourier time (hour/week/doy)\n",
    "    - Wind direction → sin/cos\n",
    "    - (optional) winsorize precip, then log1p\n",
    "    - Standardize continuous drivers (adds *_std)\n",
    "    \"\"\"\n",
    "    steps = [\n",
    "        (\"DateTime\", DateTimeFeatureAdder(timestamp_col=timestamp_col, drop_original=False)),\n",
    "        (\"Fourier\",   TimeFourierAdder(timestamp_col=timestamp_col, add_hour=True, add_week=True, add_doy=True)),\n",
    "        (\"WindDir\",   WindDirToCyclic(dir_col=\"wind_direction_meteorological\",\n",
    "                                      out_sin=\"wind_dir_sin\", out_cos=\"wind_dir_cos\", drop_original=True)),\n",
    "    ]\n",
    "    if use_winsorize_precip:\n",
    "        steps.append((\"WinsorizePrecip\", Winsorize(columns=[\"precipitation_mm\"], lower=0.0, upper=winsor_upper)))\n",
    "    steps.append((\"Log1pPrecip\", Log1pColumns(columns=[\"precipitation_mm\"])))\n",
    "\n",
    "    cont_cols = [\n",
    "        \"demand_met\",\n",
    "        \"wind_speed_mps\",\n",
    "        \"temperature_celsius\",\n",
    "        \"precipitation_mm_log1p\",  # created by Log1pColumns\n",
    "        \"surface_net_solar_radiation_joules_per_m2\",\n",
    "        \"total_cloud_cover\",\n",
    "    ]\n",
    "    steps.append((\"Standardize\", StandardizeContinuous(columns=cont_cols, suffix=\"_std\")))\n",
    "    return Pipeline(steps, verbose=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "7b35fc8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_qspline_clean(\n",
    "    preprocess: Pipeline,\n",
    "    y_var=\"tons_co2\",\n",
    "    q_col=\"demand_met\",\n",
    "    q_std_col=\"demand_met_std\",\n",
    "    n_splines_q=18, lam_q=10.0, random_state=12,\n",
    "):\n",
    "    # linear regressors (already standardized) + time Fourier + weekend + wind dir\n",
    "    linear_extras = [\n",
    "        \"wind_speed_mps_std\",\"temperature_celsius_std\",\n",
    "        \"precipitation_mm_log1p_std\",\"surface_net_solar_radiation_joules_per_m2_std\",\n",
    "        \"total_cloud_cover_std\",\"wind_dir_sin\",\"wind_dir_cos\",\n",
    "        \"hour_sin\",\"hour_cos\",\"dow_sin\",\"dow_cos\",\"doy_sin\",\"doy_cos\",\"is_weekend\",\n",
    "    ]\n",
    "\n",
    "    est = QSplineRegressorPyGAM(\n",
    "        y_var=y_var, q_col=q_col, q_std_col=q_std_col,\n",
    "        weather_linear_cols=linear_extras,\n",
    "        fe_cols=[], n_splines_q=n_splines_q, lam_q=lam_q, random_state=random_state,\n",
    "    )\n",
    "\n",
    "    return Pipeline([\n",
    "        (\"PreprocessMEF\", preprocess),\n",
    "        (\"QSplineRegressorPyGAM\", est),\n",
    "    ])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "254d761f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_qgam_clean(\n",
    "    preprocess: Pipeline,\n",
    "    y_var=\"tons_co2\",\n",
    "    q_col=\"demand_met\",\n",
    "    q_std_col=\"demand_met_std\",\n",
    "    smooth_cols=(\"wind_speed_mps_std\",\n",
    "                 \"temperature_celsius_std\",\n",
    "                 \"surface_net_solar_radiation_joules_per_m2_std\",\n",
    "                 \"precipitation_mm_log1p_std\"),\n",
    "    linear_extra=(\"total_cloud_cover_std\",\"wind_dir_sin\",\"wind_dir_cos\",\n",
    "                  \"hour_sin\",\"hour_cos\",\"dow_sin\",\"dow_cos\",\"doy_sin\",\"doy_cos\",\"is_weekend\"),\n",
    "    n_splines_q=18, lam_q=10.0,\n",
    "    n_splines_other=20, lam_other=15.0,\n",
    "    random_state=12,\n",
    "):\n",
    "    smooth_cols = list(smooth_cols)\n",
    "    linear_extra = list(linear_extra)\n",
    "\n",
    "    est = QGAMRegressorPyGAM(\n",
    "        y_var=y_var, q_col=q_col, q_std_col=q_std_col,\n",
    "        smooth_cols=smooth_cols,\n",
    "        weather_linear_cols=smooth_cols + linear_extra,   # smooth subset is detected internally\n",
    "        fe_cols=[],\n",
    "        n_splines_q=n_splines_q, lam_q=lam_q,\n",
    "        n_splines_other=n_splines_other, lam_other=lam_other,\n",
    "        random_state=random_state,\n",
    "    )\n",
    "\n",
    "    return Pipeline([\n",
    "        (\"PreprocessMEF\", preprocess),\n",
    "        (\"QGAMRegressorPyGAM\", est),\n",
    "    ])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "1b867ac2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN (spline): {'r2': 0.8640675523558579, 'rmse': 2886.505793901197, 'mae': 2260.7960070990057, 'n_obs': 2365200}\n",
      "TRAIN (gam)   : {'r2': 0.8657315569102029, 'rmse': 2868.783944217649, 'mae': 2249.037504885047, 'n_obs': 2365200}\n",
      "VAL   (spline): {'r2': 0.6634287721344834, 'rmse': 3461.775556604186, 'mae': 2876.665516368778, 'n_obs': 328320}\n",
      "VAL   (gam)   : {'r2': 0.6693585909478403, 'rmse': 3431.144710326907, 'mae': 2859.9268315417958, 'n_obs': 328320}\n"
     ]
    }
   ],
   "source": [
    "preprocess = build_mef_preprocess_clean(timestamp_col=\"timestamp\", use_winsorize_precip=True)\n",
    "\n",
    "qspline = build_qspline_clean(preprocess)\n",
    "qgam    = build_qgam_clean(preprocess)\n",
    "\n",
    "# fit (concat y so final estimator can see y_var in the same frame)\n",
    "qspline.fit(pd.concat([X_train, y_train.rename(\"tons_co2\")], axis=1), y_train)\n",
    "qgam.fit(pd.concat([X_train, y_train.rename(\"tons_co2\")], axis=1), y_train)\n",
    "\n",
    "# evaluate\n",
    "evaluate_mef_pipeline = evaluate_gam_pipeline\n",
    "train_spline = evaluate_mef_pipeline(qspline, X_train, y_train)\n",
    "train_gam    = evaluate_mef_pipeline(qgam,    X_train, y_train)\n",
    "val_spline   = evaluate_mef_pipeline(qspline, X_val,   y_val)\n",
    "val_gam      = evaluate_mef_pipeline(qgam,    X_val,   y_val)\n",
    "\n",
    "print(\"TRAIN (spline):\", metrics_from_frame(train_spline))\n",
    "print(\"TRAIN (gam)   :\", metrics_from_frame(train_gam))\n",
    "print(\"VAL   (spline):\", metrics_from_frame(val_spline))\n",
    "print(\"VAL   (gam)   :\", metrics_from_frame(val_gam))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "f5429b56",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PolynomialMERegressorRidge(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"\n",
    "    y = tons_co2\n",
    "    Features:\n",
    "      - Q_std, Q_std^2\n",
    "      - drivers_std (linear)\n",
    "      - (Q_std × driver_std) for each driver_std\n",
    "      - time_cols (linear, e.g., hour_sin/cos, dow_sin/cos, doy_sin/cos, is_weekend)\n",
    "    RidgeCV for stability. Provides y_pred and ME in ORIGINAL Q units.\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        y_var=\"tons_co2\",\n",
    "        q_col=\"demand_met\",\n",
    "        q_std_col=\"demand_met_std\",\n",
    "        driver_std_cols=(\n",
    "            \"temperature_celsius_std\",\n",
    "            \"wind_speed_mps_std\",\n",
    "            \"surface_net_solar_radiation_joules_per_m2_std\",\n",
    "            \"precipitation_mm_log1p_std\",\n",
    "            \"total_cloud_cover_std\",\n",
    "            \"wind_dir_sin\",\n",
    "            \"wind_dir_cos\",\n",
    "        ),\n",
    "        time_cols=(\"hour_sin\",\"hour_cos\",\"dow_sin\",\"dow_cos\",\"doy_sin\",\"doy_cos\",\"is_weekend\"),\n",
    "        alphas=(1e-3, 1e-2, 1e-1, 1, 3, 10, 30, 100),\n",
    "        cv=5,\n",
    "        fit_intercept=True,\n",
    "        random_state=12,\n",
    "    ):\n",
    "        self.y_var = y_var\n",
    "        self.q_col = q_col\n",
    "        self.q_std_col = q_std_col\n",
    "        self.driver_std_cols = list(driver_std_cols)\n",
    "        self.time_cols = list(time_cols)\n",
    "        self.alphas = tuple(alphas)\n",
    "        self.cv = int(cv)\n",
    "        self.fit_intercept = bool(fit_intercept)\n",
    "        self.random_state = random_state\n",
    "\n",
    "        # learned\n",
    "        self.model_ = None\n",
    "        self.q_std_train_ = None\n",
    "        self.feature_names_ = None\n",
    "        self.coef_series_ = None\n",
    "\n",
    "    # ---------- design matrix ----------\n",
    "    def _design(self, df: pd.DataFrame) -> np.ndarray:\n",
    "        Qs = df[self.q_std_col].astype(float).to_numpy()\n",
    "        X_blocks = []\n",
    "\n",
    "        # Q terms\n",
    "        X_blocks.append(Qs.reshape(-1,1))              # Q_std\n",
    "        X_blocks.append((Qs**2).reshape(-1,1))         # Q_std^2\n",
    "\n",
    "        # drivers (linear)\n",
    "        Z = []\n",
    "        for c in self.driver_std_cols:\n",
    "            z = df[c].astype(float).to_numpy()\n",
    "            Z.append(z.reshape(-1,1))\n",
    "        Z = np.hstack(Z) if Z else np.empty((len(df),0))\n",
    "        if Z.size:\n",
    "            X_blocks.append(Z)\n",
    "\n",
    "        # Q × driver interactions\n",
    "        if Z.size:\n",
    "            X_blocks.append(Z * Qs.reshape(-1,1))\n",
    "\n",
    "        # time features (linear, already in [-1,1] except is_weekend)\n",
    "        T = []\n",
    "        for c in self.time_cols:\n",
    "            if c in df.columns:\n",
    "                T.append(df[c].astype(float).to_numpy().reshape(-1,1))\n",
    "        T = np.hstack(T) if T else np.empty((len(df),0))\n",
    "        if T.size:\n",
    "            X_blocks.append(T)\n",
    "\n",
    "        X = np.hstack(X_blocks) if X_blocks else np.empty((len(df),0))\n",
    "\n",
    "        # names in the same order\n",
    "        names = []\n",
    "        names += [\"Q_std\",\"Q_std2\"]\n",
    "        names += list(self.driver_std_cols)\n",
    "        names += [f\"Q_std:{c}\" for c in self.driver_std_cols]\n",
    "        names += [c for c in self.time_cols if c in df.columns]\n",
    "\n",
    "        self.feature_names_ = names\n",
    "        return X\n",
    "\n",
    "    # ---------- fit / predict ----------\n",
    "    def fit(self, X: pd.DataFrame, y=None):\n",
    "        if y is None:\n",
    "            if self.y_var not in X.columns:\n",
    "                raise KeyError(f\"'{self.y_var}' must be present if y=None\")\n",
    "            y_arr = X[self.y_var].astype(float).to_numpy()\n",
    "        else:\n",
    "            y_arr = (y.iloc[:,0] if isinstance(y, pd.DataFrame) else y).astype(float).to_numpy()\n",
    "\n",
    "        # need raw Q to compute std(Q) for chain rule\n",
    "        if self.q_col not in X.columns or self.q_std_col not in X.columns:\n",
    "            raise KeyError(f\"Expect both '{self.q_col}' and '{self.q_std_col}' in X\")\n",
    "\n",
    "        self.q_std_train_ = float(X[self.q_col].astype(float).std(ddof=0)) or 1.0\n",
    "\n",
    "        # design\n",
    "        D = self._design(X)\n",
    "        # model\n",
    "        self.model_ = RidgeCV(alphas=self.alphas, cv=self.cv, fit_intercept=self.fit_intercept, scoring=\"r2\")\n",
    "        self.model_.fit(D, y_arr)\n",
    "\n",
    "        # store coefficients with names for interpretation\n",
    "        self.coef_series_ = pd.Series(self.model_.coef_, index=self.feature_names_)\n",
    "        return self\n",
    "\n",
    "    def predict(self, X: pd.DataFrame, predict_type: str = \"ME\"):\n",
    "        if self.model_ is None:\n",
    "            raise RuntimeError(\"Model not fitted\")\n",
    "\n",
    "        if predict_type.lower() == \"y\":\n",
    "            D = self._design(X)\n",
    "            return self.model_.predict(D)\n",
    "\n",
    "        # ME: derivative wrt Q in ORIGINAL units\n",
    "        # dY/dQ_std = β_Q + 2β_Q2*Q_std + Σ β_{Q:z} * z_std\n",
    "        Qs = X[self.q_std_col].astype(float).to_numpy()\n",
    "        beta = self.coef_series_\n",
    "        # Safe getters (0 if missing)\n",
    "        def b(name): return float(beta.get(name, 0.0))\n",
    "\n",
    "        dY_dQstd = (\n",
    "            b(\"Q_std\")\n",
    "            + 2.0 * b(\"Q_std2\") * Qs\n",
    "            + sum(b(f\"Q_std:{c}\") * X[c].astype(float).to_numpy()\n",
    "                  for c in self.driver_std_cols if f\"Q_std:{c}\" in beta.index)\n",
    "        )\n",
    "        # chain rule to original Q\n",
    "        return dY_dQstd * (1.0 / float(self.q_std_train_))\n",
    "\n",
    "    def transform(self, X: pd.DataFrame) -> pd.DataFrame:\n",
    "        df = X.copy()\n",
    "        df[\"y_pred\"] = self.predict(df, predict_type=\"y\")\n",
    "        df[\"ME\"] = self.predict(df, predict_type=\"ME\")\n",
    "        return df\n",
    "\n",
    "    # handy exposers\n",
    "    @property\n",
    "    def coef_(self) -> pd.Series:\n",
    "        return self.coef_series_.copy() if self.coef_series_ is not None else None\n",
    "\n",
    "    @property\n",
    "    def alpha_(self) -> float:\n",
    "        return float(getattr(self.model_, \"alpha_\", np.nan))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "b52e3baf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN: {'r2': 0.8668213252728539, 'rmse': 2857.118211998734, 'mae': 2233.621682211388, 'n_obs': 2365200}\n",
      "VAL  : {'r2': 0.6759170708920273, 'rmse': 3396.9448119661834, 'mae': 2802.7023490758033, 'n_obs': 328320}\n",
      "Chosen alpha: 0.001\n",
      "Q_std                                                  7871.937154\n",
      "hour_cos                                               5555.744017\n",
      "doy_cos                                                5065.526497\n",
      "doy_sin                                                2803.229579\n",
      "surface_net_solar_radiation_joules_per_m2_std          1206.847810\n",
      "Q_std:surface_net_solar_radiation_joules_per_m2_std    -607.674321\n",
      "hour_sin                                                570.722137\n",
      "Q_std2                                                 -515.553924\n",
      "temperature_celsius_std                                 474.836922\n",
      "wind_speed_mps_std                                     -401.950392\n",
      "Q_std:temperature_celsius_std                           320.432954\n",
      "total_cloud_cover_std                                  -171.658913\n",
      "dow_sin                                                 100.301371\n",
      "Q_std:total_cloud_cover_std                             -82.576359\n",
      "Q_std:wind_dir_cos                                       74.303999\n",
      "dow_cos                                                 -73.471906\n",
      "Q_std:wind_speed_mps_std                                -36.584266\n",
      "Q_std:precipitation_mm_log1p_std                         35.102833\n",
      "is_weekend                                               24.359111\n",
      "wind_dir_sin                                            -13.762769\n",
      "wind_dir_cos                                            -12.713397\n",
      "Q_std:wind_dir_sin                                       -5.261630\n",
      "precipitation_mm_log1p_std                               -0.679630\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# 1) Preprocess (already defined in your codebase)\n",
    "preprocess = build_mef_preprocess_pipeline_2(timestamp_col=\"timestamp\", use_winsorize_precip=True)\n",
    "\n",
    "# 2) Final estimator: choose drivers present after preprocess_2\n",
    "drivers = [\n",
    "    \"temperature_celsius_std\",\n",
    "    \"wind_speed_mps_std\",\n",
    "    \"surface_net_solar_radiation_joules_per_m2_std\",\n",
    "    \"precipitation_mm_log1p_std\",\n",
    "    \"total_cloud_cover_std\",\n",
    "    \"wind_dir_sin\",\n",
    "    \"wind_dir_cos\",\n",
    "]\n",
    "\n",
    "time_cols = [\"hour_sin\",\"hour_cos\",\"dow_sin\",\"dow_cos\",\"doy_sin\",\"doy_cos\",\"is_weekend\"]\n",
    "\n",
    "poly_ridge = PolynomialMERegressorRidge(\n",
    "    y_var=\"tons_co2\",\n",
    "    q_col=\"demand_met\",\n",
    "    q_std_col=\"demand_met_std\",\n",
    "    driver_std_cols=drivers,\n",
    "    time_cols=time_cols,\n",
    "    alphas=(1e-3, 1e-2, 1e-1, 1, 3, 10, 30, 100),\n",
    "    cv=5,\n",
    "    fit_intercept=True,\n",
    "    random_state=12,\n",
    ")\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "ridge_poly_pipeline = Pipeline([\n",
    "    (\"PreprocessMEF\", preprocess),\n",
    "    (\"PolyRidgeME\", poly_ridge),\n",
    "])\n",
    "\n",
    "# 3) Fit on TRAIN (concat y so final step can see y_var column)\n",
    "ridge_poly_pipeline.fit(pd.concat([X_train, y_train.rename(\"tons_co2\")], axis=1), y_train)\n",
    "\n",
    "# 4) Evaluate with your existing helper (returns y_true, y_pred, ME)\n",
    "evaluate_mef_pipeline = evaluate_gam_pipeline  # re-use your function\n",
    "\n",
    "train_eval = evaluate_mef_pipeline(ridge_poly_pipeline, X_train, y_train)\n",
    "val_eval   = evaluate_mef_pipeline(ridge_poly_pipeline, X_val,   y_val)\n",
    "\n",
    "print(\"TRAIN:\", metrics_from_frame(train_eval))\n",
    "print(\"VAL  :\", metrics_from_frame(val_eval))\n",
    "\n",
    "# 5) Peek at coefficients and chosen alpha\n",
    "est = ridge_poly_pipeline.named_steps[\"PolyRidgeME\"]\n",
    "print(\"Chosen alpha:\", est.alpha_)\n",
    "print(est.coef_.sort_values(key=np.abs, ascending=False).head(25))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "415aeb01",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "irpenv_4",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
