{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "498cb0bb",
   "metadata": {},
   "source": [
    "# irp-dbk24 - \"Optimising Demand Response Strategies for Carbon-Intelligent Electricity Use\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d6cd5cc",
   "metadata": {},
   "source": [
    "# Developing Marginal Emissions Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7b39099",
   "metadata": {},
   "source": [
    "**NOTEBOOK PURPOSE(S):**\n",
    "* Reproduce the R Analysis provided by Shefali\n",
    "\n",
    "\n",
    "**LIMITATIONS:**\n",
    "\n",
    "**NOTEBOOK OUTPUTS:**\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04ffa399",
   "metadata": {},
   "source": [
    "### Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "23a9e701",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ────────────────────────────────────────────────────────────────────────────\n",
    "# Future (must be first)\n",
    "# ────────────────────────────────────────────────────────────────────────────\n",
    "from __future__ import annotations\n",
    "\n",
    "# ────────────────────────────────────────────────────────────────────────────\n",
    "# Jupyter/Notebook Setup\n",
    "# ────────────────────────────────────────────────────────────────────────────\n",
    "%matplotlib inline\n",
    "from IPython.display import display\n",
    "\n",
    "# ────────────────────────────────────────────────────────────────────────────\n",
    "# Standard Library\n",
    "# ────────────────────────────────────────────────────────────────────────────\n",
    "import binascii\n",
    "import calendar\n",
    "import json\n",
    "import logging\n",
    "import math\n",
    "import os\n",
    "import random\n",
    "import re\n",
    "import hashlib\n",
    "import inspect\n",
    "import time\n",
    "from copy import deepcopy\n",
    "from contextlib import contextmanager\n",
    "from dataclasses import dataclass\n",
    "from datetime import datetime, timedelta\n",
    "from functools import partial, wraps\n",
    "from itertools import combinations, product\n",
    "from multiprocessing import Manager, Pool, Lock, cpu_count\n",
    "from multiprocessing.pool import ThreadPool\n",
    "from pathlib import Path\n",
    "from typing import (\n",
    "    Any, Callable, Dict, Iterable, List, Mapping, Optional, Sequence, Tuple, Union\n",
    ")\n",
    "from zoneinfo import ZoneInfo\n",
    "\n",
    "# ────────────────────────────────────────────────────────────────────────────\n",
    "# Core Data Handling\n",
    "# ────────────────────────────────────────────────────────────────────────────\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import polars as pl\n",
    "\n",
    "# ────────────────────────────────────────────────────────────────────────────\n",
    "# Machine Learning & Statistics\n",
    "# ────────────────────────────────────────────────────────────────────────────\n",
    "from feature_engine.creation import CyclicalFeatures\n",
    "from scipy.stats import kurtosis, skew, zscore\n",
    "import statsmodels.api as sm\n",
    "import statsmodels.formula.api as smf\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "\n",
    "from sklearn.base import BaseEstimator, TransformerMixin, clone\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.kernel_approximation import RBFSampler\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import (\n",
    "    mean_absolute_error,\n",
    "    mean_squared_error,\n",
    "    r2_score,\n",
    "    root_mean_squared_error,\n",
    ")\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import OneHotEncoder, SplineTransformer\n",
    "from sklearn.utils.validation import check_is_fitted\n",
    "\n",
    "# ────────────────────────────────────────────────────────────────────────────\n",
    "# Visualization\n",
    "# ────────────────────────────────────────────────────────────────────────────\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "import matplotlib.ticker as ticker\n",
    "import seaborn as sns\n",
    "\n",
    "# ────────────────────────────────────────────────────────────────────────────\n",
    "# Geospatial\n",
    "# ────────────────────────────────────────────────────────────────────────────\n",
    "import geopandas as gpd\n",
    "from shapely.geometry import Point, Polygon\n",
    "from shapely.wkb import loads\n",
    "from pyproj import Proj, transform\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22fdf4be",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daee84a3",
   "metadata": {},
   "source": [
    "### Utilities"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7061bb1e",
   "metadata": {},
   "source": [
    "#### CSV File Handling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "0828268c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _drop_hash_from_part(\n",
    "        part_path: Path,\n",
    "        model_hash: str,\n",
    "        *,\n",
    "        chunk_size: int = 200_000,\n",
    "        delete_if_empty: bool = False,\n",
    ") -> int:\n",
    "    \"\"\"\n",
    "    Remove rows with model_id_hash == `model_hash` from a CSV part file.\n",
    "\n",
    "    - Streams in chunks (no huge memory spikes)\n",
    "    - Writes to a temp file, then atomically replaces the original\n",
    "    - Returns number of rows dropped\n",
    "    - If all rows are dropped:\n",
    "        • delete the file if `delete_if_empty=True`\n",
    "        • otherwise keep a header-only CSV\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    part_path : Path\n",
    "        CSV file to edit in place.\n",
    "    model_hash : str\n",
    "        Value to filter out from the 'model_id_hash' column.\n",
    "    chunk_size : int, default 200_000\n",
    "        Pandas read_csv chunk size.\n",
    "    delete_if_empty : bool, default False\n",
    "        If True and all rows are removed, delete the part file.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    int\n",
    "        Number of rows removed.\n",
    "    \"\"\"\n",
    "    part_path = Path(part_path)\n",
    "    if not part_path.exists():\n",
    "        return 0\n",
    "\n",
    "    # Quick header check\n",
    "    try:\n",
    "        header_df = pd.read_csv(part_path, nrows=0)\n",
    "    except Exception:\n",
    "        # Broken file — leave as-is\n",
    "        return 0\n",
    "    if \"model_id_hash\" not in header_df.columns:\n",
    "        return 0\n",
    "\n",
    "    dropped = 0\n",
    "    kept = 0\n",
    "    tmp_path = part_path.with_suffix(part_path.suffix + \".tmp\")\n",
    "\n",
    "    # Ensure no stale tmp\n",
    "    if tmp_path.exists():\n",
    "        try:\n",
    "            tmp_path.unlink()\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "    first_write = True\n",
    "    try:\n",
    "        for chunk in pd.read_csv(\n",
    "            part_path,\n",
    "            chunksize=chunk_size,\n",
    "            dtype={\"model_id_hash\": \"string\"},  # force string, avoid numeric coercion\n",
    "        ):\n",
    "            if \"model_id_hash\" not in chunk.columns:\n",
    "                # schema changed mid-file? abort safely\n",
    "                dropped = 0\n",
    "                kept = -1\n",
    "                break\n",
    "            mask = chunk[\"model_id_hash\"] != model_hash\n",
    "            kept_chunk = chunk.loc[mask]\n",
    "            n_dropped = int((~mask).sum())\n",
    "            dropped += n_dropped\n",
    "            kept += int(mask.sum())\n",
    "\n",
    "            if kept_chunk.empty:\n",
    "                continue\n",
    "\n",
    "            kept_chunk.to_csv(\n",
    "                tmp_path,\n",
    "                index=False,\n",
    "                mode=\"w\" if first_write else \"a\",\n",
    "                header=first_write,\n",
    "            )\n",
    "            first_write = False\n",
    "\n",
    "        # Nothing matched → no change\n",
    "        if dropped == 0:\n",
    "            if tmp_path.exists():\n",
    "                # wrote identical content; discard temp\n",
    "                try: tmp_path.unlink()\n",
    "                except Exception: pass\n",
    "            return 0\n",
    "\n",
    "        # All rows removed\n",
    "        if kept == 0:\n",
    "            if delete_if_empty:\n",
    "                # Delete original; remove temp if created\n",
    "                try: part_path.unlink()\n",
    "                except Exception: pass\n",
    "                if tmp_path.exists():\n",
    "                    try: tmp_path.unlink()\n",
    "                    except Exception: pass\n",
    "            else:\n",
    "                # Replace with header-only CSV\n",
    "                header_df.to_csv(tmp_path, index=False)\n",
    "                os.replace(tmp_path, part_path)\n",
    "            return dropped\n",
    "\n",
    "        # Normal case: replace atomically\n",
    "        os.replace(tmp_path, part_path)\n",
    "        return dropped\n",
    "\n",
    "    finally:\n",
    "        # Best-effort cleanup\n",
    "        if tmp_path.exists():\n",
    "            try: os.remove(tmp_path)\n",
    "            except Exception: pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "140dcc5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_model_logged_rotating_csv(\n",
    "        model_hash: str,\n",
    "        base_dir: str | Path,\n",
    "        file_prefix: str\n",
    ") -> bool:\n",
    "    \"\"\"\n",
    "    Return True if `model_hash` appears in the rolling-log index for `file_prefix`.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    model_hash : str\n",
    "        The 'model_id_hash' value to look up.\n",
    "    base_dir : str | Path\n",
    "        Directory holding the rolling CSV parts and index.\n",
    "    file_prefix : str\n",
    "        Prefix of the rolling log.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    bool\n",
    "        True if present in the index; False otherwise.\n",
    "    \"\"\"\n",
    "    idx = _read_index(_index_path(Path(base_dir), file_prefix))\n",
    "    if idx.empty or \"model_id_hash\" not in idx.columns:\n",
    "        return False\n",
    "    return str(model_hash) in idx[\"model_id_hash\"].astype(\"string\").values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "59da62ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _list_part_files(\n",
    "    base_dir: Path,\n",
    "    file_prefix: str,\n",
    "    ext: str = \"csv\",\n",
    ") -> list[Path]:\n",
    "    \"\"\"\n",
    "    List existing rolling CSV parts for a given prefix, sorted by numeric part index.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    base_dir : Path\n",
    "        Directory to search.\n",
    "    file_prefix : str\n",
    "        Prefix of the rolling CSV set (e.g., 'marginal_emissions_log').\n",
    "    ext : str, default 'csv'\n",
    "        File extension (without dot).\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    list[Path]\n",
    "        Sorted list of matching part files, e.g. [.../prefix.part000.csv, .../prefix.part001.csv, ...]\n",
    "    \"\"\"\n",
    "    if not base_dir.exists():\n",
    "        return []\n",
    "\n",
    "    rx = re.compile(rf\"^{re.escape(file_prefix)}\\.part(\\d+)\\.{re.escape(ext)}$\")\n",
    "    parts: list[tuple[int, Path]] = []\n",
    "\n",
    "    for p in base_dir.glob(f\"{file_prefix}.part*.{ext}\"):\n",
    "        if not p.is_file():\n",
    "            continue\n",
    "        m = rx.match(p.name)\n",
    "        if m:\n",
    "            parts.append((int(m.group(1)), p))\n",
    "\n",
    "    parts.sort(key=lambda t: t[0])\n",
    "    return [p for _, p in parts]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "aeab441e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_all_logs_rotating_csv(\n",
    "    results_dir: str | Path = \".\",\n",
    "    file_prefix: str = \"marginal_emissions_log\",\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Read only parts referenced by the index; drop duplicate hashes (keep last).\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    results_dir: str | Path\n",
    "        The directory containing the results.\n",
    "    file_prefix: str\n",
    "        The prefix of the log files to read.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pd.DataFrame\n",
    "        The concatenated DataFrame containing the logs.\n",
    "    \"\"\"\n",
    "    # Read the index file\n",
    "    base_dir = Path(results_dir)\n",
    "    idx = _read_index(_index_path(base_dir, file_prefix))\n",
    "    # Check if the index is empty\n",
    "    if idx.empty:\n",
    "        return pd.DataFrame()\n",
    "    # Get the unique parts to read\n",
    "    parts = idx[\"part_file\"].unique().tolist()\n",
    "    # Read the parts into DataFrames\n",
    "    dfs = [pd.read_csv(p) for p in parts if Path(p).exists()]\n",
    "    # Check if any DataFrames were read\n",
    "    if not dfs:\n",
    "        return pd.DataFrame()\n",
    "    # Concatenate the DataFrames\n",
    "    out = pd.concat(dfs, ignore_index=True)\n",
    "    # Drop duplicate model_id_hash entries\n",
    "    if \"model_id_hash\" in out.columns:\n",
    "        out = out.drop_duplicates(subset=[\"model_id_hash\"], keep=\"last\")\n",
    "    return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "7e6decb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _read_index(index_path: Path) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Read the rolling-log index CSV (id→part mapping).\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    index_path : Path\n",
    "        Path to '<file_prefix>_index.csv'.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pd.DataFrame\n",
    "        Columns ['model_id_hash','part_file'] or empty frame if not found/invalid.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        idx = pd.read_csv(index_path, dtype={\"model_id_hash\": \"string\", \"part_file\": \"string\"})\n",
    "        if not {\"model_id_hash\",\"part_file\"}.issubset(idx.columns):\n",
    "            raise ValueError(\"Index missing required columns.\")\n",
    "        return idx\n",
    "    except FileNotFoundError:\n",
    "        return pd.DataFrame(columns=[\"model_id_hash\",\"part_file\"])\n",
    "    except Exception:\n",
    "        # Be permissive but return the expected schema\n",
    "        return pd.DataFrame(columns=[\"model_id_hash\",\"part_file\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "77c18bbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_model_from_rotating_csv(\n",
    "        model_hash: str,\n",
    "        results_dir: str | Path = \".\",\n",
    "        file_prefix: str = \"marginal_emissions_log\",\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Remove all rows with `model_id_hash == model_hash` from the rolling CSV set.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    model_hash : str\n",
    "        Identifier to remove.\n",
    "    results_dir : str | Path, default \".\"\n",
    "        Directory holding parts and index.\n",
    "    file_prefix : str, default \"marginal_emissions_log\"\n",
    "        Prefix of the rolling log files.\n",
    "    \"\"\"\n",
    "    base_dir = _ensure_dir(Path(results_dir))\n",
    "    idx_path = _index_path(base_dir, file_prefix)\n",
    "\n",
    "    # Lock the index for the whole operation to avoid races with concurrent writers/readers\n",
    "    with _file_lock(_index_lock_path(idx_path)):\n",
    "        idx = _read_index(idx_path)\n",
    "        if idx.empty:\n",
    "            return\n",
    "\n",
    "        # Drop from referenced part files\n",
    "        for pf in idx.loc[idx[\"model_id_hash\"] == model_hash, \"part_file\"].dropna().unique():\n",
    "            _drop_hash_from_part(Path(pf), model_hash)\n",
    "\n",
    "        # Update index\n",
    "        idx = idx[idx[\"model_id_hash\"] != model_hash]\n",
    "        idx.to_csv(idx_path, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "3f9f7e5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_summary_to_rotating_csv(\n",
    "        summary_df: pd.DataFrame,\n",
    "        results_dir: str | Path = \".\",\n",
    "        file_prefix: str = \"marginal_emissions_log\",\n",
    "        max_mb: int = 95,\n",
    "        force_overwrite: bool = False,\n",
    "        naming: PartNaming | None = None,\n",
    "        fsync: bool = False,\n",
    ") -> Path:\n",
    "    \"\"\"\n",
    "    Append a single-row summary to a rolling CSV (<prefix>.partNNN.csv) with strict rotation:\n",
    "    - Per-file lock during append (prevents interleaved writes/duplicate headers)\n",
    "    - Under-lock preflight ensures the write will NOT push the file over `max_mb`\n",
    "      (allocates a new shard if necessary)\n",
    "    - Atomic index update under lock\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    summary_df : pd.DataFrame\n",
    "        Single-row DataFrame with at least a 'model_id_hash' column.\n",
    "    results_dir : str | Path, default \".\"\n",
    "        Directory to write parts and the index into.\n",
    "    file_prefix : str, default \"marginal_emissions_log\"\n",
    "        Prefix of the part files ('<prefix>.partNNN.csv').\n",
    "    max_mb : int, default 95\n",
    "        Rotate when current part would exceed this size (MiB) after the append.\n",
    "    force_overwrite : bool, default False\n",
    "        If True, delete existing rows with the same hash before appending.\n",
    "    naming : PartNaming, optional\n",
    "        Naming convention (token/width/ext). If provided, `ext` should include the dot\n",
    "        (e.g., \".csv\"). Internally we use the extension without the dot for matching.\n",
    "    fsync : bool, default False\n",
    "        If True, call fsync() on the file after writing to ensure data is flushed to disk.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    Path\n",
    "        The part file path that received the append.\n",
    "\n",
    "    Raises\n",
    "    ------\n",
    "    ValueError\n",
    "        If `summary_df` is empty or missing 'model_id_hash'.\n",
    "    \"\"\"\n",
    "    if summary_df.empty:\n",
    "        raise ValueError(\"summary_df is empty.\")\n",
    "    if \"model_id_hash\" not in summary_df.columns:\n",
    "        raise ValueError(\"summary_df must contain 'model_id_hash'.\")\n",
    "    if len(summary_df) != 1:\n",
    "        summary_df = summary_df.iloc[:1].copy()\n",
    "\n",
    "    naming = naming or PartNaming()\n",
    "    base_dir = _ensure_dir(Path(results_dir))\n",
    "    idx_path = _index_path(base_dir, file_prefix)\n",
    "    model_hash = str(summary_df[\"model_id_hash\"].iloc[0])\n",
    "    ext_nodot = naming.ext.lstrip(\".\")\n",
    "\n",
    "    # Optional overwrite: remove old rows (parts + index)\n",
    "    if force_overwrite:\n",
    "        remove_model_from_rotating_csv(model_hash, base_dir, file_prefix)\n",
    "    else:\n",
    "        if is_model_logged_rotating_csv(model_hash, base_dir, file_prefix):\n",
    "            print(f\"[SKIP] Hash already indexed: {model_hash}\")\n",
    "            parts = _list_part_files(base_dir, file_prefix, ext=ext_nodot)\n",
    "            return parts[-1] if parts else base_dir / naming.format(file_prefix, 0)\n",
    "\n",
    "    # Determine candidate shard\n",
    "    parts = _list_part_files(base_dir, file_prefix, ext=ext_nodot)\n",
    "    if parts:\n",
    "        target = parts[-1]\n",
    "    else:\n",
    "        target = allocate_next_part(base_dir, file_prefix, width=naming.width, ext=ext_nodot)\n",
    "\n",
    "    threshold_bytes = int(max_mb * 1024 * 1024)\n",
    "\n",
    "    # --- LOCK AND WRITE TO SHARD SAFELY ---\n",
    "    while True:\n",
    "        shard_lock = Path(str(target) + \".lock\")\n",
    "        with _file_lock(shard_lock):\n",
    "            current_size = Path(target).stat().st_size if Path(target).exists() else 0\n",
    "            write_header = (current_size == 0)\n",
    "            csv_payload = summary_df.to_csv(index=False, header=write_header)\n",
    "            payload_bytes = len(csv_payload.encode(\"utf-8\"))\n",
    "\n",
    "            if current_size + payload_bytes > threshold_bytes:\n",
    "                # rotate: leave lock, allocate new shard, try again\n",
    "                pass\n",
    "            else:\n",
    "                with open(target, \"a\", encoding=\"utf-8\", newline=\"\") as f:\n",
    "                    f.write(csv_payload)\n",
    "                    f.flush()\n",
    "                    if fsync:\n",
    "                        os.fsync(f.fileno())\n",
    "                break\n",
    "\n",
    "        target = allocate_next_part(base_dir, file_prefix, width=naming.width, ext=ext_nodot)\n",
    "\n",
    "    # --- LOCK AND UPDATE INDEX (atomic replace + optional fsync) ---\n",
    "    lock_path = _index_lock_path(idx_path)\n",
    "    with _file_lock(lock_path):\n",
    "        idx = _read_index(idx_path)\n",
    "        already = (\"model_id_hash\" in idx.columns) and (model_hash in idx[\"model_id_hash\"].astype(\"string\").values)\n",
    "        if not already:\n",
    "            idx = pd.concat(\n",
    "                [idx, pd.DataFrame([{\"model_id_hash\": model_hash, \"part_file\": str(target)}])],\n",
    "                ignore_index=True,\n",
    "            )\n",
    "            tmp_idx = idx_path.with_suffix(idx_path.suffix + \".tmp\")\n",
    "            with open(tmp_idx, \"w\", encoding=\"utf-8\", newline=\"\") as fh:\n",
    "                idx.to_csv(fh, index=False)\n",
    "                fh.flush()\n",
    "                if fsync:\n",
    "                    os.fsync(fh.fileno())\n",
    "            os.replace(tmp_idx, idx_path)\n",
    "            if fsync:\n",
    "                # Ensure directory entry for index is durable\n",
    "                dir_fd = os.open(str(idx_path.parent), os.O_DIRECTORY)\n",
    "                try:\n",
    "                    os.fsync(dir_fd)\n",
    "                finally:\n",
    "                    os.close(dir_fd)\n",
    "\n",
    "    print(f\"[SAVE] Appended to {target}, index updated.\")\n",
    "    return target"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "800a561a",
   "metadata": {},
   "source": [
    "#### General"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "9432677e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _file_size_mb(path: Path) -> float:\n",
    "    \"\"\"\n",
    "    Return size of `path` in MiB. If file doesn't exist, returns 0.0.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    path : Path\n",
    "        Path to the file.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    float\n",
    "        Size of the file in MiB.\n",
    "    \"\"\"\n",
    "    p = Path(path)\n",
    "    if not p.exists():\n",
    "        return 0.0\n",
    "    return p.stat().st_size / (1024 * 1024.0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88e93f12",
   "metadata": {},
   "source": [
    "#### Logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "8a1f9c95",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_existing_hashes(\n",
    "        results_dir: str | Path,\n",
    "        file_prefix: str,\n",
    ") -> set[str]:\n",
    "    \"\"\"\n",
    "    Get all unique `model_id_hash` values from the rolling-log index.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    results_dir : str | Path\n",
    "        Directory containing the rolling CSV parts and index.\n",
    "    file_prefix : str\n",
    "        Prefix of the rolling log files.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    set[str]\n",
    "        Unique model_id_hash values present in the index.\n",
    "    \"\"\"\n",
    "    idx = _read_index(_index_path(Path(results_dir), file_prefix))\n",
    "    if idx.empty or \"model_id_hash\" not in idx.columns:\n",
    "        return set()\n",
    "    # Ensure NA is dropped and cast to Python strings\n",
    "    return set(idx[\"model_id_hash\"].dropna().astype(str).tolist())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "42a7900d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_config_key(\n",
    "        config: Mapping[str, Any],\n",
    "          algo: str = \"sha256\"\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    Create a deterministic hash key for a configuration mapping.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    config : Mapping[str, Any]\n",
    "        Configuration to serialize. Keys should be stringable.\n",
    "    algo : {'sha256','md5','sha1',...}, default 'sha256'\n",
    "        Hash algorithm name passed to hashlib.new.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    str\n",
    "        Hex digest of the normalized, JSON-serialized configuration.\n",
    "    \"\"\"\n",
    "    def _norm(x):\n",
    "        # Order/JSON-stable normalization.\n",
    "        if isinstance(x, Mapping):\n",
    "            # sort by key string to be robust to non-string keys\n",
    "            return {str(k): _norm(v) for k, v in sorted(x.items(), key=lambda kv: str(kv[0]))}\n",
    "        if isinstance(x, (list, tuple)):\n",
    "            return [_norm(v) for v in x]\n",
    "        if isinstance(x, set):\n",
    "            # sets are unordered; sort normalized elements\n",
    "            return sorted(_norm(v) for v in x)\n",
    "        if isinstance(x, (np.floating, np.integer, np.bool_)):\n",
    "            return x.item()\n",
    "        if isinstance(x, (datetime,)):\n",
    "            return x.isoformat()\n",
    "        return x  # strings, ints, floats, bools, None, etc.\n",
    "\n",
    "    payload = json.dumps(\n",
    "        _norm(config),\n",
    "        sort_keys=True,\n",
    "        separators=(\",\", \":\"),\n",
    "        ensure_ascii=False,\n",
    "        default=str,   # last-resort for odd objects\n",
    "    )\n",
    "    h = hashlib.new(algo)\n",
    "    h.update(payload.encode(\"utf-8\"))\n",
    "    return h.hexdigest()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "0a39bb2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def signature_for_run(\n",
    "        user_pipeline: Pipeline,\n",
    "        x_columns: list[str],\n",
    "        y: pd.Series | pd.DataFrame,\n",
    "        *,\n",
    "        random_state: int,\n",
    "        eval_splits: tuple[str, ...] = (\"train\", \"validation\"),\n",
    "        compute_test: bool = False,\n",
    "        extra_info: dict | None = None,\n",
    ") -> tuple[str, dict]:\n",
    "    \"\"\"\n",
    "    Build a stable config mapping for a model run and return (hash_key, mapping).\n",
    "\n",
    "    This just standardizes what goes into the signature so different call sites\n",
    "    don’t accidentally diverge.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    user_pipeline : Pipeline\n",
    "        The user-defined pipeline to run.\n",
    "    x_columns : list[str]\n",
    "        The feature columns to use for the model.\n",
    "    y : pd.Series | pd.DataFrame\n",
    "        The target variable(s) for the model.\n",
    "    random_state : int\n",
    "        The random seed to use for the model.\n",
    "    eval_splits : tuple[str, ...], default=(\"train\", \"validation\")\n",
    "        The data splits to evaluate the model on.\n",
    "    compute_test : bool, default=False\n",
    "        Whether to compute metrics on the test split.\n",
    "    extra_info : dict | None, default=None\n",
    "        Any extra information to include in the signature.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    tuple[str, dict]\n",
    "        The hash key and the signature mapping.\n",
    "    \"\"\"\n",
    "    sig = {\n",
    "        \"pipeline_params\": user_pipeline.get_params(deep=True),\n",
    "        \"x_columns\": list(x_columns),\n",
    "        \"y_columns\": _y_columns_for_signature(y),\n",
    "        \"random_state\": int(random_state),\n",
    "        \"eval_splits\": tuple(eval_splits),\n",
    "        \"compute_test\": bool(compute_test),\n",
    "        **(extra_info or {}),\n",
    "    }\n",
    "    return make_config_key(sig), sig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "74c5acd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _y_columns_for_signature(y: pd.Series | pd.DataFrame) -> list[str]:\n",
    "    \"\"\"\n",
    "    Normalize y to a list of column names for signature purposes.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    y : pd.Series | pd.DataFrame\n",
    "        The target variable(s) for the model.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    list[str]\n",
    "        The list of column names for the target variable(s).\n",
    "    \"\"\"\n",
    "    if isinstance(y, pd.DataFrame):\n",
    "        if y.shape[1] != 1:\n",
    "            raise ValueError(\"y must be a Series or single-column DataFrame for signature.\")\n",
    "        return [str(y.columns[0])]\n",
    "    name = getattr(y, \"name\", None)\n",
    "    return [str(name)] if name is not None else [\"y\"]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e0c708b",
   "metadata": {},
   "source": [
    "#### MPI Management"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "aa54ea7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def allocate_next_part(\n",
    "        base_dir: Path,\n",
    "        file_prefix: str,\n",
    "        width: int = 3,\n",
    "        ext: str = \"csv\",\n",
    "        max_retries: int = 32,\n",
    "        jitter_ms: tuple[int, int] = (1, 40),\n",
    ") -> Path:\n",
    "    \"\"\"\n",
    "    Atomically allocate the next rotating part file by creating it exclusively.\n",
    "\n",
    "    Uses os.open(..., O_CREAT|O_EXCL) so only one process can create a given part.\n",
    "    If another process wins the race, we re-scan and try the next part number.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    base_dir : Path\n",
    "        Directory to write part files into (created if missing).\n",
    "    file_prefix : str\n",
    "        Prefix used before \".partNNN.<ext>\".\n",
    "    width : int, default 3\n",
    "        Minimum zero-padding for part numbers if none exist.\n",
    "    ext : str, default \"csv\"\n",
    "        Extension without dot.\n",
    "    max_retries : int, default 32\n",
    "        Maximum attempts before giving up.\n",
    "    jitter_ms : (int, int), default (1, 40)\n",
    "        Random backoff (min,max) milliseconds between retries.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    Path\n",
    "        The newly created, zero-length part file path (claimed for you).\n",
    "\n",
    "    Raises\n",
    "    ------\n",
    "    RuntimeError\n",
    "        If a unique part file cannot be allocated within max_retries.\n",
    "    \"\"\"\n",
    "    base_dir = Path(base_dir)\n",
    "    base_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    for _ in range(max_retries):\n",
    "        path = _next_csv_part_path(base_dir, file_prefix, width=width, ext=ext)\n",
    "        flags = os.O_CREAT | os.O_EXCL | os.O_WRONLY\n",
    "        try:\n",
    "            fd = os.open(path, flags)  # atomic claim\n",
    "            os.close(fd)               # leave it for normal open() later\n",
    "            return path\n",
    "        except FileExistsError:\n",
    "            # Someone else grabbed it; small random backoff, then try again\n",
    "            time.sleep(random.uniform(*jitter_ms) / 1000.0)\n",
    "            continue\n",
    "\n",
    "    raise RuntimeError(\"Failed to allocate a unique part file after many attempts\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "488f17ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _distribute_configs(\n",
    "        configs: list[dict],\n",
    "        rank: int,\n",
    "        size: int,\n",
    "        mode: str = \"stride\"\n",
    ") -> list[dict]:\n",
    "    \"\"\"\n",
    "    Distribute configurations across multiple ranks.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    configs: list[dict]\n",
    "        The list of configurations to distribute.\n",
    "    rank: int\n",
    "        The rank of the current process.\n",
    "    size: int\n",
    "        The total number of processes.\n",
    "    mode: str\n",
    "        The distribution mode (\"stride\" or \"chunked\").\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    list[dict]\n",
    "        The distributed list of configurations.\n",
    "    \"\"\"\n",
    "    # Handle single process case\n",
    "    if size <= 1:\n",
    "        return configs\n",
    "    # Handle multi-process case\n",
    "    if mode == \"stride\":\n",
    "        return configs[rank::size]\n",
    "    # chunked\n",
    "    n = len(configs)\n",
    "    start = (n * rank) // size\n",
    "    end   = (n * (rank + 1)) // size\n",
    "    return configs[start:end]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "5b9a18b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "@contextmanager\n",
    "def _file_lock(lock_path: Path, max_wait_s: float = 30.0, jitter_ms: tuple[int,int]=(2,25)):\n",
    "    \"\"\"\n",
    "    Simple cross-process lock using O_CREAT|O_EXCL on a lockfile.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    lock_path : Path\n",
    "        Path to the lock file to create.\n",
    "    max_wait_s : float, default 30.0\n",
    "        Maximum time to wait for the lock before raising TimeoutError.\n",
    "    jitter_ms : (int,int), default (2,25)\n",
    "        Randomized backoff between retries, in milliseconds.\n",
    "\n",
    "    Yields\n",
    "    ------\n",
    "    None\n",
    "        The lock is held for the duration of the context.\n",
    "    \"\"\"\n",
    "    # Create the lock file\n",
    "    deadline = time.time() + float(max_wait_s)\n",
    "    lock_path = Path(lock_path)\n",
    "    last_err = None\n",
    "    # Wait for the lock to be available\n",
    "    while time.time() < deadline:\n",
    "        try:\n",
    "            fd = os.open(lock_path, os.O_CREAT | os.O_EXCL | os.O_WRONLY)\n",
    "            os.close(fd)\n",
    "            try:\n",
    "                yield\n",
    "            finally:\n",
    "                try:\n",
    "                    os.unlink(lock_path)\n",
    "                except FileNotFoundError:\n",
    "                    pass\n",
    "            return\n",
    "        except FileExistsError as e:\n",
    "            last_err = e\n",
    "            time.sleep(random.uniform(*jitter_ms) / 1000.0)\n",
    "    raise TimeoutError(f\"Could not acquire lock: {lock_path}\") from last_err"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "b2625a93",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _mpi_context():\n",
    "    \"\"\"\n",
    "    Get the MPI context for distributed training.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    Tuple[COMM, int, int]\n",
    "        The MPI communicator, rank, and size.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        from mpi4py import MPI  # ensures import\n",
    "        comm = MPI.COMM_WORLD\n",
    "        return comm, comm.Get_rank(), comm.Get_size()\n",
    "    except Exception:\n",
    "        class _Dummy:  # single-process stub\n",
    "            def bcast(self, x, root=0): return x\n",
    "            def Barrier(self): pass\n",
    "        return _Dummy(), 0, 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "724d06a5",
   "metadata": {},
   "source": [
    "#### Naming Conventions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "6e3f8eb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass(frozen=True)\n",
    "class PartNaming:\n",
    "    token: str = \".part\"   # separator between stem and index\n",
    "    width: int = 3         # zero-pad width\n",
    "    ext: str = \".csv\"      # file extension, with leading dot\n",
    "\n",
    "    def format(self,\n",
    "            stem: str,\n",
    "            idx: int\n",
    "    ) -> str:\n",
    "        \"\"\"\n",
    "        Format a part filename.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        stem : str\n",
    "            The base name of the file (without extension or part token).\n",
    "        idx : int\n",
    "            The part index (zero-padded).\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        str\n",
    "            The formatted part filename.\n",
    "        \"\"\"\n",
    "        return f\"{stem}{self.token}{idx:0{self.width}d}{self.ext}\"\n",
    "\n",
    "    def split(self,\n",
    "            name: str\n",
    "    ) -> Tuple[str, int | None]:\n",
    "        \"\"\"\n",
    "        Split a part filename into its stem and index.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        name : str\n",
    "            The part filename to split.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        Tuple[str, int | None]\n",
    "            The stem and index of the part filename.\n",
    "        \"\"\"\n",
    "        # returns (stem, idx) where idx is None if no part index present\n",
    "        if not name.endswith(self.ext):\n",
    "            # unknown extension; treat everything before first '.' as stem\n",
    "            p = Path(name)\n",
    "            return (p.stem, None)\n",
    "        base = name[: -len(self.ext)]\n",
    "        if self.token in base:\n",
    "            stem, idx_str = base.split(self.token, 1)\n",
    "            if idx_str.isdigit():\n",
    "                return stem, int(idx_str)\n",
    "        return base, None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec78db08",
   "metadata": {},
   "source": [
    "#### Path and Directory Management"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "dbdf1672",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _ensure_dir(\n",
    "        d: str | Path,\n",
    "        *,\n",
    "        resolve: bool = True\n",
    ") -> Path:\n",
    "    \"\"\"\n",
    "    Ensure directory `d` exists and return it as a Path.\n",
    "\n",
    "    - Creates parent directories as needed.\n",
    "    - Raises a clear error if a non-directory already exists at `d`.\n",
    "    - Optionally returns the resolved (absolute) path.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    d : str | Path\n",
    "        Directory path to create if missing.\n",
    "    resolve : bool, default True\n",
    "        If True, return Path.resolve(strict=False) to normalize/absolutize.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    Path\n",
    "        The (optionally resolved) directory path.\n",
    "    \"\"\"\n",
    "    p = Path(d)\n",
    "    if p.exists() and not p.is_dir():\n",
    "        raise NotADirectoryError(f\"Path exists and is not a directory: {p}\")\n",
    "    p.mkdir(parents=True, exist_ok=True)\n",
    "    return p.resolve(strict=False) if resolve else p\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "43d3f9f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _index_lock_path(index_path: Path) -> Path:\n",
    "    \"\"\"\n",
    "    Derive the lock file path for an index CSV (same directory, '.lock' suffix).\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    index_path : Path\n",
    "        Path to the index CSV file.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    Path\n",
    "        Path to the lock file.\n",
    "    \"\"\"\n",
    "    return index_path.with_suffix(index_path.suffix + \".lock\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "842df3e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _index_path(\n",
    "        base_dir: Path,\n",
    "        file_prefix: str\n",
    ") -> Path:\n",
    "    \"\"\"\n",
    "    Build the path to the global index CSV for a given rolling log set.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    base_dir : Path\n",
    "        Directory that holds the rolling CSV parts.\n",
    "    file_prefix : str\n",
    "        Prefix used by the rolling CSV (e.g., 'marginal_emissions_log').\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    Path\n",
    "        '<base_dir>/<file_prefix>_index.csv'\n",
    "    \"\"\"\n",
    "    return Path(base_dir) / f\"{file_prefix}_index.csv\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "1e87df59",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _next_csv_part_path(base_dir: Path, file_prefix: str, width: int = 3, ext: str = \"csv\") -> Path:\n",
    "    \"\"\"\n",
    "    Return the next available rotating-CSV part path.\n",
    "\n",
    "    Scans for files named \"<file_prefix>.partNNN.<ext>\" in `base_dir`, where NNN is an\n",
    "    integer with zero-padding. Picks max(N) and returns the next. If none exist, returns\n",
    "    \"...part000.<ext>\" (or the padding width you pass).\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    base_dir : Path\n",
    "        Directory to scan for part files.\n",
    "    file_prefix : str\n",
    "        Prefix used before \".partNNN.<ext>\".\n",
    "    width : int, default 3\n",
    "        Minimum zero-padding width if no files exist yet.\n",
    "    ext : str, default \"csv\"\n",
    "        File extension (without dot).\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    Path\n",
    "        Path for the next part file (not created).\n",
    "    \"\"\"\n",
    "    if width < 1:\n",
    "        raise ValueError(\"width must be >= 1\")\n",
    "\n",
    "    base_dir = Path(base_dir)\n",
    "    pattern = re.compile(rf\"^{re.escape(file_prefix)}\\.part(\\d+)\\.{re.escape(ext)}$\")\n",
    "\n",
    "    max_n = -1\n",
    "    pad = width\n",
    "\n",
    "    for p in base_dir.glob(f\"{file_prefix}.part*.{ext}\"):\n",
    "        m = pattern.match(p.name)\n",
    "        if not m:\n",
    "            continue\n",
    "        n_str = m.group(1)\n",
    "        pad = max(pad, len(n_str))\n",
    "        try:\n",
    "            n = int(n_str)\n",
    "        except ValueError:\n",
    "            continue\n",
    "        if n > max_n:\n",
    "            max_n = n\n",
    "\n",
    "    next_n = max_n + 1\n",
    "    n_str = f\"{next_n:0{pad}d}\"\n",
    "    return base_dir / f\"{file_prefix}.part{n_str}.{ext}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "eb47b71c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _roll_if_needed(\n",
    "        path: Path,\n",
    "        max_mb: int,\n",
    "        *,\n",
    "        naming: PartNaming | None = None\n",
    ") -> Path:\n",
    "    \"\"\"\n",
    "    If `path` exists and is >= max_mb, return the *next* part filename.\n",
    "    Otherwise return `path` unchanged.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    path : Path\n",
    "        Current part file path (e.g., 'prefix.part007.csv').\n",
    "    max_mb : int\n",
    "        Rotation threshold in mebibytes (MiB).\n",
    "    naming : PartNaming, optional\n",
    "        Naming convention (token/width/ext). Uses defaults if not provided.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    Path\n",
    "        Either `path` or a new sibling with incremented part index.\n",
    "    \"\"\"\n",
    "    if not path.exists() or _file_size_mb(path) < float(max_mb):\n",
    "        return path\n",
    "    naming = naming or PartNaming()\n",
    "    stem, idx = naming.split(path.name)\n",
    "    next_idx = (idx or 0) + 1\n",
    "    return path.with_name(naming.format(stem=stem, idx=next_idx))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7397f3e2",
   "metadata": {},
   "source": [
    "#### Scoring & Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "6f8d1cdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _compute_group_energy_weights(\n",
    "    df: pd.DataFrame,\n",
    "    group_col: str,\n",
    "    q_col: str,\n",
    "    interval_hours: float = 0.5,\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Aggregate energy weights by group.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pd.DataFrame\n",
    "        Rows for a single split after preprocessing (must contain `group_col` and `q_col`).\n",
    "    group_col : str\n",
    "        Name of the group id column (e.g., 'median_group_id', 'quantile_group_id').\n",
    "    q_col : str\n",
    "        Name of the demand/quantity column used as Q in the regression (usually x_vars[0]).\n",
    "    interval_hours : float, default 0.5\n",
    "        Duration represented by each row in hours (half-hourly = 0.5).\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pd.DataFrame\n",
    "        Columns: [group_col, 'q_sum', 'energy_MWh']\n",
    "        where energy_MWh = q_sum * interval_hours.\n",
    "    \"\"\"\n",
    "    if group_col not in df.columns:\n",
    "        raise KeyError(f\"'{group_col}' not found in df\")\n",
    "    if q_col not in df.columns:\n",
    "        raise KeyError(f\"'{q_col}' not found in df\")\n",
    "    if not np.issubdtype(np.asarray(df[q_col]).dtype, np.number):\n",
    "        raise TypeError(f\"'{q_col}' must be numeric\")\n",
    "    if interval_hours <= 0:\n",
    "        raise ValueError(\"interval_hours must be > 0\")\n",
    "\n",
    "    g = (\n",
    "        df.groupby(group_col, observed=True)[q_col]\n",
    "          .sum()\n",
    "          .rename(\"q_sum\")\n",
    "          .reset_index()\n",
    "    )\n",
    "    g[\"energy_MWh\"] = g[\"q_sum\"] * float(interval_hours)\n",
    "    return g\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "6da05a8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def finite_difference_me_metrics(\n",
    "    df: pd.DataFrame,\n",
    "    time_col: str = \"timestamp\",\n",
    "    q_col: str = \"demand_met\",\n",
    "    y_col: str = \"tons_co2\",\n",
    "    me_col: str = \"ME\",\n",
    "    group_keys: list[str] | tuple[str, ...] = (\"city\",),\n",
    "    max_dt: pd.Timedelta = pd.Timedelta(\"2h\"),\n",
    "    min_abs_dq: float = 1e-6,\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Compare predicted ME to observed short-horizon slopes s = Δy/ΔQ on held-out data.\n",
    "\n",
    "    For each group in `group_keys`:\n",
    "      Δy = y_t - y_{t-1}, ΔQ = Q_t - Q_{t-1}, Δt = t - t_{t-1}\n",
    "      Keep pairs with Δt ≤ max_dt and |ΔQ| ≥ min_abs_dq.\n",
    "      s_t = Δy / ΔQ, ME_avg = 0.5*(ME_t + ME_{t-1})\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pd.DataFrame\n",
    "        One row per group and an optional pooled 'ALL' row:\n",
    "        ['pearson_r','spearman_r','rmse','mae','n_pairs', *group_keys]\n",
    "    \"\"\"\n",
    "    if time_col not in df.columns:\n",
    "        raise KeyError(f\"'{time_col}' not in df\")\n",
    "    # ensure datetime for Δt filtering\n",
    "    dt_series = pd.to_datetime(df[time_col], errors=\"coerce\")\n",
    "    if dt_series.isna().any():\n",
    "        raise ValueError(f\"Column '{time_col}' contains non-parseable datetimes\")\n",
    "    work = df.copy()\n",
    "    work[time_col] = dt_series\n",
    "\n",
    "    def _per_group(gdf: pd.DataFrame) -> dict:\n",
    "        gdf = gdf.sort_values(time_col).copy()\n",
    "        gdf[\"dt\"] = gdf[time_col].diff()\n",
    "        gdf[\"dQ\"] = gdf[q_col].diff()\n",
    "        gdf[\"dY\"] = gdf[y_col].diff()\n",
    "        gdf[\"ME_avg\"] = 0.5 * (gdf[me_col] + gdf[me_col].shift(1))\n",
    "\n",
    "        mask = (\n",
    "            gdf[\"dt\"].notna() & (gdf[\"dt\"] <= max_dt)\n",
    "            & gdf[\"dQ\"].notna() & (np.abs(gdf[\"dQ\"]) >= float(min_abs_dq))\n",
    "            & gdf[\"dY\"].notna() & gdf[\"ME_avg\"].notna()\n",
    "        )\n",
    "        sub = gdf.loc[mask, [\"dY\", \"dQ\", \"ME_avg\"]]\n",
    "        if sub.empty:\n",
    "            return {\"pearson_r\": np.nan, \"spearman_r\": np.nan, \"rmse\": np.nan, \"mae\": np.nan, \"n_pairs\": 0}\n",
    "\n",
    "        s = sub[\"dY\"].to_numpy(dtype=float) / sub[\"dQ\"].to_numpy(dtype=float)\n",
    "        me = sub[\"ME_avg\"].to_numpy(dtype=float)\n",
    "        return {\n",
    "            \"pearson_r\": float(pd.Series(s).corr(pd.Series(me))),\n",
    "            \"spearman_r\": float(pd.Series(s).corr(pd.Series(me), method=\"spearman\")),\n",
    "            \"rmse\": float(root_mean_squared_error(s, me)),\n",
    "            \"mae\": float(mean_absolute_error(s, me)),\n",
    "            \"n_pairs\": int(len(sub)),\n",
    "        }\n",
    "\n",
    "    parts: list[dict] = []\n",
    "    if group_keys:\n",
    "        for keys, gdf in work.groupby(list(group_keys), observed=True, sort=True):\n",
    "            row = _per_group(gdf)\n",
    "            if isinstance(keys, tuple):\n",
    "                for kname, kval in zip(group_keys, keys):\n",
    "                    row[kname] = kval\n",
    "            else:\n",
    "                row[group_keys[0]] = keys\n",
    "            parts.append(row)\n",
    "    else:\n",
    "        parts.append(_per_group(work) | {\"group\": \"ALL\"})\n",
    "\n",
    "    out = pd.DataFrame(parts)\n",
    "\n",
    "    # pooled row\n",
    "    if group_keys and (not out.empty) and out[\"n_pairs\"].sum() > 0:\n",
    "        tmp = []\n",
    "        for _, gdf in work.groupby(list(group_keys), observed=True, sort=True):\n",
    "            gdf = gdf.sort_values(time_col).copy()\n",
    "            gdf[\"dt\"] = gdf[time_col].diff()\n",
    "            gdf[\"dQ\"] = gdf[q_col].diff()\n",
    "            gdf[\"dY\"] = gdf[y_col].diff()\n",
    "            gdf[\"ME_avg\"] = 0.5 * (gdf[me_col] + gdf[me_col].shift(1))\n",
    "            mask = (\n",
    "                gdf[\"dt\"].notna() & (gdf[\"dt\"] <= max_dt)\n",
    "                & gdf[\"dQ\"].notna() & (np.abs(gdf[\"dQ\"]) >= float(min_abs_dq))\n",
    "                & gdf[\"dY\"].notna() & gdf[\"ME_avg\"].notna()\n",
    "            )\n",
    "            sub = gdf.loc[mask, [\"dY\", \"dQ\", \"ME_avg\"]]\n",
    "            if not sub.empty:\n",
    "                tmp.append(\n",
    "                    pd.DataFrame({\n",
    "                        \"s\": sub[\"dY\"].to_numpy(dtype=float) / sub[\"dQ\"].to_numpy(dtype=float),\n",
    "                        \"ME_avg\": sub[\"ME_avg\"].to_numpy(dtype=float),\n",
    "                    })\n",
    "                )\n",
    "        if tmp:\n",
    "            pooled = pd.concat(tmp, ignore_index=True)\n",
    "            pooled_row = {\n",
    "                \"pearson_r\": float(pooled[\"s\"].corr(pooled[\"ME_avg\"])),\n",
    "                \"spearman_r\": float(pooled[\"s\"].corr(pooled[\"ME_avg\"], method=\"spearman\")),\n",
    "                \"rmse\": float(root_mean_squared_error(pooled[\"s\"], pooled[\"ME_avg\"])),\n",
    "                \"mae\": float(mean_absolute_error(pooled[\"s\"], pooled[\"ME_avg\"])),\n",
    "                \"n_pairs\": int(len(pooled)),\n",
    "            }\n",
    "            for k in group_keys:\n",
    "                pooled_row[k] = \"ALL\"\n",
    "            out = pd.concat([out, pd.DataFrame([pooled_row])], ignore_index=True)\n",
    "\n",
    "    return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "efa7017a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def macro_micro_means(df: pd.DataFrame, metric: str, weight_col: str = \"n_obs\") -> dict:\n",
    "    \"\"\"\n",
    "    Compute macro (simple mean) and micro (weighted by `weight_col`) for a metric.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pd.DataFrame\n",
    "        Per-group metrics.\n",
    "    metric : str\n",
    "        Column name to average.\n",
    "    weight_col : str, default \"n_obs\"\n",
    "        Column to use as weights for micro average.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    dict\n",
    "        {\"macro\": float, \"micro\": float}\n",
    "    \"\"\"\n",
    "    macro = float(np.nanmean(df[metric].to_numpy(dtype=float)))\n",
    "    if (weight_col in df) and np.nansum(df[weight_col].to_numpy(dtype=float)) > 0:\n",
    "        micro = float(np.average(df[metric], weights=df[weight_col]))\n",
    "    else:\n",
    "        micro = np.nan\n",
    "    return {\"macro\": macro, \"micro\": micro}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "d86c328c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_absolute_percentage_error(\n",
    "        y_true,\n",
    "        y_pred,\n",
    "        eps: float = 1e-6\n",
    ") -> float:\n",
    "    \"\"\"\n",
    "    Compute MAPE robustly - adding small constant to avoid division by zero.\n",
    "\n",
    "    MAPE = mean(|(y_true - y_pred) / (|y_true| + eps)|) * 100\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    y_true : array-like\n",
    "        Ground-truth values.\n",
    "    y_pred : array-like\n",
    "        Predicted values.\n",
    "    eps : float, default 1e-6\n",
    "        Small constant to avoid division by zero.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    float\n",
    "        Mean absolute percentage error in percent.\n",
    "    \"\"\"\n",
    "    # true values for y\n",
    "    yt = np.asarray(y_true, dtype=float)\n",
    "    # predicted values for y\n",
    "    yp = np.asarray(y_pred, dtype=float)\n",
    "    # denominator\n",
    "    denom = np.abs(yt) + float(eps)\n",
    "    # compute MAPE\n",
    "    m = np.abs((yt - yp) / denom)\n",
    "    # return as percentage (*100)\n",
    "    return float(np.nanmean(m) * 100.0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "2ac69557",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_metric(df: pd.DataFrame, metric: str) -> float:\n",
    "    \"\"\"\n",
    "    Compute the mean of a metric, with a special case for MSE derived from RMSE.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pd.DataFrame\n",
    "        DataFrame containing metric columns.\n",
    "    metric : {\"r2\",\"rmse\",\"mae\",\"mape\",\"n_obs\",\"mse\"}\n",
    "        Metric to aggregate.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    float\n",
    "        NaN-safe mean of the requested metric.\n",
    "\n",
    "    Raises\n",
    "    ------\n",
    "    KeyError\n",
    "        If required columns are missing.\n",
    "    \"\"\"\n",
    "    if metric == \"mse\":\n",
    "        if \"rmse\" not in df:\n",
    "            raise KeyError(\"Cannot compute 'mse': 'rmse' column missing.\")\n",
    "        return float(np.nanmean(df[\"rmse\"].to_numpy(dtype=float) ** 2))\n",
    "    if metric not in df:\n",
    "        raise KeyError(f\"Metric '{metric}' not found in DataFrame.\")\n",
    "    return float(np.nanmean(df[metric].to_numpy(dtype=float)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "b7dc326b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pooled_co2_metrics(\n",
    "    regressor,                  # fitted GroupwiseRegressor\n",
    "    transformed_df: pd.DataFrame,\n",
    "    y_col: str | None = None,\n",
    "    group_col: str | None = None,\n",
    ") -> dict:\n",
    "    \"\"\"\n",
    "    Compute pooled (all bins together) out-of-sample metrics for CO2.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    regressor : GroupwiseRegressor\n",
    "        Must be fitted; `regressor.group_models_` is used per group.\n",
    "    transformed_df : pd.DataFrame\n",
    "        Contains features used by the regressor, the group column, and the true y.\n",
    "        (Typically validation/test X after feature+binner, with y added).\n",
    "    y_col : str, optional\n",
    "        Target column name. Defaults to regressor.y_var.\n",
    "    group_col : str, optional\n",
    "        Group column name. Defaults to regressor.group_col.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    dict\n",
    "        {'r2','rmse','mae','mape','n_obs'} (NaNs if insufficient data).\n",
    "    \"\"\"\n",
    "    y_col = y_col or regressor.y_var\n",
    "    group_col = group_col or regressor.group_col\n",
    "    if y_col not in transformed_df.columns:\n",
    "        raise KeyError(f\"'{y_col}' not found in transformed_df\")\n",
    "    if group_col not in transformed_df.columns:\n",
    "        raise KeyError(f\"'{group_col}' not found in transformed_df\")\n",
    "\n",
    "    preds = pd.Series(index=transformed_df.index, dtype=float)\n",
    "    for g, gdf in transformed_df.groupby(group_col, sort=True):\n",
    "        model = regressor.group_models_.get(g)\n",
    "        if model is None:\n",
    "            continue\n",
    "        preds.loc[gdf.index] = model.predict(gdf)\n",
    "\n",
    "    mask = preds.notna()\n",
    "    n_obs = int(mask.sum())\n",
    "    if n_obs == 0:\n",
    "        return {\"r2\": np.nan, \"rmse\": np.nan, \"mae\": np.nan, \"mape\": np.nan, \"n_obs\": 0}\n",
    "\n",
    "    y_true = transformed_df.loc[mask, y_col].to_numpy(dtype=float)\n",
    "    y_pred = preds.loc[mask].to_numpy(dtype=float)\n",
    "\n",
    "    # r2 can error for <2 samples or constant y\n",
    "    try:\n",
    "        r2 = float(r2_score(y_true, y_pred))\n",
    "    except Exception:\n",
    "        r2 = np.nan\n",
    "\n",
    "    return {\n",
    "        \"r2\": r2,\n",
    "        \"rmse\": float(root_mean_squared_error(y_true, y_pred)),\n",
    "        \"mae\": float(mean_absolute_error(y_true, y_pred)),\n",
    "        \"mape\": float(mean_absolute_percentage_error(y_true, y_pred)),\n",
    "        \"n_obs\": n_obs,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "1623002c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarise_metrics_logs(\n",
    "        train_logs: pd.DataFrame,\n",
    "        val_logs: pd.DataFrame,\n",
    "        test_logs: pd.DataFrame | None = None,\n",
    "        user_pipeline: Pipeline = None,\n",
    "        x_columns: list | None = None,\n",
    "        random_state: int = 12,\n",
    "        group_col_name: str = \"group\",\n",
    "        pooled_metrics_by_split: dict[str, dict] | None = None,\n",
    "        fd_me_metrics_by_split: dict[str, dict] | None = None,\n",
    "        energy_weight_col: str = \"energy_MWh\",\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Summarise per-split, per-group metrics and pipeline metadata into a single-row DataFrame.\n",
    "\n",
    "    This variant allows `test_logs` to be None (can skip test during tuning).\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    train_logs, val_logs : pd.DataFrame\n",
    "        Metrics frames for train/validation.\n",
    "    test_logs : pd.DataFrame or None, default None\n",
    "        Test metrics; if None, test columns are omitted from the summary.\n",
    "    user_pipeline : Pipeline\n",
    "        The fitted or configured pipeline (used for metadata).\n",
    "    x_columns : list, optional\n",
    "        Feature names used by the model.\n",
    "    random_state : int, default 12\n",
    "        Random seed to record.\n",
    "    group_col_name : str, default \"group\"\n",
    "        Canonical name for the group column.\n",
    "    pooled_metrics_by_split, fd_me_metrics_by_split : dict, optional\n",
    "        Optional extra diagnostics keyed by split.\n",
    "    energy_weight_col : str, default \"energy_MWh\"\n",
    "        Column name to use for energy-weighted micro-averages if present.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pd.DataFrame\n",
    "        One-row summary. Only includes split columns for the splits provided.\n",
    "    \"\"\"\n",
    "    def _norm(df: pd.DataFrame) -> pd.DataFrame:\n",
    "        if df is None or df.empty:\n",
    "            return df\n",
    "\n",
    "        cols = list(df.columns)\n",
    "\n",
    "        # If desired already present, use it\n",
    "        if group_col_name in cols:\n",
    "            return df\n",
    "\n",
    "        # If a plain 'group' exists, rename it to the desired name\n",
    "        if \"group\" in cols:\n",
    "            return df.rename(columns={\"group\": group_col_name})\n",
    "\n",
    "        # Known aliases we can rename from\n",
    "        candidates = [\n",
    "            \"multi_group_id\",\n",
    "            \"quantile_group_id\",\n",
    "            \"median_group_id\",\n",
    "            \"original_quantile_group_id\",\n",
    "            \"group_id\",\n",
    "        ]\n",
    "\n",
    "        # Any *_group_id pattern\n",
    "        pattern_hits = [c for c in cols if c.endswith(\"_group_id\")]\n",
    "\n",
    "        # Prefer known aliases in order\n",
    "        for c in candidates:\n",
    "            if c in cols:\n",
    "                return df.rename(columns={c: group_col_name})\n",
    "\n",
    "        # If exactly one *_group_id exists, use it\n",
    "        if len(pattern_hits) == 1:\n",
    "            return df.rename(columns={pattern_hits[0]: group_col_name})\n",
    "\n",
    "        # Nothing we recognize → fail loudly with context\n",
    "        raise KeyError(\n",
    "            f\"Could not locate a group column; expected '{group_col_name}' or any of \"\n",
    "            f\"{[c for c in candidates if c in cols] + (['group'] if 'group' in cols else []) or candidates + ['group']}. \"\n",
    "            f\"Available columns: {cols}\"\n",
    "        )\n",
    "    splits: dict[str, pd.DataFrame] = {\n",
    "        \"train\": _norm(train_logs.copy()),\n",
    "        \"validation\": _norm(val_logs.copy()),\n",
    "    }\n",
    "    if test_logs is not None:\n",
    "        splits[\"test\"] = _norm(test_logs.copy())\n",
    "\n",
    "    required = {\"r2\", \"rmse\", \"mae\", \"mape\", \"n_obs\"}\n",
    "    for name, df in splits.items():\n",
    "        missing = required.difference(df.columns)\n",
    "        if missing:\n",
    "            raise ValueError(f\"{name} logs missing metrics: {sorted(missing)}\")\n",
    "\n",
    "    first = next(iter(splits.values()))\n",
    "    model_id = first.get(\"model_id_hash\", pd.Series([np.nan])).iloc[0]\n",
    "    log_time = first.get(\"log_time\", pd.Series([np.nan])).iloc[0]\n",
    "    model_name = user_pipeline._final_estimator.__class__.__name__ if user_pipeline is not None else \"\"\n",
    "    pipeline_steps = list(user_pipeline.named_steps.keys()) if user_pipeline is not None else []\n",
    "\n",
    "    summary: dict[str, Any] = {\n",
    "        \"model_id_hash\": model_id,\n",
    "        \"random_state\": random_state,\n",
    "        \"params_json\": json.dumps(\n",
    "            user_pipeline.get_params(deep=True), sort_keys=True, separators=(\",\", \":\"), default=str\n",
    "        ) if user_pipeline is not None else \"{}\",\n",
    "        \"log_time\": log_time,\n",
    "        \"model_name\": model_name,\n",
    "        \"pipeline_steps\": pipeline_steps,\n",
    "        \"pipeline_n_steps\": len(pipeline_steps),\n",
    "        \"x_columns\": x_columns or [],\n",
    "        \"metrics_by_group\": {},\n",
    "    }\n",
    "\n",
    "    nested: dict[str, dict] = {}\n",
    "    for split, df in splits.items():\n",
    "        # macro means\n",
    "        summary[f\"r2_{split}\"] = float(df[\"r2\"].mean())\n",
    "        summary[f\"rmse_{split}\"] = float(df[\"rmse\"].mean())\n",
    "        summary[f\"mae_{split}\"] = float(df[\"mae\"].mean())\n",
    "        summary[f\"mape_{split}\"] = float(df[\"mape\"].mean())\n",
    "        # counts should be sums, not means\n",
    "        summary[f\"n_obs_{split}\"] = int(df[\"n_obs\"].sum())\n",
    "        summary[f\"mse_{split}\"] = float((df[\"rmse\"] ** 2).mean())\n",
    "\n",
    "        # micro by n_obs\n",
    "        if df[\"n_obs\"].sum() > 0:\n",
    "            w = df[\"n_obs\"].to_numpy(dtype=float)\n",
    "            summary[f\"r2_{split}_micro\"] = float(np.average(df[\"r2\"], weights=w))\n",
    "            summary[f\"rmse_{split}_micro\"] = float(np.average(df[\"rmse\"], weights=w))\n",
    "            summary[f\"mae_{split}_micro\"] = float(np.average(df[\"mae\"], weights=w))\n",
    "            summary[f\"mape_{split}_micro\"] = float(np.average(df[\"mape\"], weights=w))\n",
    "        else:\n",
    "            summary[f\"r2_{split}_micro\"] = np.nan\n",
    "            summary[f\"rmse_{split}_micro\"] = np.nan\n",
    "            summary[f\"mae_{split}_micro\"] = np.nan\n",
    "            summary[f\"mape_{split}_micro\"] = np.nan\n",
    "\n",
    "        # energy-weighted micro (if provided)\n",
    "        if (energy_weight_col in df.columns) and (df[energy_weight_col].fillna(0).sum() > 0):\n",
    "            wE = df[energy_weight_col].fillna(0).to_numpy(dtype=float)\n",
    "            summary[f\"r2_{split}_energy_micro\"] = float(np.average(df[\"r2\"], weights=wE))\n",
    "            summary[f\"rmse_{split}_energy_micro\"] = float(np.average(df[\"rmse\"], weights=wE))\n",
    "            summary[f\"mae_{split}_energy_micro\"] = float(np.average(df[\"mae\"], weights=wE))\n",
    "            summary[f\"mape_{split}_energy_micro\"] = float(np.average(df[\"mape\"], weights=wE))\n",
    "            summary[f\"{energy_weight_col}_{split}_total\"] = float(wE.sum())\n",
    "        else:\n",
    "            summary[f\"r2_{split}_energy_micro\"] = np.nan\n",
    "            summary[f\"rmse_{split}_energy_micro\"] = np.nan\n",
    "            summary[f\"mae_{split}_energy_micro\"] = np.nan\n",
    "            summary[f\"mape_{split}_energy_micro\"] = np.nan\n",
    "            summary[f\"{energy_weight_col}_{split}_total\"] = 0.0\n",
    "\n",
    "        cols = [\"r2\", \"rmse\", \"mae\", \"mape\", \"n_obs\"]\n",
    "        if energy_weight_col in df.columns:\n",
    "            cols.append(energy_weight_col)\n",
    "        nested[split] = df.set_index(group_col_name)[cols].to_dict(orient=\"index\")\n",
    "\n",
    "    summary[\"metrics_by_group\"] = nested\n",
    "\n",
    "    pooled_metrics_by_split = pooled_metrics_by_split or {}\n",
    "    fd_me_metrics_by_split = fd_me_metrics_by_split or {}\n",
    "    for split in splits.keys():\n",
    "        summary[f\"pooled_co2_{split}\"] = json.dumps(pooled_metrics_by_split.get(split, {}))\n",
    "        summary[f\"fd_me_{split}\"] = json.dumps(fd_me_metrics_by_split.get(split, {}))\n",
    "\n",
    "    return pd.DataFrame([summary])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a77c9ad9",
   "metadata": {},
   "source": [
    "### Transformers / Classes "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc771dd5",
   "metadata": {},
   "source": [
    "#### Feature Engineering Transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "9c6a3318",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AnalysisFeatureAdder(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"\n",
    "    Add core temporal and quantitative features used in the original analysis.\n",
    "\n",
    "    Adds:\n",
    "      - time_id:              HH-MM string from `timestamp_col`\n",
    "      - <Q>_sqrd:             square of `demand_met_col`\n",
    "      - log_<Q>:              log(demand_met + ε)\n",
    "      - log_<Q>_sqrd:         (log_<Q>)^2\n",
    "      - log_<CO2>:            log(tons_co2 + ε) (only if `co2_col` present)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        timestamp_col: str = \"timestamp\",\n",
    "        demand_met_col: str = \"demand_met\",\n",
    "        co2_col: str = \"tons_co2\",\n",
    "        epsilon: float = 1e-6,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        timestamp_col : str\n",
    "            Name of the datetime column (parseable by pandas).\n",
    "        demand_met_col : str\n",
    "            Name of the demand column.\n",
    "        co2_col : str\n",
    "            Name of the CO2 column (optional at transform time).\n",
    "        epsilon : float, default 1e-6\n",
    "            Small constant to avoid log(0).\n",
    "        \"\"\"\n",
    "        if not isinstance(timestamp_col, str):\n",
    "            raise ValueError(\"timestamp_col must be a string\")\n",
    "        if not isinstance(demand_met_col, str):\n",
    "            raise ValueError(\"demand_met_col must be a string\")\n",
    "        if not isinstance(co2_col, str):\n",
    "            raise ValueError(\"co2_col must be a string\")\n",
    "        if not isinstance(epsilon, (float, int)):\n",
    "            raise ValueError(\"epsilon must be a float or int\")\n",
    "\n",
    "        self.timestamp_col = timestamp_col\n",
    "        self.demand_met_col = demand_met_col\n",
    "        self.co2_col = co2_col\n",
    "        self.epsilon = float(epsilon)\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        if not isinstance(X, pd.DataFrame):\n",
    "            raise TypeError(\"Input must be a pandas DataFrame\")\n",
    "        for col in [self.timestamp_col, self.demand_met_col]:\n",
    "            if col not in X.columns:\n",
    "                raise ValueError(f\"Missing required column '{col}' in input DataFrame\")\n",
    "        self.n_features_in_ = X.shape[1]\n",
    "        self.is_fitted_ = True\n",
    "        return self\n",
    "\n",
    "    def transform(self, X: pd.DataFrame, y: pd.Series | None = None) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : pd.DataFrame\n",
    "            Must contain `timestamp_col` and `demand_met_col`.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        pd.DataFrame\n",
    "            Copy of X with additional feature columns.\n",
    "        \"\"\"\n",
    "        if not isinstance(X, pd.DataFrame):\n",
    "            raise TypeError(\"Input must be a pandas DataFrame\")\n",
    "\n",
    "        df = X.copy()\n",
    "\n",
    "        for col in [self.timestamp_col, self.demand_met_col]:\n",
    "            if col not in df.columns:\n",
    "                raise ValueError(f\"Missing required column '{col}'\")\n",
    "\n",
    "        df[self.timestamp_col] = pd.to_datetime(df[self.timestamp_col], errors=\"coerce\")\n",
    "        if df[self.timestamp_col].isna().any():\n",
    "            raise ValueError(f\"Column '{self.timestamp_col}' contains non-parseable datetimes\")\n",
    "\n",
    "        # temporal\n",
    "        df[\"time_id\"] = df[self.timestamp_col].dt.strftime(\"%H-%M\").astype(\"string\")\n",
    "\n",
    "        # quantitative\n",
    "        q = self.demand_met_col\n",
    "        df[f\"{q}_sqrd\"] = df[q] ** 2\n",
    "        df[f\"log_{q}\"] = np.log(df[q] + self.epsilon)\n",
    "        df[f\"log_{q}_sqrd\"] = df[f\"log_{q}\"] ** 2\n",
    "\n",
    "        if self.co2_col in df.columns:\n",
    "            df[f\"log_{self.co2_col}\"] = np.log(df[self.co2_col] + self.epsilon)\n",
    "\n",
    "        return df\n",
    "\n",
    "    def get_feature_names_out(self, input_features=None):\n",
    "        base = []\n",
    "        base.append(\"time_id\")\n",
    "        base += [\n",
    "            f\"{self.demand_met_col}_sqrd\",\n",
    "            f\"log_{self.demand_met_col}\",\n",
    "            f\"log_{self.demand_met_col}_sqrd\",\n",
    "        ]\n",
    "        # optional; only present if co2 is in input\n",
    "        base.append(f\"log_{self.co2_col}\")\n",
    "        if input_features is not None:\n",
    "            return np.array(list(input_features) + base)\n",
    "        return np.array(base)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "b0f2e0c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DateTimeFeatureAdder(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"\n",
    "    Add datetime-based features from a timestamp column.\n",
    "\n",
    "    New columns:\n",
    "      - year (int)\n",
    "      - month (int)\n",
    "      - week_of_year (ISO week, int)\n",
    "      - day (int)\n",
    "      - hour (int)\n",
    "      - half_hour (0..47, int)\n",
    "      - day_of_week (1=Mon..7=Sun, int)\n",
    "      - is_weekend (0/1, int)\n",
    "\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    timestamp_col : str, default=\"timestamp\"\n",
    "        Name of the column containing datetime strings or pd.Timestamp.\n",
    "    drop_original : bool, default=True\n",
    "        Whether to drop the original timestamp column after extraction.\n",
    "\n",
    "    Raises\n",
    "    ------\n",
    "    TypeError\n",
    "        If `timestamp_col` is not found in the DataFrame.\n",
    "    KeyError\n",
    "        If `timestamp_col` is not present in X.\n",
    "\\\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        timestamp_col: str = \"timestamp\",\n",
    "        drop_original: bool = False,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Initialize the feature adder.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        timestamp_col : str\n",
    "            Column name to parse as datetime.\n",
    "        \"\"\"\n",
    "        if not isinstance(timestamp_col, str):\n",
    "            raise TypeError(\"timestamp_col must be a string.\")\n",
    "        if not isinstance(drop_original, bool):\n",
    "            raise TypeError(\"drop_original must be a bool.\")\n",
    "        self.timestamp_col = timestamp_col\n",
    "        self.drop_original = drop_original\n",
    "\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        \"\"\"\n",
    "        No-op fit. Exists for sklearn compatibility.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        self : DateTimeFeatureAdder\n",
    "        \"\"\"\n",
    "        if not isinstance(X, pd.DataFrame):\n",
    "            raise TypeError(\"Input X must be a pandas DataFrame.\")\n",
    "        if self.timestamp_col not in X.columns:\n",
    "            raise KeyError(f\"Column '{self.timestamp_col}' not found in DataFrame.\")\n",
    "\n",
    "        self.n_features_in_ = X.shape[1]\n",
    "        self.is_fitted_ = True\n",
    "        return self\n",
    "\n",
    "    def transform(self, X: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Transform X by adding:\n",
    "\n",
    "        - year (int)\n",
    "        - month (int)\n",
    "        - week_of_year (int)\n",
    "        - day (int)\n",
    "        - hour (int)\n",
    "        - half_hour (int, 0-47)\n",
    "        - day_of_week (int, 1=Mon)\n",
    "        - is_weekend (0/1)\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : pd.DataFrame\n",
    "            Input DataFrame with a column named `self.timestamp_col`.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        X_out : pd.DataFrame\n",
    "            Copy of X with the above new columns appended.\n",
    "\n",
    "        Raises\n",
    "        ------\n",
    "        KeyError\n",
    "            If `self.timestamp_col` is not present in X.\n",
    "        \"\"\"\n",
    "        df = X.copy()\n",
    "        # Attempt to convert the timestamp column to datetime (if not already)\n",
    "        try:\n",
    "            df[self.timestamp_col] = pd.to_datetime(df[self.timestamp_col], errors='raise')\n",
    "        except Exception as e:\n",
    "            raise TypeError(f\"Column '{self.timestamp_col}' could not be converted to datetime: {e}\")\n",
    "\n",
    "        dt = df[self.timestamp_col]\n",
    "        df[\"year\"] = dt.dt.year.astype('int32')\n",
    "        df[\"month\"] = dt.dt.month.astype('int32')\n",
    "        df[\"week_of_year\"] = dt.dt.isocalendar().week.astype('int32')\n",
    "        df[\"day\"] = dt.dt.day.astype('int32')\n",
    "        df[\"hour\"] = dt.dt.hour.astype('int32')\n",
    "        df[\"half_hour\"]    = (dt.dt.hour * 2 + (dt.dt.minute // 30)).astype(\"int32\")\n",
    "        df[\"day_of_week\"] = (dt.dt.dayofweek).astype('int32') + 1  # Monday=1\n",
    "        df[\"is_weekend\"] = (df[\"day_of_week\"] >= 6).astype('int32')\n",
    "\n",
    "        if self.drop_original:\n",
    "            df = df.drop(columns=[self.timestamp_col])\n",
    "\n",
    "        return df\n",
    "\n",
    "    def get_feature_names_out(self, input_features=None):\n",
    "        \"\"\"\n",
    "        Get the names of the output features.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        input_features : array-like, optional\n",
    "            The input feature names. If None, the original feature names are used.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        np.ndarray\n",
    "            The output feature names.\n",
    "        \"\"\"\n",
    "        added = [\"year\",\"month\",\"week_of_year\",\"day\",\"hour\",\"half_hour\",\"day_of_week\",\"is_weekend\"]\n",
    "        if self.drop_original or input_features is None:\n",
    "            base = [] if input_features is None else [c for c in input_features if c != self.timestamp_col]\n",
    "        else:\n",
    "            base = list(input_features)\n",
    "        return np.array(base + added, dtype=object)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "ff18f49c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GenerationShareAdder(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"\n",
    "    Add percentage‐share features for specified generation columns relative to a total.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    generation_cols : List[str]\n",
    "        Columns whose shares of `total_col` are computed.\n",
    "    total_col : str, default=\"total_generation\"\n",
    "        Denominator column.\n",
    "    suffix : str, default=\"_share\"\n",
    "        Suffix appended to new share columns.\n",
    "    as_percent : bool, default=True\n",
    "        If True, multiply shares by 100; otherwise keep as 0..1 fraction.\n",
    "    clip_0_100 : bool, default=False\n",
    "        If True and `as_percent=True`, clip results into [0, 100].\n",
    "        If True and `as_percent=False`, clip into [0, 1].\n",
    "\n",
    "    Raises\n",
    "    ------\n",
    "    TypeError\n",
    "        Bad argument types.\n",
    "    KeyError\n",
    "        Missing `generation_cols` or `total_col`.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        generation_cols: List[str],\n",
    "        total_col: str = \"total_generation\",\n",
    "        suffix: str = \"_share\",\n",
    "        as_percent: bool = True,\n",
    "        clip_0_100: bool = False,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Initialize the share adder.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        generation_cols : List[str]\n",
    "            Columns to convert into percentage shares.\n",
    "        total_col : str\n",
    "            Column used as the denominator in share calculation.\n",
    "        suffix : str\n",
    "            Suffix for the new share columns.\n",
    "\n",
    "        Raises\n",
    "        ------\n",
    "        TypeError\n",
    "            If `generation_cols` is not a list of strings, or if `total_col` or `suffix` are not strings.\n",
    "        \"\"\"\n",
    "        if not isinstance(generation_cols, list) or not all(isinstance(col, str) for col in generation_cols):\n",
    "            raise TypeError(\"generation_cols must be a list of strings.\")\n",
    "        if not isinstance(total_col, str):\n",
    "            raise TypeError(\"total_col must be a string.\")\n",
    "        if not isinstance(suffix, str):\n",
    "            raise TypeError(\"suffix must be a string.\")\n",
    "        if not isinstance(as_percent, bool):\n",
    "            raise TypeError(\"as_percent must be a bool.\")\n",
    "        if not isinstance(clip_0_100, bool):\n",
    "            raise TypeError(\"clip_0_100 must be a bool.\")\n",
    "\n",
    "        self.generation_cols = generation_cols\n",
    "        self.total_col = total_col\n",
    "        self.suffix = suffix\n",
    "        self.as_percent = as_percent\n",
    "        self.clip_0_100 = clip_0_100\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        \"\"\"\n",
    "        No‐op fit for compatibility with sklearn’s transformer API.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : pd.DataFrame\n",
    "            Input DataFrame.\n",
    "        y : Ignored\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        self : GenerationShareAdder\n",
    "\n",
    "        Raises\n",
    "        ------\n",
    "        TypeError\n",
    "            If `X` is not a pandas DataFrame.\n",
    "        KeyError\n",
    "            If any of the specified `generation_cols` or `total_col` is not present in the DataFrame.\n",
    "        \"\"\"\n",
    "        if not isinstance(X, pd.DataFrame):\n",
    "            raise TypeError(\"Input X must be a pandas DataFrame.\")\n",
    "        missing_cols = [col for col in self.generation_cols if col not in X.columns]\n",
    "        if missing_cols:\n",
    "            raise KeyError(f\"Generation columns {missing_cols} not found in input DataFrame.\")\n",
    "        if self.total_col not in X.columns:\n",
    "            raise KeyError(f\"Total column '{self.total_col}' not found in input DataFrame.\")\n",
    "        return self\n",
    "\n",
    "\n",
    "    def transform(self, X: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Compute and append share columns.\n",
    "\n",
    "        For each `col` in `generation_cols`, creates a new column\n",
    "        `col + suffix` = 100 * (X[col] / X[total_col]). Zeros in `total_col`\n",
    "        are treated as NaN to avoid division‐by‐zero.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : pd.DataFrame\n",
    "            Input DataFrame containing `generation_cols` and `total_col`.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        X_out : pd.DataFrame\n",
    "            Copy of X with additional `<col><suffix>` columns.\n",
    "\n",
    "        \"\"\"\n",
    "        df = X.copy()\n",
    "        # avoid integer division & div-by-zero\n",
    "        total = df[self.total_col].astype(\"float64\").replace({0.0: np.nan})\n",
    "        scale = 100.0 if self.as_percent else 1.0\n",
    "\n",
    "        for col in self.generation_cols:\n",
    "            share_col = f\"{col}{self.suffix}\"\n",
    "            df[share_col] = (df[col].astype(\"float64\") / total) * scale\n",
    "            if self.clip_0_100:\n",
    "                lo, hi = (0.0, 100.0) if self.as_percent else (0.0, 1.0)\n",
    "                df[share_col] = df[share_col].clip(lower=lo, upper=hi)\n",
    "\n",
    "        return df\n",
    "\n",
    "\n",
    "    def get_feature_names_out(self, input_features=None):\n",
    "        added = [f\"{c}{self.suffix}\" for c in self.generation_cols]\n",
    "        base = [] if input_features is None else list(input_features)\n",
    "        return np.array(base + added, dtype=object)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96dc5119",
   "metadata": {},
   "source": [
    "#### Multi-Quantile Binner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "5a1a9696",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiQuantileBinner(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"\n",
    "    Quantile bin multiple variables, then combine their per-variable bin IDs into\n",
    "    a single mixed-radix group ID (1-based).\n",
    "\n",
    "    Example: with bin_specs={'v1':5, 'v2':4}:\n",
    "      - Fit stores quantile edges for each var.\n",
    "      - Transform assigns v1_group∈{1..5}, v2_group∈{1..4},\n",
    "        then builds group_col_name = 1 + (v1_group-1)*4 + (v2_group-1)*1.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        bin_specs: dict[str, int],\n",
    "        group_col_name: str = \"quantile_group_id\",\n",
    "        retain_flags: bool = True,\n",
    "        oob_policy: str = \"clip\",\n",
    "        max_oob_rate: float | None = None,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        bin_specs : dict[str, int]\n",
    "            Mapping of variable -> # of quantile bins (positive integers).\n",
    "        group_col_name : str, default \"quantile_group_id\"\n",
    "            Output column for the combined mixed-radix group ID (1-based).\n",
    "        retain_flags : bool, default True\n",
    "            If True, keep per-variable `<var>_group` columns.\n",
    "        oob_policy : {\"clip\",\"edge\",\"error\"}, default \"clip\"\n",
    "            Handling for values falling outside learned edges at transform time:\n",
    "              - \"clip\": send to nearest bin (1 or max)\n",
    "              - \"edge\": send to the first bin\n",
    "              - \"error\": raise ValueError\n",
    "        max_oob_rate : float or None, default None\n",
    "            If set, raise an error when an individual variable sees\n",
    "            OOB rate > max_oob_rate during transform.\n",
    "        \"\"\"\n",
    "        if not isinstance(bin_specs, dict) or not bin_specs:\n",
    "            raise ValueError(\"bin_specs must be a non-empty dict\")\n",
    "        if oob_policy not in {\"clip\", \"edge\", \"error\"}:\n",
    "            raise ValueError(\"oob_policy must be one of {'clip','edge','error'}\")\n",
    "\n",
    "        self.bin_specs = self.validate_and_convert_bins(bin_specs)\n",
    "        self.group_col_name = str(group_col_name)\n",
    "        self.retain_flags = bool(retain_flags)\n",
    "        self.oob_policy = oob_policy\n",
    "        self.max_oob_rate = max_oob_rate\n",
    "\n",
    "        self.variables_: list[str] | None = None\n",
    "        self.quantile_edges_: dict[str, list[float]] = {}\n",
    "        self.bin_sizes_: dict[str, int] = {}\n",
    "        self.multipliers_: list[int] | None = None\n",
    "        self.oob_counts_: dict[str, int] = {}\n",
    "\n",
    "    def fit(self, X: pd.DataFrame, y=None):\n",
    "        \"\"\"\n",
    "        Learn quantile edges for each variable.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : pd.DataFrame\n",
    "            Must contain all variables in `bin_specs`.\n",
    "        \"\"\"\n",
    "        self.variables_ = list(self.bin_specs.keys())\n",
    "        self.quantile_edges_.clear()\n",
    "        self.bin_sizes_.clear()\n",
    "        self.oob_counts_.clear()\n",
    "\n",
    "        eps = 1e-4\n",
    "        for var in self.variables_:\n",
    "            n_bins = self.bin_specs[var]\n",
    "            if var not in X.columns:\n",
    "                raise ValueError(f\"Column '{var}' not found in X\")\n",
    "            qs = np.linspace(0, 1, n_bins + 1)\n",
    "            raw = X[var].quantile(qs, interpolation=\"midpoint\").values\n",
    "            vmin, vmax = X[var].min(), X[var].max()\n",
    "            edges = np.unique(np.concatenate([[vmin - eps], raw, [vmax + eps]]))\n",
    "            edges.sort()\n",
    "            self.quantile_edges_[var] = edges.tolist()\n",
    "            self.bin_sizes_[var] = len(edges) - 1\n",
    "\n",
    "        bases = [self.bin_sizes_[v] for v in self.variables_]\n",
    "        m = [1]\n",
    "        for b in reversed(bases[1:]):\n",
    "            m.insert(0, m[0] * b)\n",
    "        self.multipliers_ = m\n",
    "        return self\n",
    "\n",
    "    def transform(self, X: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Assign per-variable quantile bins and the combined group ID.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        pd.DataFrame\n",
    "            X plus `<var>_group` (optional) and `group_col_name`.\n",
    "        \"\"\"\n",
    "        if not self.quantile_edges_:\n",
    "            raise RuntimeError(\"Must fit binner before transform()\")\n",
    "        df = X.copy()\n",
    "        self.oob_counts_ = {var: 0 for var in self.variables_}\n",
    "\n",
    "        for var in self.variables_:\n",
    "            edges = self.quantile_edges_[var]\n",
    "            n = len(edges) - 1\n",
    "            s = pd.cut(df[var], bins=edges, labels=range(1, n + 1), include_lowest=True, right=True)\n",
    "\n",
    "            if s.isna().any():\n",
    "                n_oob = int(s.isna().sum())\n",
    "                self.oob_counts_[var] += n_oob\n",
    "                if self.oob_policy == \"error\":\n",
    "                    bad = df.loc[s.isna(), var].unique()\n",
    "                    raise ValueError(f\"OOB values for '{var}': {bad[:10]} ...\")\n",
    "                elif self.oob_policy == \"clip\":\n",
    "                    below = df[var] < edges[1]\n",
    "                    s = s.astype(\"Float64\")\n",
    "                    s.loc[s.isna() & below] = 1\n",
    "                    s.loc[s.isna() & ~below] = n\n",
    "                    s = s.astype(\"Int64\")\n",
    "                else:  # \"edge\"\n",
    "                    s = s.fillna(1)\n",
    "\n",
    "            df[f\"{var}_group\"] = s.astype(int)\n",
    "\n",
    "        total = len(df)\n",
    "        if self.max_oob_rate is not None and total > 0:\n",
    "            for var, cnt in self.oob_counts_.items():\n",
    "                rate = cnt / total\n",
    "                if rate > self.max_oob_rate:\n",
    "                    raise ValueError(\n",
    "                        f\"OOB rate {rate:.2%} exceeds max_oob_rate={self.max_oob_rate:.2%} for '{var}'\"\n",
    "                    )\n",
    "\n",
    "        df[self.group_col_name] = 1\n",
    "        for v, m in zip(self.variables_, self.multipliers_):\n",
    "            df[self.group_col_name] += (df[f\"{v}_group\"] - 1) * m\n",
    "\n",
    "        if not self.retain_flags:\n",
    "            df.drop(columns=[f\"{v}_group\" for v in self.variables_], inplace=True)\n",
    "\n",
    "        return df\n",
    "\n",
    "    @staticmethod\n",
    "    def validate_and_convert_bins(bin_specs: dict) -> dict[str, int]:\n",
    "        converted: dict[str, int] = {}\n",
    "        for k, v in bin_specs.items():\n",
    "            try:\n",
    "                v_int = int(float(v))\n",
    "                if v_int != float(v) or v_int <= 0:\n",
    "                    raise ValueError\n",
    "                converted[str(k)] = v_int\n",
    "            except (ValueError, TypeError) as e:\n",
    "                raise TypeError(f\"Bin spec '{k}' value '{v}' must be a positive integer\") from e\n",
    "        return converted\n",
    "\n",
    "    def get_feature_names_out(self, input_features=None):\n",
    "        names = []\n",
    "        if self.retain_flags and self.variables_:\n",
    "            names += [f\"{v}_group\" for v in self.variables_]\n",
    "        names.append(self.group_col_name)\n",
    "        if input_features is not None:\n",
    "            return np.array(list(input_features) + names)\n",
    "        return np.array(names)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64560abc",
   "metadata": {},
   "source": [
    "#### Multi-Median Binner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "f5f35608",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiMedianBinner(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"\n",
    "    Median-split each variable and combine flags into a 1-based group ID.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, variables: list[str], group_col_name: str = \"median_group_id\", retain_flags: bool = True):\n",
    "        if not isinstance(variables, list) or len(variables) == 0:\n",
    "            raise ValueError(\"`variables` must be a non-empty list of column names.\")\n",
    "        if any(not isinstance(v, str) for v in variables):\n",
    "            raise TypeError(\"All entries in `variables` must be strings.\")\n",
    "        if not isinstance(group_col_name, str) or not group_col_name:\n",
    "            raise TypeError(\"`group_col_name` must be a non-empty string.\")\n",
    "        if not isinstance(retain_flags, bool):\n",
    "            raise TypeError(\"`retain_flags` must be a boolean value.\")\n",
    "\n",
    "        self.variables = variables\n",
    "        self.group_col_name = group_col_name\n",
    "        self.retain_flags = retain_flags\n",
    "        self.medians_: dict[str, float] = {}\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        if not isinstance(X, pd.DataFrame):\n",
    "            raise TypeError(\"Input X must be a pandas DataFrame.\")\n",
    "        missing = [v for v in self.variables if v not in X.columns]\n",
    "        if missing:\n",
    "            raise ValueError(f\"Columns not found in input DataFrame: {missing}\")\n",
    "        self.medians_ = X[self.variables].median(skipna=True).to_dict()\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        \"\"\"\n",
    "        Returns\n",
    "        -------\n",
    "        pd.DataFrame\n",
    "            Copy of X with optional `<var>_group` flags (0/1) and `group_col_name`.\n",
    "        \"\"\"\n",
    "        if not self.medians_:\n",
    "            raise RuntimeError(\"Must call fit() before transform().\")\n",
    "        if not isinstance(X, pd.DataFrame):\n",
    "            raise TypeError(\"Input X must be a pandas DataFrame.\")\n",
    "        missing = [v for v in self.variables if v not in X.columns]\n",
    "        if missing:\n",
    "            raise ValueError(f\"Columns missing at transform time: {missing}\")\n",
    "\n",
    "        df = X.copy()\n",
    "        # compare each column to its scalar median (aligned by column name)\n",
    "        flags = (df[self.variables] > pd.Series(self.medians_)).astype(int)\n",
    "\n",
    "        multipliers = 2 ** np.arange(len(self.variables))[::-1]\n",
    "        df[self.group_col_name] = flags.values.dot(multipliers) + 1\n",
    "\n",
    "        if self.retain_flags:\n",
    "            for var in self.variables:\n",
    "                df[f\"{var}_group\"] = flags[var]\n",
    "\n",
    "        return df\n",
    "\n",
    "    def get_feature_names_out(self, input_features=None):\n",
    "        names = []\n",
    "        if self.retain_flags:\n",
    "            names += [f\"{v}_group\" for v in self.variables]\n",
    "        names.append(self.group_col_name)\n",
    "        if input_features is not None:\n",
    "            return np.array(list(input_features) + names)\n",
    "        return np.array(names)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a48a6341",
   "metadata": {},
   "source": [
    "#### GroupwiseRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "4bf4c192",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GroupwiseRegressor(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"\n",
    "    Runs separate OLS regressions in each group and computes marginal emission factors.\n",
    "\n",
    "    For each group k, we fit:\n",
    "        y_t = α₁ₖ · x₁_t + α₂ₖ · x₂_t + Σ β_i·C(f_i)_t + ε_t\n",
    "    and compute the marginal effect:\n",
    "        ME_t = ∂y_t/∂x₁_t = α₁ₖ + 2·α₂ₖ·x₁_t.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    y_var : str\n",
    "        Target column name (e.g. 'tons_co2').\n",
    "    x_vars : List[str]\n",
    "        Predictor columns; first is Q, second is Q².\n",
    "    fe_vars : List[str], optional\n",
    "        Categorical fixed-effect columns.\n",
    "    group_col : str\n",
    "        Column with integer group IDs.\n",
    "    min_group_size : int\n",
    "        Minimum observations per group to run regression.\n",
    "    track_metrics : bool\n",
    "        If True, store per-group models and metrics.\n",
    "    verbose : bool\n",
    "        If True, log progress and metrics.\n",
    "\n",
    "    Attributes\n",
    "    ----------\n",
    "    group_models_ : dict\n",
    "        Fitted statsmodels results per group (if track_metrics=True).\n",
    "    group_metrics_ : dict\n",
    "        Computed metrics per group (if track_metrics=True).\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        y_var: str = \"tons_co2\",\n",
    "        x_vars: List[str] = [\"total_generation\", \"total_generation_sqrd\"],\n",
    "        fe_vars: Optional[List[str]] = None,\n",
    "        group_col: str = \"k\",\n",
    "        min_group_size: int = 10,\n",
    "        track_metrics: bool = True,\n",
    "        verbose: bool = True,\n",
    "        random_state: int | None = 12,\n",
    "    ):\n",
    "        if not isinstance(y_var, str):\n",
    "            raise TypeError(\"y_var must be a string\")\n",
    "        if not isinstance(x_vars, list) or not x_vars or not all(isinstance(v, str) for v in x_vars):\n",
    "            raise TypeError(\"x_vars must be a non-empty list of strings\")\n",
    "        if fe_vars is not None and (not isinstance(fe_vars, list) or not all(isinstance(v, str) for v in fe_vars)):\n",
    "            raise TypeError(\"fe_vars must be a list of strings or None\")\n",
    "        if not isinstance(group_col, str):\n",
    "            raise TypeError(\"group_col must be a string\")\n",
    "        if not isinstance(min_group_size, int) or min_group_size < 1:\n",
    "            raise ValueError(\"min_group_size must be a positive integer\")\n",
    "        if not isinstance(track_metrics, bool):\n",
    "            raise TypeError(\"track_metrics must be a boolean\")\n",
    "        if not isinstance(verbose, bool):\n",
    "            raise TypeError(\"verbose must be a boolean\")\n",
    "\n",
    "        self.y_var = y_var\n",
    "        self.x_vars = x_vars\n",
    "        self.fe_vars = fe_vars or []\n",
    "        self.group_col = group_col\n",
    "        self.min_group_size = min_group_size\n",
    "        self.track_metrics = track_metrics\n",
    "        self.verbose = verbose\n",
    "        self.random_state = random_state\n",
    "        if self.track_metrics:\n",
    "            self.group_models_: dict[Any, Any] = {}\n",
    "            self.group_metrics_: dict[Any, dict[str, float]] = {}\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        if self.random_state is not None:\n",
    "            np.random.seed(self.random_state)\n",
    "        if not isinstance(X, pd.DataFrame):\n",
    "            raise TypeError(\"X must be a pandas DataFrame\")\n",
    "        if y is None:\n",
    "            raise ValueError(\"y must be provided for fitting\")\n",
    "        if len(X) != len(y):\n",
    "            raise ValueError(f\"X and y have different lengths: {len(X)} != {len(y)}\")\n",
    "\n",
    "        df = X.copy()\n",
    "        df[self.y_var] = np.asarray(y).reshape(-1)\n",
    "\n",
    "        # avoid uint in formula design matrix\n",
    "        uint_cols = [c for c in df.columns if str(df[c].dtype).startswith((\"uint\", \"UInt\"))]\n",
    "        if uint_cols:\n",
    "            df[uint_cols] = df[uint_cols].astype(\"int64\")\n",
    "\n",
    "        if self.track_metrics:\n",
    "            self.group_models_.clear()\n",
    "            self.group_metrics_.clear()\n",
    "\n",
    "        # cast FEs to ordered categoricals\n",
    "        if \"month\" in self.fe_vars:\n",
    "            df[\"month\"] = pd.Categorical(df[\"month\"].astype(int), categories=range(1, 13), ordered=True)\n",
    "        if \"hour\" in self.fe_vars:\n",
    "            df[\"hour\"] = pd.Categorical(df[\"hour\"].astype(int), categories=range(24), ordered=True)\n",
    "        if \"day_of_week\" in self.fe_vars:\n",
    "            df[\"day_of_week\"] = pd.Categorical(df[\"day_of_week\"].astype(int), categories=range(1, 8), ordered=True)\n",
    "        if \"week_of_year\" in self.fe_vars:\n",
    "            df[\"week_of_year\"] = pd.Categorical(df[\"week_of_year\"].astype(int), categories=range(1, 54), ordered=True)\n",
    "        if \"half_hour\" in self.fe_vars:\n",
    "            df[\"half_hour\"] = pd.Categorical(df[\"half_hour\"].astype(int), categories=range(0, 48), ordered=True)\n",
    "\n",
    "        self._fitted_groups: list[Any] = []\n",
    "\n",
    "        for grp, df_grp in df.groupby(self.group_col, sort=True):\n",
    "            n = len(df_grp)\n",
    "            if n < self.min_group_size:\n",
    "                if self.verbose:\n",
    "                    logging.warning(f\"Skipping group {grp!r}: only {n} < {self.min_group_size}\")\n",
    "                continue\n",
    "\n",
    "            reg = \" + \".join(self.x_vars)\n",
    "            fe = \" + \".join(f\"C({f})\" for f in self.fe_vars)\n",
    "            formula = f\"{self.y_var} ~ {reg}\" + (f\" + {fe}\" if fe else \"\")\n",
    "\n",
    "            model = smf.ols(formula, data=df_grp).fit()\n",
    "            self._fitted_groups.append(grp)\n",
    "\n",
    "            if self.track_metrics:\n",
    "                preds = model.predict(df_grp)\n",
    "                rmse = float(np.sqrt(np.mean((preds - df_grp[self.y_var]) ** 2)))\n",
    "                mae = float(np.mean(np.abs(preds - df_grp[self.y_var])))\n",
    "                mape = float(mean_absolute_percentage_error(df_grp[self.y_var], preds))\n",
    "                self.group_models_[grp] = model\n",
    "                self.group_metrics_[grp] = {\n",
    "                    \"r2\": float(model.rsquared),\n",
    "                    \"rmse\": rmse,\n",
    "                    \"mae\": mae,\n",
    "                    \"mape\": mape,\n",
    "                    \"n_obs\": int(n),\n",
    "                }\n",
    "\n",
    "        if not self._fitted_groups:\n",
    "            raise ValueError(\"No valid groups found for fitting.\")\n",
    "        return self\n",
    "\n",
    "    def transform(self, X: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Apply groupwise OLS and compute marginal effects ME_t.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : pd.DataFrame\n",
    "            Must contain y_var, x_vars, fe_vars, and group_col.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        pd.DataFrame\n",
    "            Original rows plus 'alpha1', 'alpha2', and 'ME'.\n",
    "\n",
    "        Raises\n",
    "        ------\n",
    "        TypeError\n",
    "            If X is not a pandas DataFrame.\n",
    "        ValueError\n",
    "            If required columns missing or no group qualifies.\n",
    "        \"\"\"\n",
    "        if not getattr(self, \"group_models_\", None):\n",
    "            raise RuntimeError(\"GroupwiseRegressor must be fit before transform/predict.\")\n",
    "        if not isinstance(X, pd.DataFrame):\n",
    "            raise TypeError(\"Input X must be a pandas DataFrame\")\n",
    "\n",
    "        df = X.copy()\n",
    "\n",
    "        # keep FE category casting consistent with fit\n",
    "        if \"month\" in self.fe_vars:\n",
    "            df[\"month\"] = pd.Categorical(df[\"month\"].astype(int), categories=range(1, 13), ordered=True)\n",
    "        if \"hour\" in self.fe_vars:\n",
    "            df[\"hour\"] = pd.Categorical(df[\"hour\"].astype(int), categories=range(24), ordered=True)\n",
    "        if \"day_of_week\" in self.fe_vars:\n",
    "            df[\"day_of_week\"] = pd.Categorical(df[\"day_of_week\"].astype(int), categories=range(1, 8), ordered=True)\n",
    "        if \"week_of_year\" in self.fe_vars:\n",
    "            df[\"week_of_year\"] = pd.Categorical(df[\"week_of_year\"].astype(int), categories=range(1, 54), ordered=True)\n",
    "        if \"half_hour\" in self.fe_vars:\n",
    "            df[\"half_hour\"] = pd.Categorical(df[\"half_hour\"].astype(int), categories=range(0, 48), ordered=True)\n",
    "\n",
    "        df[\"alpha1\"] = np.nan\n",
    "        df[\"alpha2\"] = np.nan\n",
    "        df[\"ME\"] = np.nan\n",
    "\n",
    "        for grp, df_grp in df.groupby(self.group_col, sort=True):\n",
    "            model = self.group_models_.get(grp)\n",
    "            if model is None:\n",
    "                continue\n",
    "            a1 = model.params.get(self.x_vars[0], np.nan)\n",
    "            a2 = model.params.get(self.x_vars[1], 0.0)\n",
    "            idx = df_grp.index\n",
    "\n",
    "            df.loc[idx, \"alpha1\"] = a1\n",
    "            df.loc[idx, \"alpha2\"] = a2\n",
    "            df.loc[idx, \"ME\"] = a1 + 2.0 * a2 * df_grp[self.x_vars[0]]\n",
    "\n",
    "        return df\n",
    "\n",
    "\n",
    "    def predict(self, X: pd.DataFrame, predict_type: str = \"ME\") -> pd.Series:\n",
    "        \"\"\"\n",
    "        Predict marginal effects (default) or CO2 for each row in X using fitted group models.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : pd.DataFrame\n",
    "            Must contain x_vars, fe_vars, and group_col.\n",
    "        predict_type : {\"ME\",\"y\"}, default \"ME\"\n",
    "            \"ME\": return α1 + 2*α2*Q\n",
    "            \"y\" : return model.predict(...) (CO2)\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        pd.Series\n",
    "            Predictions aligned to X.index.\n",
    "        \"\"\"\n",
    "\n",
    "        if not getattr(self, \"group_models_\", None):\n",
    "            raise RuntimeError(\"GroupwiseRegressor must be fit before predict().\")\n",
    "        if not isinstance(X, pd.DataFrame):\n",
    "            raise TypeError(\"X must be a pandas DataFrame\")\n",
    "\n",
    "        required = self.x_vars + self.fe_vars + [self.group_col]\n",
    "        missing = [c for c in required if c not in X.columns]\n",
    "        if missing:\n",
    "            raise ValueError(f\"Missing columns in input DataFrame: {missing}\")\n",
    "\n",
    "        df = X.copy()\n",
    "        # consistent FE casting\n",
    "        if \"month\" in self.fe_vars:\n",
    "            df[\"month\"] = pd.Categorical(df[\"month\"].astype(int), categories=range(1, 13), ordered=True)\n",
    "        if \"hour\" in self.fe_vars:\n",
    "            df[\"hour\"] = pd.Categorical(df[\"hour\"].astype(int), categories=range(24), ordered=True)\n",
    "        if \"day_of_week\" in self.fe_vars:\n",
    "            df[\"day_of_week\"] = pd.Categorical(df[\"day_of_week\"].astype(int), categories=range(1, 8), ordered=True)\n",
    "        if \"week_of_year\" in self.fe_vars:\n",
    "            df[\"week_of_year\"] = pd.Categorical(df[\"week_of_year\"].astype(int), categories=range(1, 54), ordered=True)\n",
    "        if \"half_hour\" in self.fe_vars:\n",
    "            df[\"half_hour\"] = pd.Categorical(df[\"half_hour\"].astype(int), categories=range(0, 48), ordered=True)\n",
    "\n",
    "        out = pd.Series(index=df.index, dtype=float)\n",
    "\n",
    "        for grp, df_grp in df.groupby(self.group_col, sort=True):\n",
    "            model = self.group_models_.get(grp)\n",
    "            if model is None:\n",
    "                continue\n",
    "\n",
    "            if predict_type == \"y\":\n",
    "                preds = model.predict(df_grp)\n",
    "            else:\n",
    "                a1 = model.params.get(self.x_vars[0], np.nan)\n",
    "                a2 = model.params.get(self.x_vars[1], 0.0)\n",
    "                Q = df_grp[self.x_vars[0]]\n",
    "                preds = a1 + 2.0 * a2 * Q\n",
    "\n",
    "            out.loc[df_grp.index] = preds\n",
    "\n",
    "        return out\n",
    "\n",
    "    def get_metrics(self, summarise: bool = True) -> Union[dict, pd.DataFrame]:\n",
    "        \"\"\"\n",
    "        Get the metrics for each group.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        summarise : bool, default=True\n",
    "            If True, return a summary DataFrame; otherwise return raw metrics dict.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        dict or pd.DataFrame\n",
    "            If summarise=True, returns a DataFrame with group metrics.\n",
    "            If False, returns the raw metrics dictionary.\n",
    "\n",
    "        Raises\n",
    "        ------\n",
    "        RuntimeError\n",
    "            If track_metrics was not set to True during initialization.\n",
    "        \"\"\"\n",
    "        if not self.track_metrics:\n",
    "            raise RuntimeError(\"Metrics tracking is disabled. Set track_metrics=True to enable.\")\n",
    "        if summarise:\n",
    "            df = pd.DataFrame.from_dict(self.group_metrics_, orient=\"index\")\n",
    "            df.index.name = self.group_col\n",
    "            df.reset_index(inplace=True)\n",
    "            return df\n",
    "        return self.group_metrics_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea0e710f",
   "metadata": {},
   "source": [
    "### Running Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3eabbda",
   "metadata": {},
   "source": [
    "#### Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "a511a2d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _apply_fitted_preprocessing(user_pipeline: Pipeline, X: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Apply all *already-fitted* steps in a pipeline except the final estimator,\n",
    "    without constructing a new sklearn Pipeline (avoids 'Pipeline not fitted' warnings).\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    user_pipeline : Pipeline\n",
    "        A pipeline that has already been fitted (on train) and whose final step\n",
    "        is the estimator (e.g., GroupwiseRegressor).\n",
    "    X : pd.DataFrame\n",
    "        Raw features to transform through the fitted preprocessing steps.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pd.DataFrame\n",
    "        The transformed features as a DataFrame. If a transformer returns a numpy array,\n",
    "        we try to retrieve column names via `get_feature_names_out()`; otherwise we fall\n",
    "        back to the original column names.\n",
    "    \"\"\"\n",
    "    Z = X\n",
    "    last_transformer = None\n",
    "\n",
    "    for _, step in user_pipeline.steps[:-1]:\n",
    "        if hasattr(step, \"transform\"):\n",
    "            Z = step.transform(Z)\n",
    "            last_transformer = step\n",
    "\n",
    "    if isinstance(Z, pd.DataFrame):\n",
    "        return Z\n",
    "\n",
    "    # Try to recover column names\n",
    "    cols = None\n",
    "    try:\n",
    "        cols = user_pipeline[:-1].get_feature_names_out()  # type: ignore[index]\n",
    "    except Exception:\n",
    "        try:\n",
    "            if last_transformer is not None and hasattr(last_transformer, \"get_feature_names_out\"):\n",
    "                cols = last_transformer.get_feature_names_out()  # type: ignore[assignment]\n",
    "        except Exception:\n",
    "            cols = None\n",
    "\n",
    "    if cols is None:\n",
    "        cols = X.columns\n",
    "    return pd.DataFrame(Z, index=X.index, columns=list(cols))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "7fa207f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_me_for_split(\n",
    "    fitted_pipeline: Pipeline,\n",
    "    X: pd.DataFrame,\n",
    "    split_name: str | None = None,\n",
    "    id_cols: list[str] = (\"timestamp\", \"city\"),\n",
    "    include_params: bool = True,\n",
    "    keep_cols: list[str] = (\"demand_met\", \"tons_co2\"),\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Use a FITTED pipeline to compute marginal emissions (ME) for a single features DataFrame.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    fitted_pipeline : Pipeline\n",
    "        A pipeline that has already been fit on the training data. Its final step must be\n",
    "        GroupwiseRegressor, whose transform adds 'ME' (and 'alpha1','alpha2').\n",
    "    X : pd.DataFrame\n",
    "        Feature table to transform. Must include the columns required by the pipeline’s\n",
    "        feature steps and binner (e.g., weather vars), plus any IDs you want to keep.\n",
    "    split_name : str, optional\n",
    "        If provided, a 'split' column is added with this value ('train'/'validation'/'test'/etc).\n",
    "    id_cols : list[str], default ('timestamp','city')\n",
    "        Identifier columns to carry into the output if present in `X` after transform.\n",
    "    include_params : bool, default True\n",
    "        If True, also include 'alpha1' and 'alpha2' in the output.\n",
    "    keep_cols : list[str], default ('demand_met','tons_co2')\n",
    "        Additional columns to include if present (useful for diagnostics).\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pd.DataFrame\n",
    "        One row per input row with at least: id_cols ∩ columns, 'ME', and optionally\n",
    "        'alpha1','alpha2', the regressor’s group column, keep_cols, and 'split'.\n",
    "    \"\"\"\n",
    "    # Transform through all steps → last step (GroupwiseRegressor) computes ME\n",
    "    out = fitted_pipeline.transform(X)\n",
    "\n",
    "    # Final estimator for group column name\n",
    "    reg = getattr(fitted_pipeline, \"_final_estimator\", None)\n",
    "    gcol = getattr(reg, \"group_col\", None)\n",
    "\n",
    "    # Build column list in a safe, present-only way\n",
    "    cols: list[str] = [c for c in id_cols if c in out.columns]\n",
    "    if \"ME\" not in out.columns:\n",
    "        raise RuntimeError(\"Pipeline transform did not produce 'ME'. Was the final estimator fitted?\")\n",
    "    cols.append(\"ME\")\n",
    "\n",
    "    if include_params:\n",
    "        for c in (\"alpha1\", \"alpha2\"):\n",
    "            if c in out.columns:\n",
    "                cols.append(c)\n",
    "\n",
    "    if gcol and gcol in out.columns:\n",
    "        cols.append(gcol)\n",
    "\n",
    "    for c in keep_cols:\n",
    "        if c in out.columns and c not in cols:\n",
    "            cols.append(c)\n",
    "\n",
    "    result = out[cols].copy()\n",
    "    if split_name is not None:\n",
    "        result[\"split\"] = split_name\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "3d9c6a96",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_on_split(\n",
    "        regression_model: GroupwiseRegressor,\n",
    "        full_df: pd.DataFrame\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    After pipeline.transform → full_df with group IDs & original y_var,\n",
    "    compute per‑group r2/rmse/mae/n_obs using reg.group_models_.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    reg : GroupwiseRegressor\n",
    "        Fitted GroupwiseRegressor instance with group_models_ populated.\n",
    "    full_df : pd.DataFrame\n",
    "        DataFrame containing the original y_var and group_col.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pd.DataFrame\n",
    "        DataFrame with group metrics: r2, rmse, mae, n_obs.\n",
    "    \"\"\"\n",
    "    df = full_df.copy()\n",
    "    gcol = regression_model.group_col\n",
    "    yname = regression_model.y_var\n",
    "\n",
    "    if gcol not in df.columns or yname not in df.columns:\n",
    "        missing = [c for c in (gcol, yname) if c not in df.columns]\n",
    "        raise KeyError(f\"Required columns missing: {missing}\")\n",
    "\n",
    "    # Use the regressor's predict to ensure FE category handling is consistent\n",
    "    y_true = df[yname]\n",
    "    y_pred = regression_model.predict(df, predict_type=\"y\")\n",
    "\n",
    "    rows = []\n",
    "    for grp, idx in df.groupby(gcol).groups.items():\n",
    "        yt = y_true.loc[idx]\n",
    "        yp = y_pred.loc[idx].dropna()\n",
    "        # align just in case\n",
    "        yt = yt.loc[yp.index]\n",
    "        if len(yt) == 0:\n",
    "            continue\n",
    "        rows.append({\n",
    "            \"group\": grp,\n",
    "            \"r2\": r2_score(yt, yp),\n",
    "            \"rmse\": root_mean_squared_error(yt, yp),\n",
    "            \"mae\": mean_absolute_error(yt, yp),\n",
    "            \"mape\": mean_absolute_percentage_error(yt, yp),\n",
    "            \"n_obs\": int(len(yt)),\n",
    "        })\n",
    "\n",
    "    mdf = pd.DataFrame(rows)\n",
    "    if mdf.empty:\n",
    "        # return empty with expected columns\n",
    "        mdf = pd.DataFrame(columns=[\"group\",\"r2\",\"rmse\",\"mae\",\"mape\",\"n_obs\"])\n",
    "    return mdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "715edc52",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_and_export_marginal_emissions(\n",
    "    pipeline: Pipeline,\n",
    "    x_splits: dict,\n",
    "    y_splits: dict,\n",
    "    out_parquet_path: str,\n",
    "    *,\n",
    "    id_cols: list[str] = (\"timestamp\", \"city\"),\n",
    "    include_params: bool = True,\n",
    "    keep_cols: list[str] = (\"demand_met\", \"tons_co2\"),\n",
    "    order_splits: list[str] = (\"train\", \"validation\", \"test\"),\n",
    "    save_mode: str = \"single\",              # \"single\" | \"per_split\"\n",
    "    compression: str | None = \"snappy\",     # passed to pandas.to_parquet\n",
    "    return_df: bool = True,                 # set False on huge runs\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Fit the pipeline on the train split, compute marginal emissions (ME) for each split,\n",
    "    concatenate, and save to a single Parquet file.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    pipeline : Pipeline\n",
    "        Your full pipeline: [FeatureAddition → Binner → GroupwiseRegressor].\n",
    "    x_splits : dict\n",
    "        Feature splits, e.g. {\"train\": X_train, \"validation\": X_val, \"test\": X_test}.\n",
    "    y_splits : dict\n",
    "        Target splits, e.g. {\"train\": y_train, \"validation\": y_val, \"test\": y_test}.\n",
    "        Only the train target is used for fitting; others are not needed for transform.\n",
    "    out_parquet_path : str\n",
    "        File path for the output Parquet dataset.\n",
    "    id_cols : list[str], default ('timestamp','city')\n",
    "        Identifier columns to include if present.\n",
    "    include_params : bool, default True\n",
    "        Include 'alpha1' and 'alpha2' in the export.\n",
    "    keep_cols : list[str], default ('demand_met','tons_co2')\n",
    "        Additional useful columns to include if present.\n",
    "    order_splits : list[str], default ('train','validation','test')\n",
    "        Order in which to compute and stack splits.\n",
    "    save_mode : {\"single\",\"per_split\"}, default \"single\"\n",
    "        Use \"per_split\" on HPC/MPI (let rank 0 write or give each rank a different path).\n",
    "    compression : str or None, default \"snappy\"\n",
    "        Parquet compression codec (requires pyarrow/fastparquet support).\n",
    "    return_df : bool, default True\n",
    "        If False, skip building the concatenated DataFrame in memory.\n",
    "\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pd.DataFrame or None\n",
    "        Concatenated results if return_df=True and save_mode=\"single\"; otherwise None.\n",
    "\n",
    "\n",
    "    Notes\n",
    "    -----\n",
    "    - Binner quantile edges and groupwise OLS coefficients are learned on TRAIN only.\n",
    "    - Validation and test are transformed using those learned edges/coefficients.\n",
    "    - In MPI jobs, prefer save_mode=\"per_split\" or call this only on rank 0.\n",
    "    - Binner edges and group OLS coefs are learned on train only.\n",
    "    \"\"\"\n",
    "    out_parquet_path = Path(out_parquet_path)\n",
    "\n",
    "    # Fit on train\n",
    "    X_tr = x_splits[\"train\"]\n",
    "    y_tr = y_splits[\"train\"]\n",
    "    _ = pipeline.fit_transform(X_tr, y_tr)\n",
    "\n",
    "    if save_mode not in {\"single\", \"per_split\"}:\n",
    "        raise ValueError(\"save_mode must be 'single' or 'per_split'.\")\n",
    "\n",
    "    # Compute ME for each requested split\n",
    "    parts: list[pd.DataFrame] = []\n",
    "    for split in order_splits:\n",
    "        if split not in x_splits:\n",
    "            continue\n",
    "        df_me = compute_me_for_split(\n",
    "            fitted_pipeline=pipeline,\n",
    "            X=x_splits[split],\n",
    "            split_name=split,\n",
    "            id_cols=id_cols,\n",
    "            include_params=include_params,\n",
    "            keep_cols=keep_cols,\n",
    "        )\n",
    "\n",
    "        if save_mode == \"per_split\":\n",
    "            split_path = out_parquet_path.with_name(\n",
    "                f\"{out_parquet_path.stem}__{split}{out_parquet_path.suffix or '.parquet'}\"\n",
    "            )\n",
    "            split_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "            df_me.to_parquet(split_path, index=False, compression=compression)\n",
    "            # optionally avoid keeping in memory on huge runs\n",
    "            if return_df:\n",
    "                parts.append(df_me)\n",
    "        else:\n",
    "            parts.append(df_me)\n",
    "\n",
    "    if save_mode == \"single\":\n",
    "        final = pd.concat(parts, ignore_index=True) if parts else pd.DataFrame()\n",
    "        out_parquet_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "        final.to_parquet(out_parquet_path, index=False, compression=compression)\n",
    "        print(f\"[SAVE] Wrote marginal emissions to {out_parquet_path} (rows={len(final):,})\")\n",
    "        return final if return_df else None\n",
    "    else:\n",
    "        print(f\"[SAVE] Wrote per-split Parquet files next to {out_parquet_path}\")\n",
    "        if return_df:\n",
    "            return pd.concat(parts, ignore_index=True) if parts else pd.DataFrame()\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "id": "5278d1b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_and_export_marginal_emissions_full(\n",
    "    pipeline: Pipeline,\n",
    "    X_full: pd.DataFrame,\n",
    "    y_full: pd.Series | pd.DataFrame,\n",
    "    out_parquet_path: str,\n",
    "    *,\n",
    "    id_cols: list[str] = (\"timestamp\", \"city\"),\n",
    "    include_params: bool = True,\n",
    "    keep_cols: list[str] = (\"demand_met\", \"tons_co2\"),\n",
    "    compression: str | None = \"snappy\",\n",
    "    return_df: bool = True,\n",
    ") -> pd.DataFrame | None:\n",
    "    \"\"\"\n",
    "    Fit the pipeline on ALL data and export marginal emissions for the ENTIRE dataset (no splits).\n",
    "\n",
    "    Parameters:\n",
    "    ----------\n",
    "    pipeline: Pipeline\n",
    "        The pipeline to fit and use for predictions.\n",
    "    X_full: pd.DataFrame\n",
    "        The feature data to fit the pipeline on.\n",
    "    y_full: pd.Series | pd.DataFrame\n",
    "        The target data to fit the pipeline on.\n",
    "    out_parquet_path: str\n",
    "        The path to save the output parquet file.\n",
    "    id_cols: list[str], optional\n",
    "        The columns to use as identifiers (default is [\"timestamp\", \"city\"]).\n",
    "    include_params: bool, optional\n",
    "        Whether to include model parameters in the output (default is True).\n",
    "    keep_cols: list[str], optional\n",
    "        The columns to keep in the output (default is [\"demand_met\", \"tons_co2\"]).\n",
    "    compression: str | None, optional\n",
    "        The compression method to use for the output parquet file (default is \"snappy\").\n",
    "    return_df: bool, optional\n",
    "        Whether to return the resulting DataFrame (default is True).\n",
    "\n",
    "    Returns:\n",
    "    -------\n",
    "    pd.DataFrame | None\n",
    "        The resulting DataFrame with marginal emissions or None if not requested.\n",
    "    \"\"\"\n",
    "    out_parquet_path = Path(out_parquet_path)\n",
    "\n",
    "    # Fit on all data\n",
    "    pipeline.fit(X_full, y_full)\n",
    "\n",
    "    # Compute ME for the whole table\n",
    "    df_me = compute_me_for_split(\n",
    "        fitted_pipeline=pipeline,\n",
    "        X=X_full,\n",
    "        split_name=None,              # no split column\n",
    "        id_cols=id_cols,\n",
    "        include_params=include_params,\n",
    "        keep_cols=keep_cols,\n",
    "    )\n",
    "\n",
    "    # Save\n",
    "    out_parquet_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    df_me.to_parquet(out_parquet_path, index=False, compression=compression)\n",
    "    print(f\"[SAVE] Wrote marginal emissions to {out_parquet_path} (rows={len(df_me):,})\")\n",
    "\n",
    "    return df_me if return_df else None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f70dc699",
   "metadata": {},
   "source": [
    "#### Runners & Orchestrators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "41cecb8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_regressor_model(\n",
    "    user_pipeline: Pipeline,\n",
    "    x_df: pd.DataFrame,\n",
    "    y_df: pd.Series | pd.DataFrame,\n",
    "    split_name: str,\n",
    "    extra_info: dict | None = None,\n",
    "    return_model: bool = False,\n",
    "    random_state: int = 12,\n",
    "    interval_hours: float = 0.5,\n",
    "    *,\n",
    " model_id_hash: str | None = None,\n",
    "    params_json_str: str | None = None,\n",
    ") -> tuple[pd.DataFrame, list[str], GroupwiseRegressor | dict]:\n",
    "    \"\"\"\n",
    "    Run a pipeline on one split, compute per-group metrics, attach energy weights,\n",
    "    and compute diagnostics (pooled CO₂ fit + finite-difference ME checks).\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    user_pipeline : Pipeline\n",
    "        Full pipeline [FeatureAddition → (Binner) → GroupwiseRegressor].\n",
    "    x_df : pd.DataFrame\n",
    "        Features for the split.\n",
    "    y_df : pd.Series or single-column pd.DataFrame\n",
    "        Target for the split.\n",
    "    split_name : {\"train\",\"validation\",\"test\"}\n",
    "        Which split to run.\n",
    "    extra_info : dict, optional\n",
    "        Extra metadata to stamp onto the output rows.\n",
    "    return_model : bool, default False\n",
    "        If True, returns the final estimator as the 3rd tuple item; otherwise returns extras dict.\n",
    "    random_state : int, default 12\n",
    "        Random seed for reproducibility.\n",
    "    interval_hours : float, default 0.5\n",
    "        Duration represented by each row (half-hourly = 0.5).\n",
    "    model_id_hash : str, optional\n",
    "        If provided, stamp this precomputed run-level hash (recommended).\n",
    "        If None, a local signature is computed (useful for ad-hoc calls).\n",
    "    params_json_str : str, optional\n",
    "        Pre-rendered pipeline params JSON to stamp; if None, it is computed.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    metrics_df : pd.DataFrame\n",
    "        Per-group metrics with added 'energy_MWh' and metadata columns.\n",
    "    x_cols_used : list[str]\n",
    "        Regressor feature names used by the GroupwiseRegressor (x_vars + fe_vars).\n",
    "    model_or_extras : GroupwiseRegressor | dict\n",
    "        If return_model=True → the fitted final estimator; else a dict of diagnostics.\n",
    "    \"\"\"\n",
    "    np.random.seed(random_state)\n",
    "\n",
    "    for col in x_df.columns:\n",
    "        dt = x_df[col].dtype\n",
    "        if str(dt).startswith((\"uint\", \"UInt\")):\n",
    "            x_df[col] = x_df[col].astype(\"int64\")\n",
    "\n",
    "    if split_name not in (\"train\", \"validation\", \"test\"):\n",
    "        raise ValueError(f\"split_name must be 'train', 'validation', or 'test' (got {split_name!r})\")\n",
    "\n",
    "    X = x_df.copy()\n",
    "    if isinstance(y_df, pd.DataFrame):\n",
    "        if y_df.shape[1] != 1:\n",
    "            raise ValueError(\"y_df must be a Series or single-column DataFrame.\")\n",
    "        y_ser = y_df.iloc[:, 0]\n",
    "    else:\n",
    "        y_ser = y_df\n",
    "\n",
    "    # Use provided model_id_hash (from orchestrator) or compute a local one\n",
    "    if model_id_hash is None:\n",
    "        model_id_hash, _ = signature_for_run(\n",
    "            user_pipeline,\n",
    "            x_columns=list(X.columns),\n",
    "            y=y_ser,\n",
    "            random_state=random_state,\n",
    "            eval_splits=(split_name,),   # local call; orchestrator passes a shared hash\n",
    "            compute_test=False,\n",
    "            extra_info=extra_info,\n",
    "        )\n",
    "\n",
    "    if params_json_str is None:\n",
    "        params_json_str = json.dumps(\n",
    "            user_pipeline.get_params(deep=True),\n",
    "            sort_keys=True, separators=(\",\", \":\"), default=str\n",
    "        )\n",
    "\n",
    "    extras: dict[str, Any] = {}\n",
    "\n",
    "    if split_name == \"train\":\n",
    "        # Fit → metrics from regressor\n",
    "        _ = user_pipeline.fit_transform(X, y_ser)\n",
    "        model = user_pipeline._final_estimator  # type: ignore[attr-defined]\n",
    "        metrics_df = model.get_metrics(summarise=True).reset_index(drop=True)\n",
    "\n",
    "        # Canonicalize group col to \"group\"\n",
    "        if model.group_col in metrics_df.columns:\n",
    "            metrics_df = metrics_df.rename(columns={model.group_col: \"group\"})\n",
    "        elif \"group\" not in metrics_df.columns:\n",
    "            metrics_df = metrics_df.rename(columns={metrics_df.columns[0]: \"group\"})\n",
    "\n",
    "        # Preprocessed rows for weights & diagnostics\n",
    "        x_tr = _apply_fitted_preprocessing(user_pipeline, X)\n",
    "        x_tr[model.y_var] = np.asarray(y_ser, dtype=float)\n",
    "\n",
    "        # Energy weights\n",
    "        w = _compute_group_energy_weights(\n",
    "            df=x_tr, group_col=model.group_col, q_col=model.x_vars[0], interval_hours=interval_hours\n",
    "        ).rename(columns={model.group_col: \"group\"})\n",
    "        metrics_df = metrics_df.merge(w, on=\"group\", how=\"left\")\n",
    "\n",
    "        # Diagnostics (in-sample)\n",
    "        extras[\"pooled_co2\"] = pooled_co2_metrics(\n",
    "            model, x_tr, y_col=model.y_var, group_col=model.group_col\n",
    "        )\n",
    "        me_df = model.transform(x_tr)\n",
    "        fd_df = finite_difference_me_metrics(\n",
    "            df=me_df,\n",
    "            time_col=\"timestamp\" if \"timestamp\" in me_df.columns else \"time_id\",\n",
    "            q_col=model.x_vars[0],\n",
    "            y_col=model.y_var,\n",
    "            me_col=\"ME\",\n",
    "            group_keys=[k for k in (\"city\",) if k in me_df.columns],\n",
    "        )\n",
    "        extras[\"fd_me_by_city\"] = fd_df.to_dict(orient=\"records\") if not fd_df.empty else []\n",
    "        extras[\"fd_me_pooled\"] = (\n",
    "            fd_df.loc[fd_df[\"city\"] == \"ALL\"].iloc[0].to_dict()\n",
    "            if (not fd_df.empty and \"city\" in fd_df.columns and \"ALL\" in fd_df[\"city\"].values)\n",
    "            else (fd_df.sort_values(\"n_pairs\", ascending=False).iloc[0].to_dict() if not fd_df.empty else {})\n",
    "        )\n",
    "\n",
    "    else:\n",
    "        # Use fitted preprocessing + regressor\n",
    "        model = user_pipeline._final_estimator  # type: ignore[attr-defined]\n",
    "        x_tr = _apply_fitted_preprocessing(user_pipeline, X)\n",
    "\n",
    "        if model.group_col not in x_tr.columns:\n",
    "            raise KeyError(\n",
    "                f\"Group column '{model.group_col}' is missing after transform. \"\n",
    "                \"Ensure your binner outputs it.\"\n",
    "            )\n",
    "\n",
    "        x_tr[model.y_var] = np.asarray(y_ser, dtype=float)\n",
    "\n",
    "        # Per-group metrics\n",
    "        metrics_df = evaluate_on_split(model, x_tr)\n",
    "\n",
    "        # Energy weights\n",
    "        w = _compute_group_energy_weights(\n",
    "            df=x_tr, group_col=model.group_col, q_col=model.x_vars[0], interval_hours=interval_hours\n",
    "        ).rename(columns={model.group_col: \"group\"})\n",
    "        metrics_df = metrics_df.merge(w, on=\"group\", how=\"left\")\n",
    "\n",
    "        # Out-of-sample diagnostics\n",
    "        extras[\"pooled_co2\"] = pooled_co2_metrics(\n",
    "            model, x_tr, y_col=model.y_var, group_col=model.group_col\n",
    "        )\n",
    "        me_df = model.transform(x_tr)\n",
    "        fd_df = finite_difference_me_metrics(\n",
    "            df=me_df,\n",
    "            time_col=\"timestamp\" if \"timestamp\" in me_df.columns else \"time_id\",\n",
    "            q_col=model.x_vars[0],\n",
    "            y_col=model.y_var,\n",
    "            me_col=\"ME\",\n",
    "            group_keys=[k for k in (\"city\",) if k in me_df.columns],\n",
    "        )\n",
    "        extras[\"fd_me_by_city\"] = fd_df.to_dict(orient=\"records\") if not fd_df.empty else []\n",
    "        extras[\"fd_me_pooled\"] = (\n",
    "            fd_df.loc[fd_df[\"city\"] == \"ALL\"].iloc[0].to_dict()\n",
    "            if (not fd_df.empty and \"city\" in fd_df.columns and \"ALL\" in fd_df[\"city\"].values)\n",
    "            else (fd_df.sort_values(\"n_pairs\", ascending=False).iloc[0].to_dict() if not fd_df.empty else {})\n",
    "        )\n",
    "\n",
    "    # Stamp metadata\n",
    "    metrics_df[\"data_split\"] = split_name\n",
    "    metrics_df[\"model_id_hash\"] = model_id_hash\n",
    "    metrics_df[\"random_state\"] = random_state\n",
    "    metrics_df[\"pipeline_params_json\"] = params_json_str\n",
    "    metrics_df[\"log_time\"] = datetime.now().isoformat()\n",
    "\n",
    "    model = user_pipeline._final_estimator  # type: ignore[attr-defined]\n",
    "    metrics_df[\"x_columns_used\"] = \",\".join(model.x_vars + model.fe_vars)\n",
    "    for k, v in (extra_info or {}).items():\n",
    "        metrics_df[k] = v\n",
    "\n",
    "    x_cols_used = model.x_vars + model.fe_vars\n",
    "    print(f\"[LOG] {len(metrics_df)} rows for split={split_name}, model_id={model_id_hash}, random_state={random_state}\")\n",
    "\n",
    "    return (metrics_df, x_cols_used, model) if return_model else (metrics_df, x_cols_used, extras)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "44cd5394",
   "metadata": {},
   "outputs": [],
   "source": [
    "def regressor_orchestrator(\n",
    "        user_pipeline: Pipeline,\n",
    "        x_splits: dict,\n",
    "        y_splits: dict,\n",
    "        log_csv_path: str | None = \"marginal_emissions_log.csv\",   # legacy\n",
    "        extra_info: dict | None = None,\n",
    "        force_run: bool = False,\n",
    "        force_overwrite: bool = False,\n",
    "        random_state: int = 12,\n",
    "        group_col_name: str = \"group\",\n",
    "        interval_hours: float = 0.5,\n",
    "        eval_splits: tuple[str, ...] | None = None,\n",
    "        compute_test: bool = False,\n",
    "        # rotating CSV\n",
    "        results_dir: str | None = None,\n",
    "        file_prefix: str | None = None,\n",
    "        max_log_mb: int = 95,\n",
    "        fsync: bool = True,\n",
    ") -> pd.DataFrame | None:\n",
    "    \"\"\"\n",
    "    Fit/evaluate a pipeline on train/validation/test, summarise metrics, and append to a CSV log.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    user_pipeline : Pipeline\n",
    "        Full pipeline, typically [FeatureAddition → Binner → GroupwiseRegressor].\n",
    "    x_splits : dict\n",
    "        Must include \"train\" and \"validation\". Include \"test\" iff compute_test=True.\n",
    "    y_splits : dict\n",
    "        Target splits with the same keys as x_splits.  Must include \"train\" and \"validation\". Include \"test\" iff compute_test=True.\n",
    "     log_csv_path : str, optional\n",
    "        Legacy path; used only to infer default results_dir/file_prefix if those are None.\n",
    "    extra_info : dict, optional\n",
    "        Extra metadata to stamp onto per-split logs (propagates into `run_regressor_model`).\n",
    "    force_run : bool, default=False\n",
    "        If False and an identical model signature was previously logged, skip this run.\n",
    "    force_overwrite : bool, default=False\n",
    "        If True, allows re-logging the same model_id_hash (previous rows are NOT removed here;\n",
    "        use `save_summary_to_csv(..., force_overwrite=True)` for row replacement).\n",
    "    random_state : int, default=12\n",
    "        Random seed recorded in the model signature and summary.\n",
    "    group_col_name : str, default=\"group\"\n",
    "        Canonical group column name used by `summarise_metrics_logs` for nested metrics.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pd.DataFrame or None\n",
    "        One-row summary DataFrame if the run executes; None if skipped due to prior identical log.\n",
    "\n",
    "    Notes\n",
    "    -----\n",
    "    - The model signature (hash) is computed from pipeline parameters, feature columns, target name(s),\n",
    "      random_state, and any `extra_info`. If unchanged and `force_run=False`, the run is skipped.\n",
    "    - `x_columns` recorded in the summary are taken from the **train** split’s evaluation result.\n",
    "    \"\"\"\n",
    "    # in regressor_orchestrator before signature_for_run(...)\n",
    "    if eval_splits is None:\n",
    "        eval_splits = (\"train\",\"validation\",\"test\") if compute_test else (\"train\",\"validation\")\n",
    "    compute_test = (\"test\" in eval_splits)  # ← keep hash consistent with actual splits\n",
    "\n",
    "    # One signature for the whole run (based on TRAIN)\n",
    "    model_key, sig = signature_for_run(\n",
    "        user_pipeline,\n",
    "        x_columns=list(x_splits[\"train\"].columns),\n",
    "        y=y_splits[\"train\"],\n",
    "        random_state=random_state,\n",
    "        eval_splits=eval_splits,\n",
    "        compute_test=compute_test,\n",
    "        extra_info=extra_info,\n",
    "    )\n",
    "\n",
    "    # Resolve dir + prefix (fallback to legacy path)\n",
    "    if results_dir is None or file_prefix is None:\n",
    "        base = Path(log_csv_path or \"marginal_emissions_log.csv\")\n",
    "        inferred_dir = base.parent if str(base.parent) != \"\" else Path(\".\")\n",
    "        inferred_prefix = base.stem\n",
    "        results_dir = results_dir or str(inferred_dir)\n",
    "        file_prefix = file_prefix or inferred_prefix\n",
    "\n",
    "    # De-dupe via index\n",
    "    if not force_run and not force_overwrite:\n",
    "        if is_model_logged_rotating_csv(model_key, results_dir, file_prefix):\n",
    "            print(f\"[SKIP] Model already logged (hash: {model_key})\")\n",
    "            return None\n",
    "\n",
    "    # Precompute params JSON once (consistent across splits)\n",
    "    params_json_str = json.dumps(\n",
    "        user_pipeline.get_params(deep=True),\n",
    "        sort_keys=True, separators=(\",\", \":\"), default=str\n",
    "    )\n",
    "\n",
    "    logs, pooled_extras, fd_extras = {}, {}, {}\n",
    "    x_cols_used: list[str] | None = None\n",
    "\n",
    "    for split in eval_splits:\n",
    "        metrics_df, x_cols_used, extras = run_regressor_model(\n",
    "            user_pipeline=user_pipeline,\n",
    "            x_df=x_splits[split],\n",
    "            y_df=y_splits[split],\n",
    "            split_name=split,\n",
    "            extra_info=extra_info,\n",
    "            return_model=False,\n",
    "            random_state=random_state,\n",
    "            interval_hours=interval_hours,\n",
    "            model_id_hash=model_key,          # shared ID across splits\n",
    "            params_json_str=params_json_str,  # shared params JSON\n",
    "        )\n",
    "        logs[split] = metrics_df\n",
    "        pooled_extras[split] = extras.get(\"pooled_co2\", {})\n",
    "        fd_extras[split] = extras.get(\"fd_me_pooled\", {})\n",
    "\n",
    "    summary_df = summarise_metrics_logs(\n",
    "        train_logs=logs[\"train\"],\n",
    "        val_logs=logs[\"validation\"],\n",
    "        test_logs=logs.get(\"test\"),\n",
    "        user_pipeline=user_pipeline,\n",
    "        x_columns=x_cols_used or [],\n",
    "        random_state=random_state,\n",
    "        group_col_name=group_col_name,           # <- use the parameter you accept\n",
    "        pooled_metrics_by_split=pooled_extras,\n",
    "        fd_me_metrics_by_split=fd_extras,\n",
    "    )\n",
    "\n",
    "    save_summary_to_rotating_csv(\n",
    "        summary_df,\n",
    "        results_dir=results_dir,\n",
    "        file_prefix=file_prefix,\n",
    "        max_mb=max_log_mb,\n",
    "        force_overwrite=force_overwrite,\n",
    "        fsync=fsync,\n",
    "    )\n",
    "    return summary_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2f96d8d",
   "metadata": {},
   "source": [
    "#### Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "4dcb9c36",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_grid_search(\n",
    "        base_feature_pipeline: Pipeline,\n",
    "        regressor_cls,\n",
    "        regressor_kwargs: dict,\n",
    "        grid_config: list[dict],\n",
    "        x_splits: dict,\n",
    "        y_splits: dict,\n",
    "        log_path: str | None,  # legacy; optional now\n",
    "        global_extra_info: dict | None = None,\n",
    "        force_run: bool = False,\n",
    "        force_overwrite: bool = False,\n",
    "        base_feature_pipeline_name: str = \"BaseFeaturePipeline\",\n",
    "        eval_splits: tuple[str, ...] = (\"train\",\"validation\"),\n",
    "        results_dir: str | None = None,\n",
    "        file_prefix: str | None = None,\n",
    "        max_log_mb: int = 95,\n",
    "        fsync: bool = True,\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Execute a series of [features → binner → regressor] runs and log one summary row per config.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    base_feature_pipeline : Pipeline\n",
    "        Preprocessing steps applied before binning. This object is cloned per run to avoid state leakage.\n",
    "    regressor_cls : type\n",
    "        Estimator class to instantiate for the final step (e.g., GroupwiseRegressor).\n",
    "    regressor_kwargs : dict\n",
    "        Baseline kwargs for the regressor. Per-config overrides from `grid_config` are merged on top.\n",
    "        IMPORTANT: This function will not mutate the caller's dict.\n",
    "    grid_config : list of dict\n",
    "        Each item should contain:\n",
    "            - \"binner_class\": class (e.g., MultiQuantileBinner or MultiMedianBinner)\n",
    "            - \"binner_kwargs\": dict of init args for the binner\n",
    "            - \"label\": str label for printing/logging (optional)\n",
    "            - Optional: \"x_vars\", \"fe_vars\" to override the regressor’s predictors per-config\n",
    "            - Optional: anything else you want echoed into `extra_info`\n",
    "    x_splits, y_splits : dict\n",
    "        Dicts keyed by {\"train\",\"validation\",\"test\"} with DataFrames/Series for each split.\n",
    "    log_path : str\n",
    "        CSV path where each successful config appends one summary row.\n",
    "    global_extra_info : dict, optional\n",
    "        Extra metadata stamped into each run’s logs.\n",
    "    force_run, force_overwrite : bool\n",
    "        Passed through to `regressor_orchestrator`.\n",
    "    base_feature_pipeline_name : str, default \"BaseFeaturePipeline\"\n",
    "        Step name used for the features sub-pipeline.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    None\n",
    "        Prints progress and writes rows to `log_path`. Skips silently (with a message) if a config\n",
    "        is already logged and `force_run=False`.\n",
    "\n",
    "    Notes\n",
    "    -----\n",
    "    - We clone `base_feature_pipeline` per run to avoid cross-config state sharing.\n",
    "    - If a binner provides `group_col_name` and the regressor does not specify `group_col`,\n",
    "      we set the regressor’s `group_col` to match.\n",
    "    - If a config provides `x_vars`/`fe_vars`, they override the baseline `regressor_kwargs`.\n",
    "    \"\"\"\n",
    "    missing_x = [s for s in eval_splits if s not in x_splits]\n",
    "    missing_y = [s for s in eval_splits if s not in y_splits]\n",
    "    if missing_x or missing_y:\n",
    "        raise KeyError(f\"Missing splits: X{missing_x} Y{missing_y}\")\n",
    "\n",
    "    total = len(grid_config)\n",
    "    for i, raw_config in enumerate(grid_config, start=1):\n",
    "        config = dict(raw_config)\n",
    "        binner_class = config[\"binner_class\"]\n",
    "        binner_kwargs = dict(config.get(\"binner_kwargs\", {}))\n",
    "        label = config.get(\"label\", binner_class.__name__)\n",
    "\n",
    "        reg_kwargs = dict(regressor_kwargs)\n",
    "        if \"x_vars\" in config:\n",
    "            reg_kwargs[\"x_vars\"] = list(config[\"x_vars\"])\n",
    "        if \"fe_vars\" in config:\n",
    "            reg_kwargs[\"fe_vars\"] = list(config[\"fe_vars\"])\n",
    "        reg_kwargs[\"random_state\"] = reg_kwargs.get(\"random_state\", 12)\n",
    "\n",
    "        binner_group_col = binner_kwargs.get(\"group_col_name\")\n",
    "        if binner_group_col and \"group_col\" not in reg_kwargs:\n",
    "            reg_kwargs[\"group_col\"] = binner_group_col\n",
    "\n",
    "        try:\n",
    "            features_step = clone(base_feature_pipeline)\n",
    "        except Exception:\n",
    "            features_step = base_feature_pipeline\n",
    "\n",
    "        binner = binner_class(**binner_kwargs)\n",
    "        regressor = regressor_cls(**reg_kwargs)\n",
    "\n",
    "        full_pipeline = Pipeline([\n",
    "            (base_feature_pipeline_name, features_step),\n",
    "            (binner_class.__name__, binner),\n",
    "            (regressor_cls.__name__, regressor),\n",
    "        ])\n",
    "\n",
    "        extra_info = {\n",
    "            \"binner_class\": binner_class.__name__,\n",
    "            \"binner_params\": binner_kwargs,\n",
    "            \"regressor_params\": reg_kwargs,\n",
    "            \"grid_label\": label,\n",
    "            **(global_extra_info or {}),\n",
    "        }\n",
    "\n",
    "        rank_tag = \"\"\n",
    "        try:\n",
    "            _, rank, size = _mpi_context()\n",
    "            rank_tag = f\"[R{rank}/{max(size-1,0)}] \"\n",
    "        except Exception:\n",
    "            pass\n",
    "        print(f\"\\n{rank_tag}[GRID {i}/{total}] {label}\")\n",
    "\n",
    "        try:\n",
    "            summary_df = regressor_orchestrator(\n",
    "                user_pipeline=full_pipeline,\n",
    "                x_splits=x_splits,\n",
    "                y_splits=y_splits,\n",
    "                log_csv_path=log_path,            # legacy OK\n",
    "                extra_info=extra_info,\n",
    "                force_run=force_run,\n",
    "                force_overwrite=force_overwrite,\n",
    "                random_state=reg_kwargs[\"random_state\"],\n",
    "                eval_splits=eval_splits,\n",
    "                # NEW\n",
    "                results_dir=results_dir,\n",
    "                file_prefix=file_prefix,\n",
    "                max_log_mb=max_log_mb,\n",
    "                fsync=fsync,\n",
    "                )\n",
    "            if summary_df is not None:\n",
    "                print(f\"[GRID] Logged: {label}\")\n",
    "            else:\n",
    "                print(f\"[GRID] Skipped (already logged): {label}\")\n",
    "        except Exception as e:\n",
    "            print(f\"[GRID] ERROR in '{label}': {type(e).__name__}: {e}\")\n",
    "            continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "3c452581",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_grid_search_auto(\n",
    "        base_feature_pipeline,\n",
    "        regressor_cls,\n",
    "        regressor_kwargs: dict,\n",
    "        grid_config: list[dict],\n",
    "        x_splits: dict,\n",
    "        y_splits: dict,\n",
    "        *,\n",
    "        # logging/rotation knobs\n",
    "        results_dir: str,\n",
    "        file_prefix: str,\n",
    "        max_log_mb: int = 95,\n",
    "        fsync: bool = False,              # set True on HPC if you want durable writes\n",
    "        # orchestration\n",
    "        base_feature_pipeline_name: str = \"FeatureAdditionPipeline\",\n",
    "        eval_splits: tuple[str, ...] = (\"train\",\"validation\"),\n",
    "        force_run: bool = False,\n",
    "        force_overwrite: bool = False,\n",
    "        distribute: str = \"auto\",         # \"auto\" | \"mpi\" | \"single\"\n",
    "        dist_mode: str = \"stride\",        # \"stride\" | \"chunked\"\n",
    "        seed: int = 12,\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Single-node or MPI-parallel grid search runner.\n",
    "\n",
    "    - Auto-detects MPI and splits `grid_config` across ranks.\n",
    "    - Ensures per-rank deterministic RNG via `seed + rank`.\n",
    "    - Uses rotating CSV logging with per-file & index locks.\n",
    "\n",
    "    Parameters are passed straight to `run_grid_search`, except we slice `grid_config`.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    base_feature_pipeline: Pipeline\n",
    "        The base feature pipeline to use for each config.\n",
    "    regressor_cls: Type[BaseEstimator]\n",
    "        The regression model class to use.\n",
    "    regressor_kwargs: dict\n",
    "        Keyword arguments to pass to the regression model.\n",
    "    grid_config: list[dict]\n",
    "        The grid search configuration to use.\n",
    "    x_splits: dict\n",
    "        The input feature splits.\n",
    "    y_splits: dict\n",
    "        The target variable splits.\n",
    "    results_dir: str\n",
    "        The directory to save results.\n",
    "    file_prefix: str\n",
    "        The prefix for result files.\n",
    "    max_log_mb: int\n",
    "        The maximum log file size in MB.\n",
    "    naming: PartNaming | None\n",
    "        Optional naming scheme for output files.\n",
    "    fsync: bool\n",
    "        Whether to fsync log files (for durability).\n",
    "    base_feature_pipeline_name: str\n",
    "        The name of the base feature pipeline.\n",
    "    eval_splits: tuple[str, ...]\n",
    "        The evaluation splits to use.\n",
    "    force_run: bool\n",
    "        Whether to force re-running of existing configs.\n",
    "    force_overwrite: bool\n",
    "        Whether to force overwriting of existing results.\n",
    "    distribute: str\n",
    "        The distribution strategy to use.\n",
    "    dist_mode: str\n",
    "        The distribution mode to use.\n",
    "    seed: int\n",
    "        The random seed to use.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    None\n",
    "        Logs the results of the grid search.\n",
    "    \"\"\"\n",
    "    comm, rank, size = _mpi_context()\n",
    "    if distribute == \"auto\":\n",
    "        distribute = \"mpi\" if size > 1 else \"single\"\n",
    "\n",
    "    # Partition the configs\n",
    "    local_configs = _distribute_configs(grid_config, rank=rank, size=size, mode=dist_mode) \\\n",
    "                    if distribute == \"mpi\" else grid_config\n",
    "    if not local_configs:\n",
    "        if rank == 0:\n",
    "            print(\"[GRID] No configs assigned (empty grid or partition).\")\n",
    "        return\n",
    "\n",
    "    # Per-rank RNG — override/augment existing random_state\n",
    "    local_reg_kwargs = dict(regressor_kwargs)\n",
    "    local_reg_kwargs[\"random_state\"] = int(local_reg_kwargs.get(\"random_state\", seed))\n",
    "\n",
    "    if rank == 0 and distribute == \"mpi\":\n",
    "        print(f\"[MPI] size={size} → ~{len(grid_config)/max(size,1):.1f} configs per rank\")\n",
    "    else:\n",
    "        if distribute == \"mpi\":\n",
    "            print(f\"[MPI] rank={rank}/{size-1} assigned {len(local_configs)} configs\")\n",
    "\n",
    "    run_grid_search(\n",
    "        base_feature_pipeline=base_feature_pipeline,\n",
    "        regressor_cls=regressor_cls,\n",
    "        regressor_kwargs=local_reg_kwargs,\n",
    "        grid_config=local_configs,\n",
    "        x_splits=x_splits,\n",
    "        y_splits=y_splits,\n",
    "        log_path=None,  # legacy path unused when using rotating logs\n",
    "        global_extra_info={\"runner_rank\": rank, \"runner_size\": size},\n",
    "        force_run=force_run,\n",
    "        force_overwrite=force_overwrite,\n",
    "        base_feature_pipeline_name=base_feature_pipeline_name,\n",
    "        eval_splits=eval_splits,\n",
    "        results_dir=results_dir,\n",
    "        file_prefix=file_prefix,\n",
    "        max_log_mb=max_log_mb,\n",
    "        fsync=fsync,\n",
    "    )\n",
    "\n",
    "    # Optional barrier for neat logs\n",
    "    try:\n",
    "        comm.Barrier()\n",
    "    except Exception:\n",
    "        pass\n",
    "    if rank == 0:\n",
    "        print(\"[GRID] Completed (all ranks).\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4002a4a",
   "metadata": {},
   "source": [
    "#### Parameter Grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "26223e11",
   "metadata": {},
   "outputs": [],
   "source": [
    "def all_nonempty_subsets(columns: list[str]) -> list[list[str]]:\n",
    "    \"\"\"All non-empty subsets preserving input order.\"\"\"\n",
    "    return [list(c) for i in range(1, len(columns) + 1) for c in combinations(columns, i)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "f3142ef6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_fe_vars(all_cols: list[str], x_vars: list[str]) -> list[str]:\n",
    "    \"\"\"Complement of x_vars within all_cols.\"\"\"\n",
    "    xset = set(x_vars)\n",
    "    return [c for c in all_cols if c not in xset]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "f4694214",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_x_fe_combinations_disjoint(\n",
    "    candidate_x_vars: list[str],\n",
    "    candidate_fe_vars: list[str],\n",
    "    x_var_length: int = 2,\n",
    "    max_fe_len: int | None = None,\n",
    "    *,\n",
    "    allow_empty_fe: bool = False,\n",
    ") -> list[dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Generate all disjoint non-empty combinations of x_vars and fe_vars.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    candidate_x_vars : list of str\n",
    "        Columns eligible to be used as predictors (x_vars).\n",
    "    candidate_fe_vars : list of str\n",
    "        Columns eligible to be used as fixed effects (fe_vars).\n",
    "    x_var_length : int\n",
    "        Number of x_vars to include in each combination.\n",
    "    max_fe_len : int | None\n",
    "        Maximum number of fe_vars to include in each combination.\n",
    "    allow_empty_fe : bool\n",
    "        Whether to allow empty fe_vars in the combinations.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    list of dicts\n",
    "        Each dict has keys: {'x_vars': [...], 'fe_vars': [...]}\n",
    "    \"\"\"\n",
    "    if x_var_length < 1:\n",
    "        raise ValueError(\"x_var_length must be >= 1\")\n",
    "    if len(candidate_x_vars) < x_var_length:\n",
    "        raise ValueError(\"Not enough candidate_x_vars for requested x_var_length\")\n",
    "\n",
    "    results: list[dict[str, Any]] = []\n",
    "\n",
    "    x_subsets = [list(c) for c in combinations(candidate_x_vars, x_var_length)]\n",
    "    fe_pool = [list(c) for i in range(0 if allow_empty_fe else 1, len(candidate_fe_vars) + 1)\n",
    "               for c in combinations(candidate_fe_vars, i)]\n",
    "\n",
    "    for x_vars in x_subsets:\n",
    "        for fe_vars in fe_pool:\n",
    "            if max_fe_len is not None and len(fe_vars) > max_fe_len:\n",
    "                continue\n",
    "            if set(x_vars).isdisjoint(fe_vars):\n",
    "                results.append({\"x_vars\": x_vars, \"fe_vars\": list(fe_vars)})\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "6ec79c73",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_quantile_grid_configs(\n",
    "        candidate_binning_vars: list[str],\n",
    "        candidate_bin_counts: list[int],\n",
    "        candidate_x_vars: list[str],\n",
    "        candidate_fe_vars: list[str],\n",
    "        x_var_length: int = 2,\n",
    "        binner_extra_grid: dict | list[dict] | None = None,\n",
    ") -> list[dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Produce configs for MultiQuantileBinner sweeping:\n",
    "      - which vars to bin on\n",
    "      - how many bins\n",
    "      - x/fe combinations (disjoint from binned vars)\n",
    "      - optional extra binner kwargs via dict-of-lists or list-of-dicts\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    candidate_binning_vars : list[str]\n",
    "        Variables to be binned.\n",
    "    candidate_bin_counts : list[int]\n",
    "        Number of bins to create for each variable.\n",
    "    candidate_x_vars : list[str]\n",
    "        Variables to use as predictors (x_vars).\n",
    "    candidate_fe_vars : list[str]\n",
    "        Variables to use as fixed effects (fe_vars).\n",
    "    x_var_length : int\n",
    "        Number of x_vars to include in each combination.\n",
    "    binner_extra_grid : dict | list[dict] | None\n",
    "        Optional extra parameters for the binner.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    list[dict[str, Any]]\n",
    "        A list of configuration dictionaries for the binner.\n",
    "    \"\"\"\n",
    "    if not candidate_binning_vars:\n",
    "        return []\n",
    "    if not candidate_bin_counts:\n",
    "        return []\n",
    "\n",
    "    def _expand(grid):\n",
    "        if grid is None:\n",
    "            return [dict()]\n",
    "        if isinstance(grid, list):\n",
    "            return [dict(d) for d in grid]\n",
    "        if isinstance(grid, dict):\n",
    "            keys = list(grid.keys())\n",
    "            vals = [list(v) if isinstance(v, (list, tuple, set)) else [v] for v in (grid[k] for k in keys)]\n",
    "            return [dict(zip(keys, combo)) for combo in product(*vals)]\n",
    "        raise TypeError(\"binner_extra_grid must be a dict or list of dicts\")\n",
    "\n",
    "    extra_list = _expand(binner_extra_grid)\n",
    "    configs: list[dict[str, Any]] = []\n",
    "\n",
    "    # compute once (perf)\n",
    "    x_fe_grid = build_x_fe_combinations_disjoint(\n",
    "        candidate_x_vars, candidate_fe_vars, x_var_length=x_var_length\n",
    "    )\n",
    "\n",
    "    for bin_vars in all_nonempty_subsets(candidate_binning_vars):\n",
    "        bset = set(bin_vars)\n",
    "        for bin_count in candidate_bin_counts:\n",
    "            if int(bin_count) < 2:\n",
    "                continue\n",
    "            bin_spec = {v: int(bin_count) for v in bin_vars}\n",
    "\n",
    "            for combo in x_fe_grid:\n",
    "                if not set(combo[\"x_vars\"]).isdisjoint(bset):\n",
    "                    continue\n",
    "                for extra in extra_list:\n",
    "                    binner_kwargs = {\"bin_specs\": bin_spec, **extra}\n",
    "\n",
    "                    # label suffix for clarity in logs\n",
    "                    tag_bits = []\n",
    "                    pol = extra.get(\"oob_policy\")\n",
    "                    if pol: tag_bits.append(f\"oob{pol}\")\n",
    "                    rate = extra.get(\"max_oob_rate\")\n",
    "                    if rate is not None: tag_bits.append(f\"rate{float(rate):g}\")\n",
    "                    tag = f\"__{'_'.join(tag_bits)}\" if tag_bits else \"\"\n",
    "\n",
    "                    configs.append({\n",
    "                        \"binner_class\": MultiQuantileBinner,\n",
    "                        \"binner_kwargs\": binner_kwargs,\n",
    "                        \"label\": (\n",
    "                            f\"qbin_{bin_count}_{'-'.join(bin_vars)}\"\n",
    "                            f\"__x_{'-'.join(combo['x_vars'])}\"\n",
    "                            f\"__fe_{'-'.join(combo['fe_vars'])}{tag}\"\n",
    "                        ),\n",
    "                        \"x_vars\": combo[\"x_vars\"],\n",
    "                        \"fe_vars\": combo[\"fe_vars\"],\n",
    "                    })\n",
    "    return configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "e844b32b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_median_binner_configs(\n",
    "    candidate_binning_vars: list[str],\n",
    "    candidate_x_vars: list[str],\n",
    "    candidate_fe_vars: list[str],\n",
    "    x_var_length: int = 2,\n",
    "    max_fe_len: int | None = None,\n",
    "    binner_extra_grid: dict | list[dict] | None = None,\n",
    ") -> list[dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Produce configs for MultiMedianBinner sweeping subsets of variables and x/fe combos.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    candidate_binning_vars : list[str]\n",
    "        Variables to be binned.\n",
    "    candidate_x_vars : list[str]\n",
    "        Variables to use as predictors (x_vars).\n",
    "    candidate_fe_vars : list[str]\n",
    "        Variables to use as fixed effects (fe_vars).\n",
    "    x_var_length : int\n",
    "        Number of x_vars to include in each combination.\n",
    "    max_fe_len : int | None\n",
    "        Maximum number of fixed effects to include in each combination.\n",
    "    binner_extra_grid : dict | list[dict] | None\n",
    "        Optional extra parameters for the binner.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    list[dict[str, Any]]\n",
    "        A list of configuration dictionaries for the binner.\n",
    "    \"\"\"\n",
    "    if not candidate_binning_vars:\n",
    "        return []\n",
    "\n",
    "    def _expand(grid):\n",
    "        if grid is None:\n",
    "            return [dict()]\n",
    "        if isinstance(grid, list):\n",
    "            return [dict(d) for d in grid]\n",
    "        if isinstance(grid, dict):\n",
    "            keys = list(grid.keys())\n",
    "            vals = [ (v if isinstance(v, (list, tuple, set)) else [v]) for v in grid.values() ]\n",
    "            return [dict(zip(keys, combo)) for combo in product(*vals)]\n",
    "        raise TypeError(\"binner_extra_grid must be a dict or list of dicts\")\n",
    "\n",
    "    extra_list = _expand(binner_extra_grid)\n",
    "\n",
    "    configs: list[dict[str, Any]] = []\n",
    "    x_fe_grid = build_x_fe_combinations_disjoint(\n",
    "        candidate_x_vars, candidate_fe_vars, x_var_length=x_var_length, max_fe_len=max_fe_len\n",
    "    )\n",
    "\n",
    "    for bin_vars in all_nonempty_subsets(candidate_binning_vars):\n",
    "        bset = set(bin_vars)\n",
    "        for combo in x_fe_grid:\n",
    "            if not set(combo[\"x_vars\"]).isdisjoint(bset):\n",
    "                continue\n",
    "            for extra in extra_list:\n",
    "                binner_kwargs = {\n",
    "                    \"variables\": bin_vars,\n",
    "                    \"group_col_name\": \"median_group_id\",\n",
    "                    \"retain_flags\": True,\n",
    "                    **extra,\n",
    "                }\n",
    "                tag_bits = []\n",
    "                if \"retain_flags\" in extra:\n",
    "                    tag_bits.append(f\"rf{int(bool(extra['retain_flags']))}\")\n",
    "                for k, v in extra.items():\n",
    "                    if k == \"retain_flags\":\n",
    "                        continue\n",
    "                    tag_bits.append(f\"{k}{v}\")\n",
    "                tag = f\"__{'_'.join(tag_bits)}\" if tag_bits else \"\"\n",
    "\n",
    "                configs.append({\n",
    "                    \"binner_class\": MultiMedianBinner,\n",
    "                    \"binner_kwargs\": binner_kwargs,\n",
    "                    \"label\": (\n",
    "                        f\"median_{'-'.join(bin_vars)}\"\n",
    "                        f\"__x_{'-'.join(combo['x_vars'])}\"\n",
    "                        f\"__fe_{'-'.join(combo['fe_vars'])}{tag}\"\n",
    "                    ),\n",
    "                    \"x_vars\": combo[\"x_vars\"],\n",
    "                    \"fe_vars\": combo[\"fe_vars\"],\n",
    "                })\n",
    "    return configs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c14bcc0",
   "metadata": {},
   "source": [
    "## Loading Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1c30f6b",
   "metadata": {},
   "source": [
    "#### Directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "ba754c05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DIRECTORIES AND PATHS\n",
    "# This is a redundant code block, but it is included as a reminder of the directory variables.\n",
    "base_data_directory = \"data\"  # Base directory where the dataframes will be saved\n",
    "hitachi_data_directory = os.path.join(base_data_directory, \"hitachi_copy\")  # Directory where the dataframes will be saved\n",
    "marginal_emissions_development_directory = os.path.join(base_data_directory, \"marginal_emissions_development\")  # Directory for marginal emissions development data\n",
    "marginal_emissions_results_directory = os.path.join(marginal_emissions_development_directory, \"results\")\n",
    "marginal_emissions_logs_directory = os.path.join(marginal_emissions_development_directory, \"logs\")\n",
    "\n",
    "marginal_emissions_prefix = \"marginal_emissions_results\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "4e6996bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "Contents of 'data/hitachi_copy' and subdirectories:\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "  - .DS_Store\n",
      "  - customers_20250606_1901.parquet\n",
      "  - customers_20250630_1215.parquet\n",
      "  - customers_20250701_1318.parquet\n",
      "  - customers_20250714_1401.parquet\n",
      "  - customers_weather_mapping_20250630_1215.parquet\n",
      "  - grid_readings_20250606_1901.parquet\n",
      "  - grid_readings_20250630_1215.parquet\n",
      "  - grid_readings_20250701_1318.parquet\n",
      "  - grid_readings_20250714_1401.parquet\n",
      "  - grid_readings_20250714_1401_processed.parquet\n",
      "  - grid_readings_20250714_1401_processed_half_hourly.parquet\n",
      "  - meter_readings_20250606_1901.parquet\n",
      "  - meter_readings_20250630_1215.parquet\n",
      "  - meter_readings_20250701_1318.parquet\n",
      "  - weather_20250606_1901.parquet\n",
      "  - weather_20250630_1215.parquet\n",
      "  - weather_20250701_1318.parquet\n",
      "  - weather_20250714_1401.parquet\n",
      "  - weather_20250714_1401_processed.parquet\n",
      "  - weather_and_grid_data_half-hourly_20250714_1401.parquet\n",
      "  - weather_data_combined_20250714_1401.parquet\n",
      "  - meter_primary_files/.DS_Store\n",
      "  - meter_primary_files/meter_readings_2021_20250714_2015.parquet\n",
      "  - meter_primary_files/meter_readings_2021_20250714_2015_formatted.parquet\n",
      "  - meter_primary_files/meter_readings_2021_Q4_20250714_2015_formatted.parquet\n",
      "  - meter_primary_files/meter_readings_2022_20250714_2324.parquet\n",
      "  - meter_primary_files/meter_readings_2022_20250714_2324_formatted.parquet\n",
      "  - meter_primary_files/meter_readings_2022_Q1_20250714_2324_formatted.parquet\n",
      "  - meter_primary_files/meter_readings_2022_Q2_20250714_2324_formatted.parquet\n",
      "  - meter_primary_files/meter_readings_2022_Q3_20250714_2324_formatted.parquet\n",
      "  - meter_primary_files/meter_readings_2022_Q4_20250714_2324_formatted.parquet\n",
      "  - meter_primary_files/meter_readings_2023_20250714_2039.parquet\n",
      "  - meter_primary_files/meter_readings_2023_20250714_2039_formatted.parquet\n",
      "  - meter_primary_files/meter_readings_2023_Q1_20250714_2039_formatted.parquet\n",
      "  - meter_primary_files/meter_readings_all_years_20250714_formatted.parquet\n",
      "  - meter_primary_files/meter_readings_delhi_2021_20250714_2015_formatted.parquet\n",
      "  - meter_primary_files/meter_readings_delhi_2021_Q4_20250714_2015_formatted.parquet\n",
      "  - meter_primary_files/meter_readings_delhi_2022_20250714_2324_formatted.parquet\n",
      "  - meter_primary_files/meter_readings_delhi_2022_Q1_20250714_2324_formatted.parquet\n",
      "  - meter_primary_files/meter_readings_delhi_2022_Q2_20250714_2324_formatted.parquet\n",
      "  - meter_primary_files/meter_readings_delhi_2022_Q3_20250714_2324_formatted.parquet\n",
      "  - meter_primary_files/meter_readings_mumbai_2022_20250714_2324_formatted.parquet\n",
      "  - meter_primary_files/meter_readings_mumbai_2022_Q3_20250714_2324_formatted.parquet\n",
      "  - meter_primary_files/meter_readings_mumbai_2022_Q4_20250714_2324_formatted.parquet\n",
      "  - meter_primary_files/meter_readings_mumbai_2023_20250714_2039_formatted.parquet\n",
      "  - meter_primary_files/meter_readings_mumbai_2023_Q1_20250714_2039_formatted.parquet\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"-\" * 120)\n",
    "print(f\"Contents of '{hitachi_data_directory}' and subdirectories:\\n\" + \"-\" * 120)\n",
    "for root, dirs, files in os.walk(hitachi_data_directory):\n",
    "    for f in sorted(files):\n",
    "        rel_dir = os.path.relpath(root, hitachi_data_directory)\n",
    "        rel_file = os.path.join(rel_dir, f) if rel_dir != \".\" else f\n",
    "        print(f\"  - {rel_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b38bf5a",
   "metadata": {},
   "source": [
    "#### File Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "8269b795",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cleaned weather data\n",
    "base_file = \"weather_and_grid_data_half-hourly_20250714_1401\"\n",
    "\n",
    "train_file = \"marginal_emissions_estimation_20250714_1401_train_data\"\n",
    "validation_file = \"marginal_emissions_estimation_20250714_1401_validation_data\"\n",
    "test_file = \"marginal_emissions_estimation_20250714_1401_test_data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "90515dbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_filepath = os.path.join(hitachi_data_directory, base_file + \".parquet\")\n",
    "\n",
    "train_filepath = os.path.join(marginal_emissions_development_directory, train_file + \".parquet\")\n",
    "validation_filepath = os.path.join(marginal_emissions_development_directory, validation_file + \".parquet\")\n",
    "test_filepath = os.path.join(marginal_emissions_development_directory, test_file + \".parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a9e2c01",
   "metadata": {},
   "source": [
    "#### Load and Look at Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "0da9f9c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_pldf = pl.read_parquet(base_filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "fc6df3d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_pldf = pl.read_parquet(train_filepath)\n",
    "validation_pldf = pl.read_parquet(validation_filepath)\n",
    "test_pldf = pl.read_parquet(test_filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "70726a07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "Sample rows of prepared dataset [train_pldf]:\n",
      "------------------------------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (8, 30)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>timestamp</th><th>city</th><th>land_latitude</th><th>land_longitude</th><th>wind_speed_mps</th><th>wind_direction_meteorological</th><th>temperature_celsius</th><th>precipitation_mm</th><th>surface_net_solar_radiation_kWh_per_m2</th><th>surface_solar_radiation_downwards_kWh_per_m2</th><th>surface_net_solar_radiation_joules_per_m2</th><th>surface_solar_radiation_downwards_joules_per_m2</th><th>total_cloud_cover</th><th>high_cloud_cover</th><th>medium_cloud_cover</th><th>low_cloud_cover</th><th>thermal_generation</th><th>gas_generation</th><th>hydro_generation</th><th>nuclear_generation</th><th>renewable_generation</th><th>total_generation</th><th>demand_met</th><th>non_renewable_generation</th><th>tons_co2</th><th>g_co2_per_kwh</th><th>tons_co2_per_mwh</th><th>wind_dir_cardinal_8</th><th>wind_dir_cardinal_16</th><th>wind_dir_cardinal_4</th></tr><tr><td>datetime[μs, Asia/Kolkata]</td><td>cat</td><td>f64</td><td>f64</td><td>f32</td><td>f32</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>str</td><td>str</td><td>str</td></tr></thead><tbody><tr><td>2021-09-27 20:00:00 IST</td><td>&quot;mumbai&quot;</td><td>19.0</td><td>72.87</td><td>0.924853</td><td>212.124847</td><td>25.391083</td><td>0.073388</td><td>0.208333</td><td>0.0</td><td>0.75</td><td>0.0</td><td>0.982086</td><td>0.977661</td><td>0.377747</td><td>0.186584</td><td>116852.166667</td><td>5783.166667</td><td>30008.75</td><td>5023.333333</td><td>11357.916667</td><td>169025.333333</td><td>167794.083333</td><td>157667.416667</td><td>58231.5881</td><td>689.034097</td><td>0.689034</td><td>&quot;SW&quot;</td><td>&quot;SSW&quot;</td><td>&quot;S&quot;</td></tr><tr><td>2021-09-12 20:30:00 IST</td><td>&quot;delhi&quot;</td><td>28.5</td><td>77.34</td><td>1.63929</td><td>50.575806</td><td>25.773087</td><td>0.211457</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.834717</td><td>0.802658</td><td>0.480942</td><td>0.050446</td><td>103866.0</td><td>3856.416667</td><td>32174.5</td><td>4873.916667</td><td>14401.583333</td><td>159172.416667</td><td>157736.083333</td><td>144770.833333</td><td>51471.20895</td><td>646.733876</td><td>0.646734</td><td>&quot;NE&quot;</td><td>&quot;NE&quot;</td><td>&quot;E&quot;</td></tr><tr><td>2022-10-15 17:30:00 IST</td><td>&quot;delhi&quot;</td><td>28.7</td><td>76.84</td><td>1.425882</td><td>218.357635</td><td>21.027451</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>118955.75</td><td>1514.166667</td><td>26546.166667</td><td>5602.166667</td><td>9030.25</td><td>161648.5</td><td>158935.416667</td><td>152618.25</td><td>58295.9289</td><td>721.248411</td><td>0.721248</td><td>&quot;SW&quot;</td><td>&quot;SW&quot;</td><td>&quot;S&quot;</td></tr><tr><td>2021-05-16 23:30:00 IST</td><td>&quot;mumbai&quot;</td><td>19.2</td><td>72.77</td><td>11.142207</td><td>94.925476</td><td>27.693573</td><td>0.448465</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>1.0</td><td>1.0</td><td>0.879959</td><td>0.598038</td><td>104849.916667</td><td>4703.0</td><td>21369.75</td><td>5115.416667</td><td>13190.5</td><td>149228.583333</td><td>147783.5</td><td>136038.083333</td><td>52141.0544</td><td>698.812579</td><td>0.698813</td><td>&quot;E&quot;</td><td>&quot;E&quot;</td><td>&quot;E&quot;</td></tr><tr><td>2021-01-16 09:00:00 IST</td><td>&quot;mumbai&quot;</td><td>18.6</td><td>72.97</td><td>4.02385</td><td>290.266449</td><td>28.962738</td><td>0.0</td><td>343449.538203</td><td>396765.752842</td><td>1.2364e6</td><td>1.4284e6</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>129002.5</td><td>4321.0</td><td>20597.5</td><td>4572.916667</td><td>10641.416667</td><td>169135.333333</td><td>168428.083333</td><td>158493.916667</td><td>63822.243</td><td>754.698572</td><td>0.754699</td><td>&quot;W&quot;</td><td>&quot;WNW&quot;</td><td>&quot;W&quot;</td></tr><tr><td>2022-11-05 23:30:00 IST</td><td>&quot;delhi&quot;</td><td>28.5</td><td>77.24</td><td>2.291211</td><td>110.456741</td><td>20.155869</td><td>0.0</td><td>5.0000e-19</td><td>0.0</td><td>1.8000e-12</td><td>0.0</td><td>0.018951</td><td>0.018951</td><td>0.0</td><td>0.0</td><td>118432.916667</td><td>1970.583333</td><td>16261.416667</td><td>5249.416667</td><td>4395.333333</td><td>146309.666667</td><td>144045.583333</td><td>141914.333333</td><td>58143.89825</td><td>794.814058</td><td>0.794814</td><td>&quot;E&quot;</td><td>&quot;ESE&quot;</td><td>&quot;E&quot;</td></tr><tr><td>2023-02-07 01:30:00 IST</td><td>&quot;mumbai&quot;</td><td>18.8</td><td>72.87</td><td>3.429985</td><td>30.331482</td><td>21.22171</td><td>0.0</td><td>2.5000e-19</td><td>0.0</td><td>9.0000e-13</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>131601.916667</td><td>2420.416667</td><td>7165.5</td><td>5412.5</td><td>4301.666667</td><td>150902.0</td><td>148853.166667</td><td>146600.333333</td><td>64661.04755</td><td>856.997924</td><td>0.856998</td><td>&quot;NE&quot;</td><td>&quot;NNE&quot;</td><td>&quot;N&quot;</td></tr><tr><td>2023-02-27 23:30:00 IST</td><td>&quot;delhi&quot;</td><td>28.6</td><td>77.14</td><td>1.487409</td><td>2.909271</td><td>15.831146</td><td>0.0</td><td>5.0000e-19</td><td>0.0</td><td>1.8000e-12</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>144964.166667</td><td>2460.25</td><td>11306.083333</td><td>4950.666667</td><td>6624.166667</td><td>170305.333333</td><td>169720.166667</td><td>163681.166667</td><td>71180.09825</td><td>835.914292</td><td>0.835914</td><td>&quot;N&quot;</td><td>&quot;N&quot;</td><td>&quot;N&quot;</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (8, 30)\n",
       "┌────────────┬────────┬────────────┬───────────┬───┬───────────┬───────────┬───────────┬───────────┐\n",
       "│ timestamp  ┆ city   ┆ land_latit ┆ land_long ┆ … ┆ tons_co2_ ┆ wind_dir_ ┆ wind_dir_ ┆ wind_dir_ │\n",
       "│ ---        ┆ ---    ┆ ude        ┆ itude     ┆   ┆ per_mwh   ┆ cardinal_ ┆ cardinal_ ┆ cardinal_ │\n",
       "│ datetime[μ ┆ cat    ┆ ---        ┆ ---       ┆   ┆ ---       ┆ 8         ┆ 16        ┆ 4         │\n",
       "│ s, Asia/Ko ┆        ┆ f64        ┆ f64       ┆   ┆ f64       ┆ ---       ┆ ---       ┆ ---       │\n",
       "│ lkata]     ┆        ┆            ┆           ┆   ┆           ┆ str       ┆ str       ┆ str       │\n",
       "╞════════════╪════════╪════════════╪═══════════╪═══╪═══════════╪═══════════╪═══════════╪═══════════╡\n",
       "│ 2021-09-27 ┆ mumbai ┆ 19.0       ┆ 72.87     ┆ … ┆ 0.689034  ┆ SW        ┆ SSW       ┆ S         │\n",
       "│ 20:00:00   ┆        ┆            ┆           ┆   ┆           ┆           ┆           ┆           │\n",
       "│ IST        ┆        ┆            ┆           ┆   ┆           ┆           ┆           ┆           │\n",
       "│ 2021-09-12 ┆ delhi  ┆ 28.5       ┆ 77.34     ┆ … ┆ 0.646734  ┆ NE        ┆ NE        ┆ E         │\n",
       "│ 20:30:00   ┆        ┆            ┆           ┆   ┆           ┆           ┆           ┆           │\n",
       "│ IST        ┆        ┆            ┆           ┆   ┆           ┆           ┆           ┆           │\n",
       "│ 2022-10-15 ┆ delhi  ┆ 28.7       ┆ 76.84     ┆ … ┆ 0.721248  ┆ SW        ┆ SW        ┆ S         │\n",
       "│ 17:30:00   ┆        ┆            ┆           ┆   ┆           ┆           ┆           ┆           │\n",
       "│ IST        ┆        ┆            ┆           ┆   ┆           ┆           ┆           ┆           │\n",
       "│ 2021-05-16 ┆ mumbai ┆ 19.2       ┆ 72.77     ┆ … ┆ 0.698813  ┆ E         ┆ E         ┆ E         │\n",
       "│ 23:30:00   ┆        ┆            ┆           ┆   ┆           ┆           ┆           ┆           │\n",
       "│ IST        ┆        ┆            ┆           ┆   ┆           ┆           ┆           ┆           │\n",
       "│ 2021-01-16 ┆ mumbai ┆ 18.6       ┆ 72.97     ┆ … ┆ 0.754699  ┆ W         ┆ WNW       ┆ W         │\n",
       "│ 09:00:00   ┆        ┆            ┆           ┆   ┆           ┆           ┆           ┆           │\n",
       "│ IST        ┆        ┆            ┆           ┆   ┆           ┆           ┆           ┆           │\n",
       "│ 2022-11-05 ┆ delhi  ┆ 28.5       ┆ 77.24     ┆ … ┆ 0.794814  ┆ E         ┆ ESE       ┆ E         │\n",
       "│ 23:30:00   ┆        ┆            ┆           ┆   ┆           ┆           ┆           ┆           │\n",
       "│ IST        ┆        ┆            ┆           ┆   ┆           ┆           ┆           ┆           │\n",
       "│ 2023-02-07 ┆ mumbai ┆ 18.8       ┆ 72.87     ┆ … ┆ 0.856998  ┆ NE        ┆ NNE       ┆ N         │\n",
       "│ 01:30:00   ┆        ┆            ┆           ┆   ┆           ┆           ┆           ┆           │\n",
       "│ IST        ┆        ┆            ┆           ┆   ┆           ┆           ┆           ┆           │\n",
       "│ 2023-02-27 ┆ delhi  ┆ 28.6       ┆ 77.14     ┆ … ┆ 0.835914  ┆ N         ┆ N         ┆ N         │\n",
       "│ 23:30:00   ┆        ┆            ┆           ┆   ┆           ┆           ┆           ┆           │\n",
       "│ IST        ┆        ┆            ┆           ┆   ┆           ┆           ┆           ┆           │\n",
       "└────────────┴────────┴────────────┴───────────┴───┴───────────┴───────────┴───────────┴───────────┘"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Schema([('timestamp', Datetime(time_unit='us', time_zone='Asia/Kolkata')),\n",
       "        ('city', Categorical(ordering='physical')),\n",
       "        ('land_latitude', Float64),\n",
       "        ('land_longitude', Float64),\n",
       "        ('wind_speed_mps', Float32),\n",
       "        ('wind_direction_meteorological', Float32),\n",
       "        ('temperature_celsius', Float64),\n",
       "        ('precipitation_mm', Float64),\n",
       "        ('surface_net_solar_radiation_kWh_per_m2', Float64),\n",
       "        ('surface_solar_radiation_downwards_kWh_per_m2', Float64),\n",
       "        ('surface_net_solar_radiation_joules_per_m2', Float64),\n",
       "        ('surface_solar_radiation_downwards_joules_per_m2', Float64),\n",
       "        ('total_cloud_cover', Float32),\n",
       "        ('high_cloud_cover', Float32),\n",
       "        ('medium_cloud_cover', Float32),\n",
       "        ('low_cloud_cover', Float32),\n",
       "        ('thermal_generation', Float64),\n",
       "        ('gas_generation', Float64),\n",
       "        ('hydro_generation', Float64),\n",
       "        ('nuclear_generation', Float64),\n",
       "        ('renewable_generation', Float64),\n",
       "        ('total_generation', Float64),\n",
       "        ('demand_met', Float64),\n",
       "        ('non_renewable_generation', Float64),\n",
       "        ('tons_co2', Float64),\n",
       "        ('g_co2_per_kwh', Float64),\n",
       "        ('tons_co2_per_mwh', Float64),\n",
       "        ('wind_dir_cardinal_8', String),\n",
       "        ('wind_dir_cardinal_16', String),\n",
       "        ('wind_dir_cardinal_4', String)])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Sample Rows of the DataFrame\n",
    "print(\"\\n\" + \"-\" * 120)\n",
    "print(f\"Sample rows of prepared dataset [train_pldf]:\\n\" + \"-\" * 120)\n",
    "display(train_pldf.sample(8))\n",
    "display(train_pldf.schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "4be4bbb6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (9, 31)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>statistic</th><th>timestamp</th><th>city</th><th>land_latitude</th><th>land_longitude</th><th>wind_speed_mps</th><th>wind_direction_meteorological</th><th>temperature_celsius</th><th>precipitation_mm</th><th>surface_net_solar_radiation_kWh_per_m2</th><th>surface_solar_radiation_downwards_kWh_per_m2</th><th>surface_net_solar_radiation_joules_per_m2</th><th>surface_solar_radiation_downwards_joules_per_m2</th><th>total_cloud_cover</th><th>high_cloud_cover</th><th>medium_cloud_cover</th><th>low_cloud_cover</th><th>thermal_generation</th><th>gas_generation</th><th>hydro_generation</th><th>nuclear_generation</th><th>renewable_generation</th><th>total_generation</th><th>demand_met</th><th>non_renewable_generation</th><th>tons_co2</th><th>g_co2_per_kwh</th><th>tons_co2_per_mwh</th><th>wind_dir_cardinal_8</th><th>wind_dir_cardinal_16</th><th>wind_dir_cardinal_4</th></tr><tr><td>str</td><td>str</td><td>str</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>str</td><td>str</td><td>str</td></tr></thead><tbody><tr><td>&quot;count&quot;</td><td>&quot;2365200&quot;</td><td>&quot;2365200&quot;</td><td>2.3652e6</td><td>2.3652e6</td><td>2.3652e6</td><td>2.3652e6</td><td>2.3652e6</td><td>2.3652e6</td><td>2.3652e6</td><td>2.3652e6</td><td>2.3652e6</td><td>2.3652e6</td><td>2.3652e6</td><td>2.3652e6</td><td>2.3652e6</td><td>2.3652e6</td><td>2.3652e6</td><td>2.3652e6</td><td>2.3652e6</td><td>2.3652e6</td><td>2.3652e6</td><td>2.3652e6</td><td>2.3652e6</td><td>2.3652e6</td><td>2.3652e6</td><td>2.3652e6</td><td>2.3652e6</td><td>&quot;2365200&quot;</td><td>&quot;2365200&quot;</td><td>&quot;2365200&quot;</td></tr><tr><td>&quot;null_count&quot;</td><td>&quot;0&quot;</td><td>&quot;0&quot;</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>&quot;0&quot;</td><td>&quot;0&quot;</td><td>&quot;0&quot;</td></tr><tr><td>&quot;mean&quot;</td><td>&quot;2022-07-02 11:45:00+05:30&quot;</td><td>null</td><td>25.393333</td><td>75.701111</td><td>2.6235</td><td>209.367767</td><td>25.327742</td><td>0.074796</td><td>77502.482101</td><td>93200.185481</td><td>322438.613659</td><td>386230.698</td><td>0.368437</td><td>0.268966</td><td>0.15923</td><td>0.108627</td><td>127381.266888</td><td>3495.228177</td><td>18310.196241</td><td>4896.593691</td><td>17231.281789</td><td>171305.662978</td><td>169819.805685</td><td>154083.284997</td><td>62846.579568</td><td>737.505107</td><td>0.737505</td><td>null</td><td>null</td><td>null</td></tr><tr><td>&quot;std&quot;</td><td>null</td><td>null</td><td>4.538596</td><td>1.969453</td><td>1.384926</td><td>108.197655</td><td>6.782098</td><td>0.301263</td><td>117310.82224</td><td>140789.172376</td><td>438591.739796</td><td>523835.164521</td><td>0.391733</td><td>0.383111</td><td>0.246949</td><td>0.21914</td><td>15967.372027</td><td>1293.828068</td><td>8769.339123</td><td>629.105609</td><td>12790.794035</td><td>21407.213375</td><td>21168.361496</td><td>18020.958844</td><td>7829.084075</td><td>73.445805</td><td>0.073446</td><td>null</td><td>null</td><td>null</td></tr><tr><td>&quot;min&quot;</td><td>&quot;2021-01-01 00:00:00+05:30&quot;</td><td>null</td><td>18.5</td><td>72.77</td><td>0.005185</td><td>0.000122</td><td>2.149261</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>75325.666667</td><td>109.483135</td><td>3093.333333</td><td>2281.083333</td><td>0.0</td><td>106969.666667</td><td>105140.916667</td><td>95292.666667</td><td>37529.27105</td><td>501.631083</td><td>0.501631</td><td>&quot;E&quot;</td><td>&quot;E&quot;</td><td>&quot;E&quot;</td></tr><tr><td>&quot;25%&quot;</td><td>&quot;2021-10-01 18:00:00+05:30&quot;</td><td>null</td><td>19.2</td><td>72.97</td><td>1.686868</td><td>107.563995</td><td>21.790451</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>115672.583333</td><td>2500.333333</td><td>10473.166667</td><td>4391.166667</td><td>6528.083333</td><td>157407.0</td><td>156063.0</td><td>141618.833333</td><td>57124.82145</td><td>688.786052</td><td>0.688786</td><td>null</td><td>null</td><td>null</td></tr><tr><td>&quot;50%&quot;</td><td>&quot;2022-07-02 12:00:00+05:30&quot;</td><td>null</td><td>28.5</td><td>76.94</td><td>2.39874</td><td>244.841629</td><td>26.34166</td><td>0.0</td><td>0.221963</td><td>0.277778</td><td>12837.411089</td><td>15372.208203</td><td>0.197662</td><td>0.001999</td><td>0.029449</td><td>0.0</td><td>128998.583333</td><td>3123.583333</td><td>17404.333342</td><td>4981.0</td><td>13140.416667</td><td>172473.75</td><td>171032.5</td><td>154705.25</td><td>63598.64055</td><td>736.896614</td><td>0.736897</td><td>null</td><td>null</td><td>null</td></tr><tr><td>&quot;75%&quot;</td><td>&quot;2023-04-02 05:30:00+05:30&quot;</td><td>null</td><td>28.7</td><td>77.14</td><td>3.289447</td><td>303.114471</td><td>29.49292</td><td>0.00441</td><td>142379.684127</td><td>171949.472127</td><td>640638.960422</td><td>770004.528013</td><td>0.786835</td><td>0.569672</td><td>0.228638</td><td>0.100479</td><td>139876.5</td><td>4266.083333</td><td>25239.166667</td><td>5389.5</td><td>25975.833333</td><td>186182.916667</td><td>184641.25</td><td>166615.5</td><td>68917.53425</td><td>797.797138</td><td>0.797797</td><td>null</td><td>null</td><td>null</td></tr><tr><td>&quot;max&quot;</td><td>&quot;2023-12-31 23:30:00+05:30&quot;</td><td>null</td><td>28.8</td><td>77.34</td><td>19.006941</td><td>359.998749</td><td>45.706146</td><td>11.963309</td><td>465916.895282</td><td>530511.99554</td><td>1.6773e6</td><td>1.9098e6</td><td>1.0</td><td>1.0</td><td>1.0</td><td>1.0</td><td>171241.430556</td><td>10157.0</td><td>43162.083333</td><td>6387.083333</td><td>63835.75</td><td>243275.75</td><td>241132.916667</td><td>210043.916667</td><td>84781.290592</td><td>905.09159</td><td>0.905092</td><td>&quot;W&quot;</td><td>&quot;WSW&quot;</td><td>&quot;W&quot;</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (9, 31)\n",
       "┌────────────┬───────────┬─────────┬───────────┬───┬───────────┬───────────┬───────────┬───────────┐\n",
       "│ statistic  ┆ timestamp ┆ city    ┆ land_lati ┆ … ┆ tons_co2_ ┆ wind_dir_ ┆ wind_dir_ ┆ wind_dir_ │\n",
       "│ ---        ┆ ---       ┆ ---     ┆ tude      ┆   ┆ per_mwh   ┆ cardinal_ ┆ cardinal_ ┆ cardinal_ │\n",
       "│ str        ┆ str       ┆ str     ┆ ---       ┆   ┆ ---       ┆ 8         ┆ 16        ┆ 4         │\n",
       "│            ┆           ┆         ┆ f64       ┆   ┆ f64       ┆ ---       ┆ ---       ┆ ---       │\n",
       "│            ┆           ┆         ┆           ┆   ┆           ┆ str       ┆ str       ┆ str       │\n",
       "╞════════════╪═══════════╪═════════╪═══════════╪═══╪═══════════╪═══════════╪═══════════╪═══════════╡\n",
       "│ count      ┆ 2365200   ┆ 2365200 ┆ 2.3652e6  ┆ … ┆ 2.3652e6  ┆ 2365200   ┆ 2365200   ┆ 2365200   │\n",
       "│ null_count ┆ 0         ┆ 0       ┆ 0.0       ┆ … ┆ 0.0       ┆ 0         ┆ 0         ┆ 0         │\n",
       "│ mean       ┆ 2022-07-0 ┆ null    ┆ 25.393333 ┆ … ┆ 0.737505  ┆ null      ┆ null      ┆ null      │\n",
       "│            ┆ 2 11:45:0 ┆         ┆           ┆   ┆           ┆           ┆           ┆           │\n",
       "│            ┆ 0+05:30   ┆         ┆           ┆   ┆           ┆           ┆           ┆           │\n",
       "│ std        ┆ null      ┆ null    ┆ 4.538596  ┆ … ┆ 0.073446  ┆ null      ┆ null      ┆ null      │\n",
       "│ min        ┆ 2021-01-0 ┆ null    ┆ 18.5      ┆ … ┆ 0.501631  ┆ E         ┆ E         ┆ E         │\n",
       "│            ┆ 1 00:00:0 ┆         ┆           ┆   ┆           ┆           ┆           ┆           │\n",
       "│            ┆ 0+05:30   ┆         ┆           ┆   ┆           ┆           ┆           ┆           │\n",
       "│ 25%        ┆ 2021-10-0 ┆ null    ┆ 19.2      ┆ … ┆ 0.688786  ┆ null      ┆ null      ┆ null      │\n",
       "│            ┆ 1 18:00:0 ┆         ┆           ┆   ┆           ┆           ┆           ┆           │\n",
       "│            ┆ 0+05:30   ┆         ┆           ┆   ┆           ┆           ┆           ┆           │\n",
       "│ 50%        ┆ 2022-07-0 ┆ null    ┆ 28.5      ┆ … ┆ 0.736897  ┆ null      ┆ null      ┆ null      │\n",
       "│            ┆ 2 12:00:0 ┆         ┆           ┆   ┆           ┆           ┆           ┆           │\n",
       "│            ┆ 0+05:30   ┆         ┆           ┆   ┆           ┆           ┆           ┆           │\n",
       "│ 75%        ┆ 2023-04-0 ┆ null    ┆ 28.7      ┆ … ┆ 0.797797  ┆ null      ┆ null      ┆ null      │\n",
       "│            ┆ 2 05:30:0 ┆         ┆           ┆   ┆           ┆           ┆           ┆           │\n",
       "│            ┆ 0+05:30   ┆         ┆           ┆   ┆           ┆           ┆           ┆           │\n",
       "│ max        ┆ 2023-12-3 ┆ null    ┆ 28.8      ┆ … ┆ 0.905092  ┆ W         ┆ WSW       ┆ W         │\n",
       "│            ┆ 1 23:30:0 ┆         ┆           ┆   ┆           ┆           ┆           ┆           │\n",
       "│            ┆ 0+05:30   ┆         ┆           ┆   ┆           ┆           ┆           ┆           │\n",
       "└────────────┴───────────┴─────────┴───────────┴───┴───────────┴───────────┴───────────┴───────────┘"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(train_pldf.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "610619e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (1_317_645, 30)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>timestamp</th><th>city</th><th>land_latitude</th><th>land_longitude</th><th>wind_speed_mps</th><th>wind_direction_meteorological</th><th>temperature_celsius</th><th>precipitation_mm</th><th>surface_net_solar_radiation_kWh_per_m2</th><th>surface_solar_radiation_downwards_kWh_per_m2</th><th>surface_net_solar_radiation_joules_per_m2</th><th>surface_solar_radiation_downwards_joules_per_m2</th><th>total_cloud_cover</th><th>high_cloud_cover</th><th>medium_cloud_cover</th><th>low_cloud_cover</th><th>thermal_generation</th><th>gas_generation</th><th>hydro_generation</th><th>nuclear_generation</th><th>renewable_generation</th><th>total_generation</th><th>demand_met</th><th>non_renewable_generation</th><th>tons_co2</th><th>g_co2_per_kwh</th><th>tons_co2_per_mwh</th><th>wind_dir_cardinal_8</th><th>wind_dir_cardinal_16</th><th>wind_dir_cardinal_4</th></tr><tr><td>datetime[μs, Asia/Kolkata]</td><td>cat</td><td>f64</td><td>f64</td><td>f32</td><td>f32</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>str</td><td>str</td><td>str</td></tr></thead><tbody><tr><td>2022-04-30 23:30:00 IST</td><td>&quot;mumbai&quot;</td><td>18.5</td><td>72.97</td><td>0.182899</td><td>94.013245</td><td>26.655075</td><td>0.000477</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.717941</td><td>0.0</td><td>0.0</td><td>0.717941</td><td>148886.591488</td><td>4306.891188</td><td>22342.690016</td><td>4204.309016</td><td>8333.267252</td><td>188073.74896</td><td>186894.414078</td><td>179740.481707</td><td>73506.59789</td><td>781.68404</td><td>0.781684</td><td>&quot;E&quot;</td><td>&quot;E&quot;</td><td>&quot;E&quot;</td></tr><tr><td>2022-04-30 23:30:00 IST</td><td>&quot;mumbai&quot;</td><td>19.3</td><td>72.97</td><td>1.368702</td><td>148.993393</td><td>26.589142</td><td>0.000477</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.640396</td><td>0.0</td><td>0.0</td><td>0.640396</td><td>148886.591488</td><td>4306.891188</td><td>22342.690016</td><td>4204.309016</td><td>8333.267252</td><td>188073.74896</td><td>186894.414078</td><td>179740.481707</td><td>73506.59789</td><td>781.68404</td><td>0.781684</td><td>&quot;SE&quot;</td><td>&quot;SSE&quot;</td><td>&quot;S&quot;</td></tr><tr><td>2022-04-30 23:30:00 IST</td><td>&quot;mumbai&quot;</td><td>19.1</td><td>72.87</td><td>1.339394</td><td>187.678055</td><td>26.867416</td><td>0.000268</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.706665</td><td>0.0</td><td>0.0</td><td>0.706665</td><td>148886.591488</td><td>4306.891188</td><td>22342.690016</td><td>4204.309016</td><td>8333.267252</td><td>188073.74896</td><td>186894.414078</td><td>179740.481707</td><td>73506.59789</td><td>781.68404</td><td>0.781684</td><td>&quot;S&quot;</td><td>&quot;S&quot;</td><td>&quot;S&quot;</td></tr><tr><td>2022-04-30 23:30:00 IST</td><td>&quot;mumbai&quot;</td><td>18.8</td><td>72.97</td><td>1.035694</td><td>175.194733</td><td>26.229767</td><td>0.000238</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.689423</td><td>0.0</td><td>0.0</td><td>0.689423</td><td>148886.591488</td><td>4306.891188</td><td>22342.690016</td><td>4204.309016</td><td>8333.267252</td><td>188073.74896</td><td>186894.414078</td><td>179740.481707</td><td>73506.59789</td><td>781.68404</td><td>0.781684</td><td>&quot;S&quot;</td><td>&quot;S&quot;</td><td>&quot;S&quot;</td></tr><tr><td>2022-04-30 23:30:00 IST</td><td>&quot;mumbai&quot;</td><td>19.0</td><td>72.87</td><td>1.457114</td><td>201.003815</td><td>27.021255</td><td>0.000238</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.706665</td><td>0.0</td><td>0.0</td><td>0.706665</td><td>148886.591488</td><td>4306.891188</td><td>22342.690016</td><td>4204.309016</td><td>8333.267252</td><td>188073.74896</td><td>186894.414078</td><td>179740.481707</td><td>73506.59789</td><td>781.68404</td><td>0.781684</td><td>&quot;S&quot;</td><td>&quot;SSW&quot;</td><td>&quot;S&quot;</td></tr><tr><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td></tr><tr><td>2023-12-31 23:30:00 IST</td><td>&quot;delhi&quot;</td><td>28.8</td><td>76.94</td><td>1.737193</td><td>339.041443</td><td>7.635208</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.514725</td><td>0.0</td><td>0.0</td><td>0.514725</td><td>126499.75</td><td>1906.583333</td><td>4063.583333</td><td>5740.083333</td><td>10116.25</td><td>148326.25</td><td>147825.583333</td><td>138210.0</td><td>62059.65945</td><td>836.804765</td><td>0.836805</td><td>&quot;N&quot;</td><td>&quot;NNW&quot;</td><td>&quot;N&quot;</td></tr><tr><td>2023-12-31 23:30:00 IST</td><td>&quot;delhi&quot;</td><td>28.8</td><td>77.04</td><td>1.823473</td><td>337.503754</td><td>7.538071</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.514725</td><td>0.0</td><td>0.0</td><td>0.514725</td><td>126499.75</td><td>1906.583333</td><td>4063.583333</td><td>5740.083333</td><td>10116.25</td><td>148326.25</td><td>147825.583333</td><td>138210.0</td><td>62059.65945</td><td>836.804765</td><td>0.836805</td><td>&quot;N&quot;</td><td>&quot;NNW&quot;</td><td>&quot;N&quot;</td></tr><tr><td>2023-12-31 23:30:00 IST</td><td>&quot;delhi&quot;</td><td>28.8</td><td>77.14</td><td>1.885798</td><td>335.592499</td><td>7.494827</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.514725</td><td>0.0</td><td>0.0</td><td>0.514725</td><td>126499.75</td><td>1906.583333</td><td>4063.583333</td><td>5740.083333</td><td>10116.25</td><td>148326.25</td><td>147825.583333</td><td>138210.0</td><td>62059.65945</td><td>836.804765</td><td>0.836805</td><td>&quot;NW&quot;</td><td>&quot;NNW&quot;</td><td>&quot;N&quot;</td></tr><tr><td>2023-12-31 23:30:00 IST</td><td>&quot;delhi&quot;</td><td>28.8</td><td>77.24</td><td>1.935602</td><td>333.553284</td><td>7.445801</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.993988</td><td>0.0</td><td>0.0</td><td>0.993988</td><td>126499.75</td><td>1906.583333</td><td>4063.583333</td><td>5740.083333</td><td>10116.25</td><td>148326.25</td><td>147825.583333</td><td>138210.0</td><td>62059.65945</td><td>836.804765</td><td>0.836805</td><td>&quot;NW&quot;</td><td>&quot;NNW&quot;</td><td>&quot;N&quot;</td></tr><tr><td>2023-12-31 23:30:00 IST</td><td>&quot;delhi&quot;</td><td>28.8</td><td>77.34</td><td>1.941074</td><td>331.378357</td><td>7.361526</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.993988</td><td>0.0</td><td>0.0</td><td>0.993988</td><td>126499.75</td><td>1906.583333</td><td>4063.583333</td><td>5740.083333</td><td>10116.25</td><td>148326.25</td><td>147825.583333</td><td>138210.0</td><td>62059.65945</td><td>836.804765</td><td>0.836805</td><td>&quot;NW&quot;</td><td>&quot;NNW&quot;</td><td>&quot;N&quot;</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (1_317_645, 30)\n",
       "┌────────────┬────────┬────────────┬───────────┬───┬───────────┬───────────┬───────────┬───────────┐\n",
       "│ timestamp  ┆ city   ┆ land_latit ┆ land_long ┆ … ┆ tons_co2_ ┆ wind_dir_ ┆ wind_dir_ ┆ wind_dir_ │\n",
       "│ ---        ┆ ---    ┆ ude        ┆ itude     ┆   ┆ per_mwh   ┆ cardinal_ ┆ cardinal_ ┆ cardinal_ │\n",
       "│ datetime[μ ┆ cat    ┆ ---        ┆ ---       ┆   ┆ ---       ┆ 8         ┆ 16        ┆ 4         │\n",
       "│ s, Asia/Ko ┆        ┆ f64        ┆ f64       ┆   ┆ f64       ┆ ---       ┆ ---       ┆ ---       │\n",
       "│ lkata]     ┆        ┆            ┆           ┆   ┆           ┆ str       ┆ str       ┆ str       │\n",
       "╞════════════╪════════╪════════════╪═══════════╪═══╪═══════════╪═══════════╪═══════════╪═══════════╡\n",
       "│ 2022-04-30 ┆ mumbai ┆ 18.5       ┆ 72.97     ┆ … ┆ 0.781684  ┆ E         ┆ E         ┆ E         │\n",
       "│ 23:30:00   ┆        ┆            ┆           ┆   ┆           ┆           ┆           ┆           │\n",
       "│ IST        ┆        ┆            ┆           ┆   ┆           ┆           ┆           ┆           │\n",
       "│ 2022-04-30 ┆ mumbai ┆ 19.3       ┆ 72.97     ┆ … ┆ 0.781684  ┆ SE        ┆ SSE       ┆ S         │\n",
       "│ 23:30:00   ┆        ┆            ┆           ┆   ┆           ┆           ┆           ┆           │\n",
       "│ IST        ┆        ┆            ┆           ┆   ┆           ┆           ┆           ┆           │\n",
       "│ 2022-04-30 ┆ mumbai ┆ 19.1       ┆ 72.87     ┆ … ┆ 0.781684  ┆ S         ┆ S         ┆ S         │\n",
       "│ 23:30:00   ┆        ┆            ┆           ┆   ┆           ┆           ┆           ┆           │\n",
       "│ IST        ┆        ┆            ┆           ┆   ┆           ┆           ┆           ┆           │\n",
       "│ 2022-04-30 ┆ mumbai ┆ 18.8       ┆ 72.97     ┆ … ┆ 0.781684  ┆ S         ┆ S         ┆ S         │\n",
       "│ 23:30:00   ┆        ┆            ┆           ┆   ┆           ┆           ┆           ┆           │\n",
       "│ IST        ┆        ┆            ┆           ┆   ┆           ┆           ┆           ┆           │\n",
       "│ 2022-04-30 ┆ mumbai ┆ 19.0       ┆ 72.87     ┆ … ┆ 0.781684  ┆ S         ┆ SSW       ┆ S         │\n",
       "│ 23:30:00   ┆        ┆            ┆           ┆   ┆           ┆           ┆           ┆           │\n",
       "│ IST        ┆        ┆            ┆           ┆   ┆           ┆           ┆           ┆           │\n",
       "│ …          ┆ …      ┆ …          ┆ …         ┆ … ┆ …         ┆ …         ┆ …         ┆ …         │\n",
       "│ 2023-12-31 ┆ delhi  ┆ 28.8       ┆ 76.94     ┆ … ┆ 0.836805  ┆ N         ┆ NNW       ┆ N         │\n",
       "│ 23:30:00   ┆        ┆            ┆           ┆   ┆           ┆           ┆           ┆           │\n",
       "│ IST        ┆        ┆            ┆           ┆   ┆           ┆           ┆           ┆           │\n",
       "│ 2023-12-31 ┆ delhi  ┆ 28.8       ┆ 77.04     ┆ … ┆ 0.836805  ┆ N         ┆ NNW       ┆ N         │\n",
       "│ 23:30:00   ┆        ┆            ┆           ┆   ┆           ┆           ┆           ┆           │\n",
       "│ IST        ┆        ┆            ┆           ┆   ┆           ┆           ┆           ┆           │\n",
       "│ 2023-12-31 ┆ delhi  ┆ 28.8       ┆ 77.14     ┆ … ┆ 0.836805  ┆ NW        ┆ NNW       ┆ N         │\n",
       "│ 23:30:00   ┆        ┆            ┆           ┆   ┆           ┆           ┆           ┆           │\n",
       "│ IST        ┆        ┆            ┆           ┆   ┆           ┆           ┆           ┆           │\n",
       "│ 2023-12-31 ┆ delhi  ┆ 28.8       ┆ 77.24     ┆ … ┆ 0.836805  ┆ NW        ┆ NNW       ┆ N         │\n",
       "│ 23:30:00   ┆        ┆            ┆           ┆   ┆           ┆           ┆           ┆           │\n",
       "│ IST        ┆        ┆            ┆           ┆   ┆           ┆           ┆           ┆           │\n",
       "│ 2023-12-31 ┆ delhi  ┆ 28.8       ┆ 77.34     ┆ … ┆ 0.836805  ┆ NW        ┆ NNW       ┆ N         │\n",
       "│ 23:30:00   ┆        ┆            ┆           ┆   ┆           ┆           ┆           ┆           │\n",
       "│ IST        ┆        ┆            ┆           ┆   ┆           ┆           ┆           ┆           │\n",
       "└────────────┴────────┴────────────┴───────────┴───┴───────────┴───────────┴───────────┴───────────┘"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_pldf.filter(\n",
    "    pl.col(\"timestamp\") > datetime(2022, 4, 30, 23, tzinfo=ZoneInfo(\"Asia/Kolkata\"))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "5b217bb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Time Boundaries:\n",
      "\tStart Time: 2021-01-01 00:00:00+05:30\n",
      "\tEnd Time: 2023-12-31 23:30:00+05:30\n",
      "\tTotal Duration: 1094 days, 23:30:00\n",
      "\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# reminder of time boundaries\n",
    "print(\"Train Time Boundaries:\")\n",
    "print(f\"\\tStart Time: {train_pldf['timestamp'].min()}\")\n",
    "print(f\"\\tEnd Time: {train_pldf['timestamp'].max()}\")\n",
    "print(f\"\\tTotal Duration: {train_pldf['timestamp'].max() - train_pldf['timestamp'].min()}\")\n",
    "print(\"\\n\" + \"-\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "6f24f372",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Time Boundaries:\n",
      "\tStart Time: 2024-01-01 00:00:00+05:30\n",
      "\tEnd Time: 2024-05-31 23:30:00+05:30\n",
      "\tTotal Duration: 151 days, 23:30:00\n",
      "\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# reminder of time boundaries\n",
    "print(\"Validation Time Boundaries:\")\n",
    "print(f\"\\tStart Time: {validation_pldf['timestamp'].min()}\")\n",
    "print(f\"\\tEnd Time: {validation_pldf['timestamp'].max()}\")\n",
    "print(f\"\\tTotal Duration: {validation_pldf['timestamp'].max() - validation_pldf['timestamp'].min()}\")\n",
    "print(\"\\n\" + \"-\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "be74c5ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Time Boundaries:\n",
      "\tStart Time: 2024-06-01 00:00:00+05:30\n",
      "\tEnd Time: 2025-05-31 23:30:00+05:30\n",
      "\tTotal Duration: 364 days, 23:30:00\n",
      "\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# reminder of time boundaries\n",
    "print(\"Test Time Boundaries:\")\n",
    "print(f\"\\tStart Time: {test_pldf['timestamp'].min()}\")\n",
    "print(f\"\\tEnd Time: {test_pldf['timestamp'].max()}\")\n",
    "print(f\"\\tTotal Duration: {test_pldf['timestamp'].max() - test_pldf['timestamp'].min()}\")\n",
    "print(\"\\n\" + \"-\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "852bfc50",
   "metadata": {},
   "source": [
    "## Data Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "8b3f0e44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conversion to Pandas DataFrame for compatibility with existing code\n",
    "train_df = train_pldf.to_pandas()\n",
    "validation_df = validation_pldf.to_pandas()\n",
    "test_df = test_pldf.to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "47c177ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns in Train DataFrame:\n",
      "['timestamp', 'city', 'land_latitude', 'land_longitude', 'wind_speed_mps', 'wind_direction_meteorological', 'temperature_celsius', 'precipitation_mm', 'surface_net_solar_radiation_kWh_per_m2', 'surface_solar_radiation_downwards_kWh_per_m2', 'surface_net_solar_radiation_joules_per_m2', 'surface_solar_radiation_downwards_joules_per_m2', 'total_cloud_cover', 'high_cloud_cover', 'medium_cloud_cover', 'low_cloud_cover', 'thermal_generation', 'gas_generation', 'hydro_generation', 'nuclear_generation', 'renewable_generation', 'total_generation', 'demand_met', 'non_renewable_generation', 'tons_co2', 'g_co2_per_kwh', 'tons_co2_per_mwh', 'wind_dir_cardinal_8', 'wind_dir_cardinal_16', 'wind_dir_cardinal_4']\n",
      "\n",
      "Columns in Validation DataFrame:\n",
      "['timestamp', 'city', 'land_latitude', 'land_longitude', 'wind_speed_mps', 'wind_direction_meteorological', 'temperature_celsius', 'precipitation_mm', 'surface_net_solar_radiation_kWh_per_m2', 'surface_solar_radiation_downwards_kWh_per_m2', 'surface_net_solar_radiation_joules_per_m2', 'surface_solar_radiation_downwards_joules_per_m2', 'total_cloud_cover', 'high_cloud_cover', 'medium_cloud_cover', 'low_cloud_cover', 'thermal_generation', 'gas_generation', 'hydro_generation', 'nuclear_generation', 'renewable_generation', 'total_generation', 'demand_met', 'non_renewable_generation', 'tons_co2', 'g_co2_per_kwh', 'tons_co2_per_mwh', 'wind_dir_cardinal_8', 'wind_dir_cardinal_16', 'wind_dir_cardinal_4']\n",
      "\n",
      "Columns in Test DataFrame:\n",
      "['timestamp', 'city', 'land_latitude', 'land_longitude', 'wind_speed_mps', 'wind_direction_meteorological', 'temperature_celsius', 'precipitation_mm', 'surface_net_solar_radiation_kWh_per_m2', 'surface_solar_radiation_downwards_kWh_per_m2', 'surface_net_solar_radiation_joules_per_m2', 'surface_solar_radiation_downwards_joules_per_m2', 'total_cloud_cover', 'high_cloud_cover', 'medium_cloud_cover', 'low_cloud_cover', 'thermal_generation', 'gas_generation', 'hydro_generation', 'nuclear_generation', 'renewable_generation', 'total_generation', 'demand_met', 'non_renewable_generation', 'tons_co2', 'g_co2_per_kwh', 'tons_co2_per_mwh', 'wind_dir_cardinal_8', 'wind_dir_cardinal_16', 'wind_dir_cardinal_4']\n"
     ]
    }
   ],
   "source": [
    "print(\"Columns in Train DataFrame:\")\n",
    "print(train_df.columns.tolist())\n",
    "print(\"\\nColumns in Validation DataFrame:\")\n",
    "print(validation_df.columns.tolist())\n",
    "print(\"\\nColumns in Test DataFrame:\")\n",
    "print(test_df.columns.tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fb0e5e3",
   "metadata": {},
   "source": [
    "### Original Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "8f6dfe84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# marginal_emissions_results_directory\n",
    "# marginal_emissions_logs_directory\n",
    "\n",
    "# marginal_emissions_prefix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f8cdbaa",
   "metadata": {},
   "source": [
    "#### Manual Construction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "0d803f7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_addition_pipeline = Pipeline([\n",
    "    (\"Add_Datetime_Features\", DateTimeFeatureAdder(timestamp_col=\"timestamp\")),\n",
    "    (\"Add_Original_Analysis_Features\", AnalysisFeatureAdder(timestamp_col=\"timestamp\", demand_met_col=\"demand_met\", co2_col=\"tons_co2\")),\n",
    "])\n",
    "feature_addition_pipeline.name = \"FeatureAdditionPipeline\"\n",
    "train_original_added_features_df = feature_addition_pipeline.fit_transform(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "c26c367e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FeatureAdditionPipeline\n"
     ]
    }
   ],
   "source": [
    "print(feature_addition_pipeline.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "3c34153f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns in Training Set (post feature transformation):\n",
      "Index(['timestamp', 'city', 'land_latitude', 'land_longitude',\n",
      "       'wind_speed_mps', 'wind_direction_meteorological',\n",
      "       'temperature_celsius', 'precipitation_mm',\n",
      "       'surface_net_solar_radiation_kWh_per_m2',\n",
      "       'surface_solar_radiation_downwards_kWh_per_m2',\n",
      "       'surface_net_solar_radiation_joules_per_m2',\n",
      "       'surface_solar_radiation_downwards_joules_per_m2', 'total_cloud_cover',\n",
      "       'high_cloud_cover', 'medium_cloud_cover', 'low_cloud_cover',\n",
      "       'thermal_generation', 'gas_generation', 'hydro_generation',\n",
      "       'nuclear_generation', 'renewable_generation', 'total_generation',\n",
      "       'demand_met', 'non_renewable_generation', 'tons_co2', 'g_co2_per_kwh',\n",
      "       'tons_co2_per_mwh', 'wind_dir_cardinal_8', 'wind_dir_cardinal_16',\n",
      "       'wind_dir_cardinal_4', 'year', 'month', 'week_of_year', 'day', 'hour',\n",
      "       'half_hour', 'day_of_week', 'is_weekend', 'time_id', 'demand_met_sqrd',\n",
      "       'log_demand_met', 'log_demand_met_sqrd', 'log_tons_co2'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# Columns in the training set (post feature transformation):\n",
    "print(\"Columns in Training Set (post feature transformation):\")\n",
    "print(train_original_added_features_df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "25c37042",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns in x_original_train_added_features_df:\n",
      "Index(['demand_met', 'demand_met_sqrd',\n",
      "       'surface_net_solar_radiation_kWh_per_m2', 'wind_speed_mps', 'month',\n",
      "       'hour'],\n",
      "      dtype='object')\n",
      "Columns in y_original_train_added_features_df:\n",
      "Index(['tons_co2'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# Original Columns from the R Analysis\n",
    "# y_var = \"tons_co2\"\n",
    "# x_vars = Q : \"demand_met\", Q2 : \"demand_met_sqrd\"\n",
    "# fe_vars = mo: \"month\", h: \"hour\"\n",
    "# group_col = \"k\" (from MultiQuantileBinner on ssr : \"surface_net_solar_radiation_kwh\" and v2: \"wind_speed\"\n",
    "\n",
    "# Dropping to only the relevant columns for reproducing MultiQuantileBinner & Regresion\n",
    "x_original_relevant_columns = [\n",
    "    \"demand_met\", \"demand_met_sqrd\",\n",
    "    \"surface_net_solar_radiation_kWh_per_m2\", \"wind_speed_mps\",\n",
    "    \"month\", \"hour\",\n",
    "]\n",
    "y_original_relevant_columns = [\"tons_co2\"]\n",
    "\n",
    "x_original_train_added_features_df = train_original_added_features_df[x_original_relevant_columns]\n",
    "y_original_train_added_features_df = train_original_added_features_df[y_original_relevant_columns]\n",
    "\n",
    "# confirm the columns in the DataFrame\n",
    "print(\"Columns in x_original_train_added_features_df:\")\n",
    "print(x_original_train_added_features_df.columns)\n",
    "print(\"Columns in y_original_train_added_features_df:\")\n",
    "print(y_original_train_added_features_df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "7b6e51e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the same features to the validation and test sets\n",
    "validation_added_features_df = feature_addition_pipeline.transform(validation_df)\n",
    "test_added_features_df = feature_addition_pipeline.transform(test_df)\n",
    "\n",
    "# Split to X and y for validation and test sets\n",
    "x_validation_added_features_df = validation_added_features_df[x_original_relevant_columns]\n",
    "y_validation_added_features_df = validation_added_features_df[y_original_relevant_columns]\n",
    "x_test_added_features_df = test_added_features_df[x_original_relevant_columns]\n",
    "y_test_added_features_df = test_added_features_df[y_original_relevant_columns]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d472ee6",
   "metadata": {},
   "source": [
    "##### Multi-Quantile Binning Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "49e9b949",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns in x_original_multi_binner_train_added_features_df:\n",
      "Index(['demand_met', 'demand_met_sqrd',\n",
      "       'surface_net_solar_radiation_kWh_per_m2', 'wind_speed_mps', 'month',\n",
      "       'hour', 'surface_net_solar_radiation_kWh_per_m2_group',\n",
      "       'wind_speed_mps_group', 'original_quantile_group_id'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# Run the MultiQuantileBinner with original parameters\n",
    "original_multi_binner = MultiQuantileBinner(\n",
    "    bin_specs={\n",
    "        \"surface_net_solar_radiation_kWh_per_m2\": 5,\n",
    "        \"wind_speed_mps\": 5,\n",
    "    },\n",
    "    group_col_name=\"original_quantile_group_id\"\n",
    ")\n",
    "# Fit the binner on the x_original_train_added_features_df\n",
    "original_multi_binner.fit(x_original_train_added_features_df)\n",
    "\n",
    "# Transform the DataFrame to get the group IDs\n",
    "x_original_multi_binner_train_added_features_df = original_multi_binner.transform(x_original_train_added_features_df)\n",
    "\n",
    "# Checking the columns in the binned DataFrame\n",
    "print(\"Columns in x_original_multi_binner_train_added_features_df:\")\n",
    "print(x_original_multi_binner_train_added_features_df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "5233522d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Skipping group 29: only 1 < 10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns in result_df:\n",
      "Index(['demand_met', 'demand_met_sqrd',\n",
      "       'surface_net_solar_radiation_kWh_per_m2', 'wind_speed_mps', 'month',\n",
      "       'hour', 'surface_net_solar_radiation_kWh_per_m2_group',\n",
      "       'wind_speed_mps_group', 'original_quantile_group_id', 'alpha1',\n",
      "       'alpha2', 'ME'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# Run the GroupwiseRegressor with these original parameters\n",
    "original_quantile_regressor = GroupwiseRegressor(\n",
    "    y_var=\"tons_co2\",\n",
    "    x_vars=[\"demand_met\", \"demand_met_sqrd\"],\n",
    "    fe_vars=[\"month\", \"hour\"],\n",
    "    group_col=\"original_quantile_group_id\",\n",
    "    min_group_size=10,\n",
    "    track_metrics=True,\n",
    "    verbose=True\n",
    ")\n",
    "# Fit and transform the binned DataFrame\n",
    "original_quantile_regressor_result_df = original_quantile_regressor.fit_transform(x_original_multi_binner_train_added_features_df, y_original_train_added_features_df)\n",
    "# Checking the columns in the result DataFrame\n",
    "print(\"Columns in result_df:\")\n",
    "print(original_quantile_regressor_result_df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "90ac04ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Binning the validation and test sets\n",
    "x_original_multi_binner_validation_added_features_df = original_multi_binner.transform(x_validation_added_features_df)\n",
    "x_original_multi_binner_test_added_features_df = original_multi_binner.transform(x_test_added_features_df)\n",
    "\n",
    "# Run the GroupwiseRegressor on the validation set\n",
    "original_quantile_regressor_validation_result_df = original_quantile_regressor.transform(pd.concat([x_original_multi_binner_validation_added_features_df, y_validation_added_features_df], axis=1))\n",
    "# Run the GroupwiseRegressor on the test set\n",
    "original_quantile_regressor_test_result_df = original_quantile_regressor.transform(pd.concat([x_original_multi_binner_test_added_features_df, y_test_added_features_df], axis=1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "d2730541",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>demand_met</th>\n",
       "      <th>demand_met_sqrd</th>\n",
       "      <th>surface_net_solar_radiation_kWh_per_m2</th>\n",
       "      <th>wind_speed_mps</th>\n",
       "      <th>month</th>\n",
       "      <th>hour</th>\n",
       "      <th>surface_net_solar_radiation_kWh_per_m2_group</th>\n",
       "      <th>wind_speed_mps_group</th>\n",
       "      <th>original_quantile_group_id</th>\n",
       "      <th>tons_co2</th>\n",
       "      <th>alpha1</th>\n",
       "      <th>alpha2</th>\n",
       "      <th>ME</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>231107.666667</td>\n",
       "      <td>5.341075e+10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.454928</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>85191.05750</td>\n",
       "      <td>0.677361</td>\n",
       "      <td>-7.758257e-07</td>\n",
       "      <td>0.318763</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>231107.666667</td>\n",
       "      <td>5.341075e+10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.886852</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>85191.05750</td>\n",
       "      <td>0.677361</td>\n",
       "      <td>-7.758257e-07</td>\n",
       "      <td>0.318763</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>231107.666667</td>\n",
       "      <td>5.341075e+10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.309101</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>85191.05750</td>\n",
       "      <td>0.674336</td>\n",
       "      <td>-7.300676e-07</td>\n",
       "      <td>0.336888</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>231107.666667</td>\n",
       "      <td>5.341075e+10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.021624</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>85191.05750</td>\n",
       "      <td>0.677361</td>\n",
       "      <td>-7.758257e-07</td>\n",
       "      <td>0.318763</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>231107.666667</td>\n",
       "      <td>5.341075e+10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.143778</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>85191.05750</td>\n",
       "      <td>0.533647</td>\n",
       "      <td>-3.042182e-07</td>\n",
       "      <td>0.393033</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>788395</th>\n",
       "      <td>212822.750000</td>\n",
       "      <td>4.529352e+10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.516465</td>\n",
       "      <td>5</td>\n",
       "      <td>23</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>74410.90595</td>\n",
       "      <td>0.677361</td>\n",
       "      <td>-7.758257e-07</td>\n",
       "      <td>0.347135</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>788396</th>\n",
       "      <td>212822.750000</td>\n",
       "      <td>4.529352e+10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.293586</td>\n",
       "      <td>5</td>\n",
       "      <td>23</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>74410.90595</td>\n",
       "      <td>0.677361</td>\n",
       "      <td>-7.758257e-07</td>\n",
       "      <td>0.347135</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>788397</th>\n",
       "      <td>212822.750000</td>\n",
       "      <td>4.529352e+10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.021289</td>\n",
       "      <td>5</td>\n",
       "      <td>23</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>74410.90595</td>\n",
       "      <td>0.677361</td>\n",
       "      <td>-7.758257e-07</td>\n",
       "      <td>0.347135</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>788398</th>\n",
       "      <td>212822.750000</td>\n",
       "      <td>4.529352e+10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.742792</td>\n",
       "      <td>5</td>\n",
       "      <td>23</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>74410.90595</td>\n",
       "      <td>0.677361</td>\n",
       "      <td>-7.758257e-07</td>\n",
       "      <td>0.347135</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>788399</th>\n",
       "      <td>212822.750000</td>\n",
       "      <td>4.529352e+10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.456929</td>\n",
       "      <td>5</td>\n",
       "      <td>23</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>74410.90595</td>\n",
       "      <td>0.674336</td>\n",
       "      <td>-7.300676e-07</td>\n",
       "      <td>0.363586</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>788400 rows × 13 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           demand_met  demand_met_sqrd  \\\n",
       "0       231107.666667     5.341075e+10   \n",
       "1       231107.666667     5.341075e+10   \n",
       "2       231107.666667     5.341075e+10   \n",
       "3       231107.666667     5.341075e+10   \n",
       "4       231107.666667     5.341075e+10   \n",
       "...               ...              ...   \n",
       "788395  212822.750000     4.529352e+10   \n",
       "788396  212822.750000     4.529352e+10   \n",
       "788397  212822.750000     4.529352e+10   \n",
       "788398  212822.750000     4.529352e+10   \n",
       "788399  212822.750000     4.529352e+10   \n",
       "\n",
       "        surface_net_solar_radiation_kWh_per_m2  wind_speed_mps month hour  \\\n",
       "0                                          0.0        3.454928     6    0   \n",
       "1                                          0.0        2.886852     6    0   \n",
       "2                                          0.0        2.309101     6    0   \n",
       "3                                          0.0        3.021624     6    0   \n",
       "4                                          0.0        4.143778     6    0   \n",
       "...                                        ...             ...   ...  ...   \n",
       "788395                                     0.0        3.516465     5   23   \n",
       "788396                                     0.0        3.293586     5   23   \n",
       "788397                                     0.0        3.021289     5   23   \n",
       "788398                                     0.0        2.742792     5   23   \n",
       "788399                                     0.0        2.456929     5   23   \n",
       "\n",
       "        surface_net_solar_radiation_kWh_per_m2_group  wind_speed_mps_group  \\\n",
       "0                                                  1                     5   \n",
       "1                                                  1                     5   \n",
       "2                                                  1                     4   \n",
       "3                                                  1                     5   \n",
       "4                                                  1                     6   \n",
       "...                                              ...                   ...   \n",
       "788395                                             1                     5   \n",
       "788396                                             1                     5   \n",
       "788397                                             1                     5   \n",
       "788398                                             1                     5   \n",
       "788399                                             1                     4   \n",
       "\n",
       "        original_quantile_group_id     tons_co2    alpha1        alpha2  \\\n",
       "0                                5  85191.05750  0.677361 -7.758257e-07   \n",
       "1                                5  85191.05750  0.677361 -7.758257e-07   \n",
       "2                                4  85191.05750  0.674336 -7.300676e-07   \n",
       "3                                5  85191.05750  0.677361 -7.758257e-07   \n",
       "4                                6  85191.05750  0.533647 -3.042182e-07   \n",
       "...                            ...          ...       ...           ...   \n",
       "788395                           5  74410.90595  0.677361 -7.758257e-07   \n",
       "788396                           5  74410.90595  0.677361 -7.758257e-07   \n",
       "788397                           5  74410.90595  0.677361 -7.758257e-07   \n",
       "788398                           5  74410.90595  0.677361 -7.758257e-07   \n",
       "788399                           4  74410.90595  0.674336 -7.300676e-07   \n",
       "\n",
       "              ME  \n",
       "0       0.318763  \n",
       "1       0.318763  \n",
       "2       0.336888  \n",
       "3       0.318763  \n",
       "4       0.393033  \n",
       "...          ...  \n",
       "788395  0.347135  \n",
       "788396  0.347135  \n",
       "788397  0.347135  \n",
       "788398  0.347135  \n",
       "788399  0.363586  \n",
       "\n",
       "[788400 rows x 13 columns]"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "original_quantile_regressor_test_result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "12cbc9b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>original_quantile_group_id</th>\n",
       "      <th>r2</th>\n",
       "      <th>rmse</th>\n",
       "      <th>mae</th>\n",
       "      <th>mape</th>\n",
       "      <th>n_obs</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>0.909125</td>\n",
       "      <td>2390.234231</td>\n",
       "      <td>1833.218307</td>\n",
       "      <td>2.946539</td>\n",
       "      <td>167292</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>0.918085</td>\n",
       "      <td>2310.649130</td>\n",
       "      <td>1780.402866</td>\n",
       "      <td>2.873374</td>\n",
       "      <td>201501</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4</td>\n",
       "      <td>0.914882</td>\n",
       "      <td>2322.109860</td>\n",
       "      <td>1789.443365</td>\n",
       "      <td>2.874476</td>\n",
       "      <td>196531</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5</td>\n",
       "      <td>0.911333</td>\n",
       "      <td>2253.774303</td>\n",
       "      <td>1728.309114</td>\n",
       "      <td>2.775676</td>\n",
       "      <td>161286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6</td>\n",
       "      <td>0.914709</td>\n",
       "      <td>2314.663837</td>\n",
       "      <td>1793.099022</td>\n",
       "      <td>2.963679</td>\n",
       "      <td>102328</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>9</td>\n",
       "      <td>0.892524</td>\n",
       "      <td>2238.845549</td>\n",
       "      <td>1473.134534</td>\n",
       "      <td>2.292101</td>\n",
       "      <td>39175</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>10</td>\n",
       "      <td>0.912670</td>\n",
       "      <td>2022.037759</td>\n",
       "      <td>1351.058584</td>\n",
       "      <td>2.098423</td>\n",
       "      <td>52028</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>11</td>\n",
       "      <td>0.905317</td>\n",
       "      <td>1987.450534</td>\n",
       "      <td>1372.955713</td>\n",
       "      <td>2.127739</td>\n",
       "      <td>57666</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>12</td>\n",
       "      <td>0.910076</td>\n",
       "      <td>1912.321243</td>\n",
       "      <td>1373.581438</td>\n",
       "      <td>2.137938</td>\n",
       "      <td>42637</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>13</td>\n",
       "      <td>0.893904</td>\n",
       "      <td>2111.793039</td>\n",
       "      <td>1523.494217</td>\n",
       "      <td>2.377541</td>\n",
       "      <td>15941</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>16</td>\n",
       "      <td>0.907164</td>\n",
       "      <td>2398.216332</td>\n",
       "      <td>1693.659435</td>\n",
       "      <td>2.756517</td>\n",
       "      <td>86979</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>17</td>\n",
       "      <td>0.915010</td>\n",
       "      <td>2304.037121</td>\n",
       "      <td>1711.804361</td>\n",
       "      <td>2.794794</td>\n",
       "      <td>74255</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>18</td>\n",
       "      <td>0.911386</td>\n",
       "      <td>2282.559220</td>\n",
       "      <td>1741.677656</td>\n",
       "      <td>2.856782</td>\n",
       "      <td>73859</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>19</td>\n",
       "      <td>0.913134</td>\n",
       "      <td>2125.950197</td>\n",
       "      <td>1630.175827</td>\n",
       "      <td>2.654994</td>\n",
       "      <td>77159</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>20</td>\n",
       "      <td>0.914238</td>\n",
       "      <td>2293.721242</td>\n",
       "      <td>1721.389160</td>\n",
       "      <td>2.829157</td>\n",
       "      <td>70483</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>23</td>\n",
       "      <td>0.915714</td>\n",
       "      <td>2397.160737</td>\n",
       "      <td>1788.764068</td>\n",
       "      <td>3.035010</td>\n",
       "      <td>94449</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>24</td>\n",
       "      <td>0.905250</td>\n",
       "      <td>2507.619036</td>\n",
       "      <td>1907.692262</td>\n",
       "      <td>3.260050</td>\n",
       "      <td>81431</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>25</td>\n",
       "      <td>0.902519</td>\n",
       "      <td>2557.778070</td>\n",
       "      <td>1945.456396</td>\n",
       "      <td>3.327301</td>\n",
       "      <td>82199</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>26</td>\n",
       "      <td>0.894228</td>\n",
       "      <td>2516.052940</td>\n",
       "      <td>1949.240321</td>\n",
       "      <td>3.310554</td>\n",
       "      <td>94900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>27</td>\n",
       "      <td>0.885658</td>\n",
       "      <td>2584.733770</td>\n",
       "      <td>2005.912788</td>\n",
       "      <td>3.456950</td>\n",
       "      <td>120061</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>30</td>\n",
       "      <td>0.896756</td>\n",
       "      <td>2402.955860</td>\n",
       "      <td>1857.078551</td>\n",
       "      <td>3.050196</td>\n",
       "      <td>85144</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>31</td>\n",
       "      <td>0.899930</td>\n",
       "      <td>2371.520766</td>\n",
       "      <td>1817.723754</td>\n",
       "      <td>2.978965</td>\n",
       "      <td>63826</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>32</td>\n",
       "      <td>0.904346</td>\n",
       "      <td>2368.269757</td>\n",
       "      <td>1825.202520</td>\n",
       "      <td>2.987409</td>\n",
       "      <td>62784</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>33</td>\n",
       "      <td>0.895512</td>\n",
       "      <td>2448.212118</td>\n",
       "      <td>1869.207230</td>\n",
       "      <td>3.030461</td>\n",
       "      <td>97058</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>34</td>\n",
       "      <td>0.880252</td>\n",
       "      <td>2493.184324</td>\n",
       "      <td>1914.426678</td>\n",
       "      <td>3.101621</td>\n",
       "      <td>164227</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    original_quantile_group_id        r2         rmse          mae      mape  \\\n",
       "0                            2  0.909125  2390.234231  1833.218307  2.946539   \n",
       "1                            3  0.918085  2310.649130  1780.402866  2.873374   \n",
       "2                            4  0.914882  2322.109860  1789.443365  2.874476   \n",
       "3                            5  0.911333  2253.774303  1728.309114  2.775676   \n",
       "4                            6  0.914709  2314.663837  1793.099022  2.963679   \n",
       "5                            9  0.892524  2238.845549  1473.134534  2.292101   \n",
       "6                           10  0.912670  2022.037759  1351.058584  2.098423   \n",
       "7                           11  0.905317  1987.450534  1372.955713  2.127739   \n",
       "8                           12  0.910076  1912.321243  1373.581438  2.137938   \n",
       "9                           13  0.893904  2111.793039  1523.494217  2.377541   \n",
       "10                          16  0.907164  2398.216332  1693.659435  2.756517   \n",
       "11                          17  0.915010  2304.037121  1711.804361  2.794794   \n",
       "12                          18  0.911386  2282.559220  1741.677656  2.856782   \n",
       "13                          19  0.913134  2125.950197  1630.175827  2.654994   \n",
       "14                          20  0.914238  2293.721242  1721.389160  2.829157   \n",
       "15                          23  0.915714  2397.160737  1788.764068  3.035010   \n",
       "16                          24  0.905250  2507.619036  1907.692262  3.260050   \n",
       "17                          25  0.902519  2557.778070  1945.456396  3.327301   \n",
       "18                          26  0.894228  2516.052940  1949.240321  3.310554   \n",
       "19                          27  0.885658  2584.733770  2005.912788  3.456950   \n",
       "20                          30  0.896756  2402.955860  1857.078551  3.050196   \n",
       "21                          31  0.899930  2371.520766  1817.723754  2.978965   \n",
       "22                          32  0.904346  2368.269757  1825.202520  2.987409   \n",
       "23                          33  0.895512  2448.212118  1869.207230  3.030461   \n",
       "24                          34  0.880252  2493.184324  1914.426678  3.101621   \n",
       "\n",
       "     n_obs  \n",
       "0   167292  \n",
       "1   201501  \n",
       "2   196531  \n",
       "3   161286  \n",
       "4   102328  \n",
       "5    39175  \n",
       "6    52028  \n",
       "7    57666  \n",
       "8    42637  \n",
       "9    15941  \n",
       "10   86979  \n",
       "11   74255  \n",
       "12   73859  \n",
       "13   77159  \n",
       "14   70483  \n",
       "15   94449  \n",
       "16   81431  \n",
       "17   82199  \n",
       "18   94900  \n",
       "19  120061  \n",
       "20   85144  \n",
       "21   63826  \n",
       "22   62784  \n",
       "23   97058  \n",
       "24  164227  "
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "original_quantile_regressor.get_metrics(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2277e49",
   "metadata": {},
   "source": [
    "##### Median Binning Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "cd178491",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns in x_original_median_binner_train_added_features_df:\n",
      "Index(['demand_met', 'demand_met_sqrd',\n",
      "       'surface_net_solar_radiation_kWh_per_m2', 'wind_speed_mps', 'month',\n",
      "       'hour', 'median_group_id',\n",
      "       'surface_net_solar_radiation_kWh_per_m2_group', 'wind_speed_mps_group'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# Run the MultiMedianBinner with original parameters\n",
    "original_median_binner = MultiMedianBinner(\n",
    "    variables=[\n",
    "        \"surface_net_solar_radiation_kWh_per_m2\",\n",
    "        \"wind_speed_mps\"\n",
    "    ],\n",
    "    group_col_name=\"median_group_id\",\n",
    ")\n",
    "# Fit the binner on the x_original_train_added_features_df\n",
    "original_median_binner.fit(x_original_train_added_features_df)\n",
    "\n",
    "# Transform the DataFrame to get the group IDs\n",
    "x_original_median_binner_train_added_features_df = original_median_binner.transform(x_original_train_added_features_df)\n",
    "\n",
    "# Checking the columns in the binned DataFrame\n",
    "print(\"Columns in x_original_median_binner_train_added_features_df:\")\n",
    "print(x_original_median_binner_train_added_features_df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "3e95c55b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns in result_df:\n",
      "Index(['demand_met', 'demand_met_sqrd',\n",
      "       'surface_net_solar_radiation_kWh_per_m2', 'wind_speed_mps', 'month',\n",
      "       'hour', 'median_group_id',\n",
      "       'surface_net_solar_radiation_kWh_per_m2_group', 'wind_speed_mps_group',\n",
      "       'alpha1', 'alpha2', 'ME'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# Run the GroupwiseRegressor with these original parameters\n",
    "original_median_regressor = GroupwiseRegressor(\n",
    "    y_var=\"tons_co2\",\n",
    "    x_vars=[\"demand_met\", \"demand_met_sqrd\"],\n",
    "    fe_vars=[\"month\", \"hour\"],\n",
    "    group_col=\"median_group_id\",\n",
    "    min_group_size=10,\n",
    "    track_metrics=True,\n",
    "    verbose=True\n",
    ")\n",
    "# Fit and transform the binned DataFrame\n",
    "result_df = original_median_regressor.fit_transform(x_original_median_binner_train_added_features_df, y_original_train_added_features_df)\n",
    "# Checking the columns in the result DataFrame\n",
    "print(\"Columns in result_df:\")\n",
    "print(result_df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "bd9b99ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Binning the validation and test sets\n",
    "x_original_median_binner_validation_added_features_df = original_median_binner.transform(x_validation_added_features_df)\n",
    "x_original_median_binner_test_added_features_df = original_median_binner.transform(x_test_added_features_df)\n",
    "\n",
    "# Run the GroupwiseRegressor on the validation set\n",
    "original_median_regressor_validation_result_df = original_median_regressor.transform(pd.concat([x_original_median_binner_validation_added_features_df, y_validation_added_features_df], axis=1))\n",
    "# Run the GroupwiseRegressor on the test set\n",
    "original_median_regressor_test_result_df = original_median_regressor.transform(pd.concat([x_original_median_binner_test_added_features_df, y_test_added_features_df], axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "f59581e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>demand_met</th>\n",
       "      <th>demand_met_sqrd</th>\n",
       "      <th>surface_net_solar_radiation_kWh_per_m2</th>\n",
       "      <th>wind_speed_mps</th>\n",
       "      <th>month</th>\n",
       "      <th>hour</th>\n",
       "      <th>median_group_id</th>\n",
       "      <th>surface_net_solar_radiation_kWh_per_m2_group</th>\n",
       "      <th>wind_speed_mps_group</th>\n",
       "      <th>tons_co2</th>\n",
       "      <th>alpha1</th>\n",
       "      <th>alpha2</th>\n",
       "      <th>ME</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>231107.666667</td>\n",
       "      <td>5.341075e+10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.454928</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>85191.05750</td>\n",
       "      <td>0.609027</td>\n",
       "      <td>-5.970059e-07</td>\n",
       "      <td>0.333082</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>231107.666667</td>\n",
       "      <td>5.341075e+10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.886852</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>85191.05750</td>\n",
       "      <td>0.609027</td>\n",
       "      <td>-5.970059e-07</td>\n",
       "      <td>0.333082</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>231107.666667</td>\n",
       "      <td>5.341075e+10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.309101</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>85191.05750</td>\n",
       "      <td>0.618650</td>\n",
       "      <td>-6.012432e-07</td>\n",
       "      <td>0.340746</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>231107.666667</td>\n",
       "      <td>5.341075e+10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.021624</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>85191.05750</td>\n",
       "      <td>0.609027</td>\n",
       "      <td>-5.970059e-07</td>\n",
       "      <td>0.333082</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>231107.666667</td>\n",
       "      <td>5.341075e+10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.143778</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>85191.05750</td>\n",
       "      <td>0.609027</td>\n",
       "      <td>-5.970059e-07</td>\n",
       "      <td>0.333082</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>788395</th>\n",
       "      <td>212822.750000</td>\n",
       "      <td>4.529352e+10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.516465</td>\n",
       "      <td>5</td>\n",
       "      <td>23</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>74410.90595</td>\n",
       "      <td>0.609027</td>\n",
       "      <td>-5.970059e-07</td>\n",
       "      <td>0.354914</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>788396</th>\n",
       "      <td>212822.750000</td>\n",
       "      <td>4.529352e+10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.293586</td>\n",
       "      <td>5</td>\n",
       "      <td>23</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>74410.90595</td>\n",
       "      <td>0.609027</td>\n",
       "      <td>-5.970059e-07</td>\n",
       "      <td>0.354914</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>788397</th>\n",
       "      <td>212822.750000</td>\n",
       "      <td>4.529352e+10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.021289</td>\n",
       "      <td>5</td>\n",
       "      <td>23</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>74410.90595</td>\n",
       "      <td>0.609027</td>\n",
       "      <td>-5.970059e-07</td>\n",
       "      <td>0.354914</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>788398</th>\n",
       "      <td>212822.750000</td>\n",
       "      <td>4.529352e+10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.742792</td>\n",
       "      <td>5</td>\n",
       "      <td>23</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>74410.90595</td>\n",
       "      <td>0.609027</td>\n",
       "      <td>-5.970059e-07</td>\n",
       "      <td>0.354914</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>788399</th>\n",
       "      <td>212822.750000</td>\n",
       "      <td>4.529352e+10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.456929</td>\n",
       "      <td>5</td>\n",
       "      <td>23</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>74410.90595</td>\n",
       "      <td>0.609027</td>\n",
       "      <td>-5.970059e-07</td>\n",
       "      <td>0.354914</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>788400 rows × 13 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           demand_met  demand_met_sqrd  \\\n",
       "0       231107.666667     5.341075e+10   \n",
       "1       231107.666667     5.341075e+10   \n",
       "2       231107.666667     5.341075e+10   \n",
       "3       231107.666667     5.341075e+10   \n",
       "4       231107.666667     5.341075e+10   \n",
       "...               ...              ...   \n",
       "788395  212822.750000     4.529352e+10   \n",
       "788396  212822.750000     4.529352e+10   \n",
       "788397  212822.750000     4.529352e+10   \n",
       "788398  212822.750000     4.529352e+10   \n",
       "788399  212822.750000     4.529352e+10   \n",
       "\n",
       "        surface_net_solar_radiation_kWh_per_m2  wind_speed_mps month hour  \\\n",
       "0                                          0.0        3.454928     6    0   \n",
       "1                                          0.0        2.886852     6    0   \n",
       "2                                          0.0        2.309101     6    0   \n",
       "3                                          0.0        3.021624     6    0   \n",
       "4                                          0.0        4.143778     6    0   \n",
       "...                                        ...             ...   ...  ...   \n",
       "788395                                     0.0        3.516465     5   23   \n",
       "788396                                     0.0        3.293586     5   23   \n",
       "788397                                     0.0        3.021289     5   23   \n",
       "788398                                     0.0        2.742792     5   23   \n",
       "788399                                     0.0        2.456929     5   23   \n",
       "\n",
       "        median_group_id  surface_net_solar_radiation_kWh_per_m2_group  \\\n",
       "0                     2                                             0   \n",
       "1                     2                                             0   \n",
       "2                     1                                             0   \n",
       "3                     2                                             0   \n",
       "4                     2                                             0   \n",
       "...                 ...                                           ...   \n",
       "788395                2                                             0   \n",
       "788396                2                                             0   \n",
       "788397                2                                             0   \n",
       "788398                2                                             0   \n",
       "788399                2                                             0   \n",
       "\n",
       "        wind_speed_mps_group     tons_co2    alpha1        alpha2        ME  \n",
       "0                          1  85191.05750  0.609027 -5.970059e-07  0.333082  \n",
       "1                          1  85191.05750  0.609027 -5.970059e-07  0.333082  \n",
       "2                          0  85191.05750  0.618650 -6.012432e-07  0.340746  \n",
       "3                          1  85191.05750  0.609027 -5.970059e-07  0.333082  \n",
       "4                          1  85191.05750  0.609027 -5.970059e-07  0.333082  \n",
       "...                      ...          ...       ...           ...       ...  \n",
       "788395                     1  74410.90595  0.609027 -5.970059e-07  0.354914  \n",
       "788396                     1  74410.90595  0.609027 -5.970059e-07  0.354914  \n",
       "788397                     1  74410.90595  0.609027 -5.970059e-07  0.354914  \n",
       "788398                     1  74410.90595  0.609027 -5.970059e-07  0.354914  \n",
       "788399                     1  74410.90595  0.609027 -5.970059e-07  0.354914  \n",
       "\n",
       "[788400 rows x 13 columns]"
      ]
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "original_median_regressor_test_result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "ffa2befe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   median_group_id        r2         rmse          mae      mape   n_obs\n",
      "0                1  0.905345  2357.142913  1756.956437  2.802556  671350\n",
      "1                2  0.900719  2336.512436  1790.228601  2.877601  511252\n",
      "2                3  0.907930  2471.083530  1887.122476  3.152542  511250\n",
      "3                4  0.894911  2546.163130  1979.647324  3.290793  671348\n"
     ]
    }
   ],
   "source": [
    "print(original_median_regressor.get_metrics(summarise=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc94ee78",
   "metadata": {},
   "source": [
    "#### Pipeline Development"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "e4a20740",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Engineering Pipeline\n",
    "feature_addition_pipeline = Pipeline([\n",
    "    (\"Add_Datetime_Features\", DateTimeFeatureAdder(timestamp_col=\"timestamp\")),\n",
    "    (\"Add_Original_Analysis_Features\", AnalysisFeatureAdder(timestamp_col=\"timestamp\", demand_met_col=\"demand_met\", co2_col=\"tons_co2\")),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "f85d48fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# assuming full_pipeline = Pipeline([...,\"regressor\", reg])\n",
    "train_pdf_x_all = train_df.drop(columns=[\"tons_co2\"])\n",
    "train_pdf_y = train_df[\"tons_co2\"]\n",
    "\n",
    "validation_pdf_x_all = validation_df.drop(columns=[\"tons_co2\"])\n",
    "validation_pdf_y = validation_df[\"tons_co2\"]\n",
    "test_pdf_x_all = test_df.drop(columns=[\"tons_co2\"])\n",
    "test_pdf_y = test_df[\"tons_co2\"]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8f16e00",
   "metadata": {},
   "source": [
    "##### Quantiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "773103b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BINNERS\n",
    "original_multi_binner = MultiQuantileBinner(\n",
    "    bin_specs={\n",
    "        \"surface_net_solar_radiation_kWh_per_m2\": 5,\n",
    "        \"wind_speed_mps\": 5,\n",
    "    },\n",
    "    group_col_name=\"original_quantile_group_id\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "0072eb69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# REGRESSORS\n",
    "original_multi_binner_regressor = GroupwiseRegressor(\n",
    "    y_var=\"tons_co2\",\n",
    "    x_vars=[\"demand_met\", \"demand_met_sqrd\"],\n",
    "    fe_vars=[\"month\", \"hour\"],\n",
    "    group_col=\"original_quantile_group_id\",\n",
    "    min_group_size=20,\n",
    "    track_metrics=True,\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "8952d13d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PIPELINES\n",
    "original_multi_binner_regressor_pipeline = Pipeline([\n",
    "    (\"Feature_Addition\", feature_addition_pipeline),\n",
    "    (\"Multi_Quantile_Binner\", original_multi_binner),\n",
    "    (\"Groupwise_Regressor\", original_multi_binner_regressor)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "0944e805",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # TESTING - note that this will run regardless - so uncomment only if you want it run\n",
    "# train_logs, x_cols_used_train, _ = run_regressor_model(original_multi_binner_regressor_pipeline, train_pdf_x_all, train_pdf_y, split_name=\"train\")\n",
    "# val_logs, x_cols_used_val, _ = run_regressor_model(original_multi_binner_regressor_pipeline, validation_pdf_x_all, validation_pdf_y, split_name=\"validation\")\n",
    "# test_logs, x_cols_used_test, _ = run_regressor_model(original_multi_binner_regressor_pipeline, test_pdf_x_all, test_pdf_y, split_name=\"test\")\n",
    "# summarise_metrics_logs(train_logs=train_logs, val_logs=val_logs, test_logs=test_logs, user_pipeline=original_multi_binner_regressor_pipeline, x_columns=x_cols_used_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "02e3305f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Skipping group 29: only 1 < 20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LOG] 25 rows for split=train, model_id=13b053372ad4d1d6601337e6d6bd236be66de3cff09dd16909159b65bce23758, random_state=12\n",
      "[LOG] 25 rows for split=validation, model_id=13b053372ad4d1d6601337e6d6bd236be66de3cff09dd16909159b65bce23758, random_state=12\n",
      "[SAVE] Appended to /Users/Daniel/Desktop/IRP_WORK_UPDATED/irp-dbk24/code_and_analysis/data/marginal_emissions_development/logs/marginal_emissions_results.part000.csv, index updated.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model_id_hash</th>\n",
       "      <th>random_state</th>\n",
       "      <th>params_json</th>\n",
       "      <th>log_time</th>\n",
       "      <th>model_name</th>\n",
       "      <th>pipeline_steps</th>\n",
       "      <th>pipeline_n_steps</th>\n",
       "      <th>x_columns</th>\n",
       "      <th>metrics_by_group</th>\n",
       "      <th>r2_train</th>\n",
       "      <th>...</th>\n",
       "      <th>mape_validation_micro</th>\n",
       "      <th>r2_validation_energy_micro</th>\n",
       "      <th>rmse_validation_energy_micro</th>\n",
       "      <th>mae_validation_energy_micro</th>\n",
       "      <th>mape_validation_energy_micro</th>\n",
       "      <th>energy_MWh_validation_total</th>\n",
       "      <th>pooled_co2_train</th>\n",
       "      <th>fd_me_train</th>\n",
       "      <th>pooled_co2_validation</th>\n",
       "      <th>fd_me_validation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>13b053372ad4d1d6601337e6d6bd236be66de3cff09dd1...</td>\n",
       "      <td>12</td>\n",
       "      <td>{\"Feature_Addition\":\"Pipeline(steps=[('Add_Dat...</td>\n",
       "      <td>2025-08-16T23:30:38.565267</td>\n",
       "      <td>GroupwiseRegressor</td>\n",
       "      <td>[Feature_Addition, Multi_Quantile_Binner, Grou...</td>\n",
       "      <td>3</td>\n",
       "      <td>[demand_met, demand_met_sqrd, month, hour]</td>\n",
       "      <td>{'train': {2: {'r2': 0.9091252456040023, 'rmse...</td>\n",
       "      <td>0.904949</td>\n",
       "      <td>...</td>\n",
       "      <td>3.164732</td>\n",
       "      <td>0.695585</td>\n",
       "      <td>2909.583009</td>\n",
       "      <td>2341.624912</td>\n",
       "      <td>3.175503</td>\n",
       "      <td>3.165294e+10</td>\n",
       "      <td>{\"r2\": 0.9094852785665317, \"rmse\": 2355.431764...</td>\n",
       "      <td>{\"pearson_r\": -0.0010488140073209547, \"spearma...</td>\n",
       "      <td>{\"r2\": 0.758011743909146, \"rmse\": 2935.3213851...</td>\n",
       "      <td>{\"pearson_r\": 0.01864741184605731, \"spearman_r...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows × 43 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       model_id_hash  random_state  \\\n",
       "0  13b053372ad4d1d6601337e6d6bd236be66de3cff09dd1...            12   \n",
       "\n",
       "                                         params_json  \\\n",
       "0  {\"Feature_Addition\":\"Pipeline(steps=[('Add_Dat...   \n",
       "\n",
       "                     log_time          model_name  \\\n",
       "0  2025-08-16T23:30:38.565267  GroupwiseRegressor   \n",
       "\n",
       "                                      pipeline_steps  pipeline_n_steps  \\\n",
       "0  [Feature_Addition, Multi_Quantile_Binner, Grou...                 3   \n",
       "\n",
       "                                    x_columns  \\\n",
       "0  [demand_met, demand_met_sqrd, month, hour]   \n",
       "\n",
       "                                    metrics_by_group  r2_train  ...  \\\n",
       "0  {'train': {2: {'r2': 0.9091252456040023, 'rmse...  0.904949  ...   \n",
       "\n",
       "   mape_validation_micro  r2_validation_energy_micro  \\\n",
       "0               3.164732                    0.695585   \n",
       "\n",
       "   rmse_validation_energy_micro  mae_validation_energy_micro  \\\n",
       "0                   2909.583009                  2341.624912   \n",
       "\n",
       "   mape_validation_energy_micro  energy_MWh_validation_total  \\\n",
       "0                      3.175503                 3.165294e+10   \n",
       "\n",
       "                                    pooled_co2_train  \\\n",
       "0  {\"r2\": 0.9094852785665317, \"rmse\": 2355.431764...   \n",
       "\n",
       "                                         fd_me_train  \\\n",
       "0  {\"pearson_r\": -0.0010488140073209547, \"spearma...   \n",
       "\n",
       "                               pooled_co2_validation  \\\n",
       "0  {\"r2\": 0.758011743909146, \"rmse\": 2935.3213851...   \n",
       "\n",
       "                                    fd_me_validation  \n",
       "0  {\"pearson_r\": 0.01864741184605731, \"spearman_r...  \n",
       "\n",
       "[1 rows x 43 columns]"
      ]
     },
     "execution_count": 188,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#ORCHESTRATORS\n",
    "regressor_orchestrator(\n",
    "    user_pipeline=original_multi_binner_regressor_pipeline,\n",
    "    x_splits={\n",
    "        \"train\": train_pdf_x_all,\n",
    "        \"validation\": validation_pdf_x_all,\n",
    "        \"test\": test_pdf_x_all\n",
    "    },\n",
    "    y_splits={\n",
    "        \"train\": train_pdf_y,\n",
    "        \"validation\": validation_pdf_y,\n",
    "        \"test\": test_pdf_y\n",
    "    },\n",
    "    random_state=12,\n",
    "    group_col_name=\"quantile_group_id\",\n",
    "    interval_hours=0.5,\n",
    "    results_dir = marginal_emissions_logs_directory,\n",
    "    file_prefix = marginal_emissions_prefix,\n",
    "    force_run=True,\n",
    "    compute_test=False,\n",
    "    force_overwrite=True,\n",
    "    fsync=False,    # set to True when running on HPC\n",
    "    max_log_mb=95,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "a4f618da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model_id_hash</th>\n",
       "      <th>random_state</th>\n",
       "      <th>params_json</th>\n",
       "      <th>log_time</th>\n",
       "      <th>model_name</th>\n",
       "      <th>pipeline_steps</th>\n",
       "      <th>pipeline_n_steps</th>\n",
       "      <th>x_columns</th>\n",
       "      <th>metrics_by_group</th>\n",
       "      <th>r2_train</th>\n",
       "      <th>...</th>\n",
       "      <th>mape_validation_micro</th>\n",
       "      <th>r2_validation_energy_micro</th>\n",
       "      <th>rmse_validation_energy_micro</th>\n",
       "      <th>mae_validation_energy_micro</th>\n",
       "      <th>mape_validation_energy_micro</th>\n",
       "      <th>energy_MWh_validation_total</th>\n",
       "      <th>pooled_co2_train</th>\n",
       "      <th>fd_me_train</th>\n",
       "      <th>pooled_co2_validation</th>\n",
       "      <th>fd_me_validation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>690f498780e0712e252cc93bcfd4abacd65e1efdf3fd1a...</td>\n",
       "      <td>12</td>\n",
       "      <td>{\"Feature_Addition\":\"Pipeline(steps=[('Add_Dat...</td>\n",
       "      <td>2025-08-12T02:37:45.646345</td>\n",
       "      <td>GroupwiseRegressor</td>\n",
       "      <td>['Feature_Addition', 'Multi_Median_Binner', 'G...</td>\n",
       "      <td>3</td>\n",
       "      <td>['demand_met', 'demand_met_sqrd', 'month', 'ho...</td>\n",
       "      <td>{'train': {1: {'r2': 0.7759440466267724, 'rmse...</td>\n",
       "      <td>0.777564</td>\n",
       "      <td>...</td>\n",
       "      <td>3.762266</td>\n",
       "      <td>0.651356</td>\n",
       "      <td>3643.997308</td>\n",
       "      <td>2596.004372</td>\n",
       "      <td>3.774859</td>\n",
       "      <td>3.162945e+10</td>\n",
       "      <td>{\"r2\": 0.7792042909467476, \"rmse\": 3942.115886...</td>\n",
       "      <td>{\"pearson_r\": -0.0013750900776175363, \"spearma...</td>\n",
       "      <td>{\"r2\": 0.6674819506158596, \"rmse\": 3666.654134...</td>\n",
       "      <td>{\"pearson_r\": -0.0027290646933841333, \"spearma...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ff562dc468b7f0d2d7dadb624dec64eb545fdfe735279f...</td>\n",
       "      <td>12</td>\n",
       "      <td>{\"Feature_Addition\":\"Pipeline(steps=[('Add_Dat...</td>\n",
       "      <td>2025-08-12T02:38:45.678555</td>\n",
       "      <td>GroupwiseRegressor</td>\n",
       "      <td>['Feature_Addition', 'Multi_Median_Binner', 'G...</td>\n",
       "      <td>3</td>\n",
       "      <td>['demand_met', 'demand_met_sqrd', 'month', 'ho...</td>\n",
       "      <td>{'train': {1: {'r2': 0.8271049782262871, 'rmse...</td>\n",
       "      <td>0.794030</td>\n",
       "      <td>...</td>\n",
       "      <td>3.828647</td>\n",
       "      <td>0.593359</td>\n",
       "      <td>3712.061289</td>\n",
       "      <td>2664.312118</td>\n",
       "      <td>3.842705</td>\n",
       "      <td>3.162945e+10</td>\n",
       "      <td>{\"r2\": 0.8006168219666828, \"rmse\": 3746.091056...</td>\n",
       "      <td>{\"pearson_r\": -0.002654555749724851, \"spearman...</td>\n",
       "      <td>{\"r2\": 0.6555052060512503, \"rmse\": 3732.103382...</td>\n",
       "      <td>{\"pearson_r\": -0.0019075507390846269, \"spearma...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>13b053372ad4d1d6601337e6d6bd236be66de3cff09dd1...</td>\n",
       "      <td>12</td>\n",
       "      <td>{\"Feature_Addition\":\"Pipeline(steps=[('Add_Dat...</td>\n",
       "      <td>2025-08-16T23:30:38.565267</td>\n",
       "      <td>GroupwiseRegressor</td>\n",
       "      <td>['Feature_Addition', 'Multi_Quantile_Binner', ...</td>\n",
       "      <td>3</td>\n",
       "      <td>['demand_met', 'demand_met_sqrd', 'month', 'ho...</td>\n",
       "      <td>{'train': {2: {'r2': 0.9091252456040023, 'rmse...</td>\n",
       "      <td>0.904949</td>\n",
       "      <td>...</td>\n",
       "      <td>3.164732</td>\n",
       "      <td>0.695585</td>\n",
       "      <td>2909.583009</td>\n",
       "      <td>2341.624912</td>\n",
       "      <td>3.175503</td>\n",
       "      <td>3.165294e+10</td>\n",
       "      <td>{\"r2\": 0.9094852785665317, \"rmse\": 2355.431764...</td>\n",
       "      <td>{\"pearson_r\": -0.0010488140073209547, \"spearma...</td>\n",
       "      <td>{\"r2\": 0.758011743909146, \"rmse\": 2935.3213851...</td>\n",
       "      <td>{\"pearson_r\": 0.01864741184605731, \"spearman_r...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 43 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       model_id_hash  random_state  \\\n",
       "0  690f498780e0712e252cc93bcfd4abacd65e1efdf3fd1a...            12   \n",
       "1  ff562dc468b7f0d2d7dadb624dec64eb545fdfe735279f...            12   \n",
       "2  13b053372ad4d1d6601337e6d6bd236be66de3cff09dd1...            12   \n",
       "\n",
       "                                         params_json  \\\n",
       "0  {\"Feature_Addition\":\"Pipeline(steps=[('Add_Dat...   \n",
       "1  {\"Feature_Addition\":\"Pipeline(steps=[('Add_Dat...   \n",
       "2  {\"Feature_Addition\":\"Pipeline(steps=[('Add_Dat...   \n",
       "\n",
       "                     log_time          model_name  \\\n",
       "0  2025-08-12T02:37:45.646345  GroupwiseRegressor   \n",
       "1  2025-08-12T02:38:45.678555  GroupwiseRegressor   \n",
       "2  2025-08-16T23:30:38.565267  GroupwiseRegressor   \n",
       "\n",
       "                                      pipeline_steps  pipeline_n_steps  \\\n",
       "0  ['Feature_Addition', 'Multi_Median_Binner', 'G...                 3   \n",
       "1  ['Feature_Addition', 'Multi_Median_Binner', 'G...                 3   \n",
       "2  ['Feature_Addition', 'Multi_Quantile_Binner', ...                 3   \n",
       "\n",
       "                                           x_columns  \\\n",
       "0  ['demand_met', 'demand_met_sqrd', 'month', 'ho...   \n",
       "1  ['demand_met', 'demand_met_sqrd', 'month', 'ho...   \n",
       "2  ['demand_met', 'demand_met_sqrd', 'month', 'ho...   \n",
       "\n",
       "                                    metrics_by_group  r2_train  ...  \\\n",
       "0  {'train': {1: {'r2': 0.7759440466267724, 'rmse...  0.777564  ...   \n",
       "1  {'train': {1: {'r2': 0.8271049782262871, 'rmse...  0.794030  ...   \n",
       "2  {'train': {2: {'r2': 0.9091252456040023, 'rmse...  0.904949  ...   \n",
       "\n",
       "   mape_validation_micro  r2_validation_energy_micro  \\\n",
       "0               3.762266                    0.651356   \n",
       "1               3.828647                    0.593359   \n",
       "2               3.164732                    0.695585   \n",
       "\n",
       "   rmse_validation_energy_micro  mae_validation_energy_micro  \\\n",
       "0                   3643.997308                  2596.004372   \n",
       "1                   3712.061289                  2664.312118   \n",
       "2                   2909.583009                  2341.624912   \n",
       "\n",
       "   mape_validation_energy_micro  energy_MWh_validation_total  \\\n",
       "0                      3.774859                 3.162945e+10   \n",
       "1                      3.842705                 3.162945e+10   \n",
       "2                      3.175503                 3.165294e+10   \n",
       "\n",
       "                                    pooled_co2_train  \\\n",
       "0  {\"r2\": 0.7792042909467476, \"rmse\": 3942.115886...   \n",
       "1  {\"r2\": 0.8006168219666828, \"rmse\": 3746.091056...   \n",
       "2  {\"r2\": 0.9094852785665317, \"rmse\": 2355.431764...   \n",
       "\n",
       "                                         fd_me_train  \\\n",
       "0  {\"pearson_r\": -0.0013750900776175363, \"spearma...   \n",
       "1  {\"pearson_r\": -0.002654555749724851, \"spearman...   \n",
       "2  {\"pearson_r\": -0.0010488140073209547, \"spearma...   \n",
       "\n",
       "                               pooled_co2_validation  \\\n",
       "0  {\"r2\": 0.6674819506158596, \"rmse\": 3666.654134...   \n",
       "1  {\"r2\": 0.6555052060512503, \"rmse\": 3732.103382...   \n",
       "2  {\"r2\": 0.758011743909146, \"rmse\": 2935.3213851...   \n",
       "\n",
       "                                    fd_me_validation  \n",
       "0  {\"pearson_r\": -0.0027290646933841333, \"spearma...  \n",
       "1  {\"pearson_r\": -0.0019075507390846269, \"spearma...  \n",
       "2  {\"pearson_r\": 0.01864741184605731, \"spearman_r...  \n",
       "\n",
       "[3 rows x 43 columns]"
      ]
     },
     "execution_count": 189,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_logs = load_all_logs_rotating_csv(\n",
    "    results_dir=marginal_emissions_logs_directory,\n",
    "    file_prefix=marginal_emissions_prefix,\n",
    ")\n",
    "all_logs.head(8)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03b24411",
   "metadata": {},
   "source": [
    "##### Medians"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "a3fc9455",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BINNING PIPELINES\n",
    "original_median_binner = MultiMedianBinner(\n",
    "    variables=[\n",
    "        \"surface_net_solar_radiation_kWh_per_m2\",\n",
    "        \"wind_speed_mps\"\n",
    "    ],\n",
    "    group_col_name=\"median_group_id\",\n",
    ")\n",
    "\n",
    "median_binner_v1 = MultiMedianBinner(\n",
    "    variables=[\n",
    "        \"surface_net_solar_radiation_kWh_per_m2\",\n",
    "        \"wind_speed_mps\",\n",
    "        \"temperature_celsius\",\n",
    "    ],\n",
    "    group_col_name=\"median_group_id\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "6d8e606e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# REGRESSORS\n",
    "original_median_regressor = GroupwiseRegressor(\n",
    "    y_var=\"tons_co2\",\n",
    "    x_vars=[\"demand_met\", \"demand_met_sqrd\"],\n",
    "    fe_vars=[\"month\", \"hour\"],\n",
    "    group_col=\"median_group_id\",\n",
    "    min_group_size=20,\n",
    "    track_metrics=True,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "median_regressor_v1 = GroupwiseRegressor(\n",
    "    y_var=\"tons_co2\",\n",
    "    x_vars=[\"demand_met\", \"demand_met_sqrd\"],\n",
    "    fe_vars=[\"month\", \"hour\", \"week_of_year\"],\n",
    "    group_col=\"median_group_id\",\n",
    "    min_group_size=20,\n",
    "    track_metrics=True,\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "5e93da08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# REGRESSOR PIPELINES\n",
    "original_median_regressor_pipeline = Pipeline([\n",
    "    (\"Feature_Addition\", feature_addition_pipeline),\n",
    "    (\"Multi_Median_Binner\", original_median_binner),\n",
    "    (\"Groupwise_Regressor\", original_median_regressor)\n",
    "])\n",
    "\n",
    "median_regressor_pipeline_v1 = Pipeline([\n",
    "    (\"Feature_Addition\", feature_addition_pipeline),\n",
    "    (\"Multi_Median_Binner\", median_binner_v1),\n",
    "    (\"Groupwise_Regressor\", median_regressor_v1)\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "65fc8bb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TESTING - note that this will run regardless - so\n",
    "# train_logs, x_cols_used_train, _ = run_regressor_model(original_median_regressor_pipeline, train_pdf_x_all, train_pdf_y, split_name=\"train\")\n",
    "# val_logs, x_cols_used_val, _ = run_regressor_model(original_median_regressor_pipeline, validation_pdf_x_all, validation_pdf_y, split_name=\"validation\")\n",
    "# test_logs, x_cols_used_test, _ = run_regressor_model(original_median_regressor_pipeline, test_pdf_x_all, test_pdf_y, split_name=\"test\")\n",
    "# summarise_metrics_logs(train_logs, val_logs=val_logs, test_logs, original_median_regressor_pipeline, x_cols_used_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "6ccc07a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LOG] 4 rows for split=train, model_id=690f498780e0712e252cc93bcfd4abacd65e1efdf3fd1a2cd607095eff5f0af2, random_state=12\n",
      "[LOG] 4 rows for split=validation, model_id=690f498780e0712e252cc93bcfd4abacd65e1efdf3fd1a2cd607095eff5f0af2, random_state=12\n",
      "[SAVE] Appended to /Users/Daniel/Desktop/IRP_WORK_UPDATED/irp-dbk24/code_and_analysis/data/marginal_emissions_development/logs/marginal_emissions_results.part000.csv, index updated.\n",
      "[LOG] 8 rows for split=train, model_id=ff562dc468b7f0d2d7dadb624dec64eb545fdfe735279f75661c855f018354f3, random_state=12\n",
      "[LOG] 8 rows for split=validation, model_id=ff562dc468b7f0d2d7dadb624dec64eb545fdfe735279f75661c855f018354f3, random_state=12\n",
      "[SAVE] Appended to /Users/Daniel/Desktop/IRP_WORK_UPDATED/irp-dbk24/code_and_analysis/data/marginal_emissions_development/logs/marginal_emissions_results.part000.csv, index updated.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model_id_hash</th>\n",
       "      <th>random_state</th>\n",
       "      <th>params_json</th>\n",
       "      <th>log_time</th>\n",
       "      <th>model_name</th>\n",
       "      <th>pipeline_steps</th>\n",
       "      <th>pipeline_n_steps</th>\n",
       "      <th>x_columns</th>\n",
       "      <th>metrics_by_group</th>\n",
       "      <th>r2_train</th>\n",
       "      <th>...</th>\n",
       "      <th>mape_validation_micro</th>\n",
       "      <th>r2_validation_energy_micro</th>\n",
       "      <th>rmse_validation_energy_micro</th>\n",
       "      <th>mae_validation_energy_micro</th>\n",
       "      <th>mape_validation_energy_micro</th>\n",
       "      <th>energy_MWh_validation_total</th>\n",
       "      <th>pooled_co2_train</th>\n",
       "      <th>fd_me_train</th>\n",
       "      <th>pooled_co2_validation</th>\n",
       "      <th>fd_me_validation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ff562dc468b7f0d2d7dadb624dec64eb545fdfe735279f...</td>\n",
       "      <td>12</td>\n",
       "      <td>{\"Feature_Addition\":\"Pipeline(steps=[('Add_Dat...</td>\n",
       "      <td>2025-08-16T23:32:20.812324</td>\n",
       "      <td>GroupwiseRegressor</td>\n",
       "      <td>[Feature_Addition, Multi_Median_Binner, Groupw...</td>\n",
       "      <td>3</td>\n",
       "      <td>[demand_met, demand_met_sqrd, month, hour, wee...</td>\n",
       "      <td>{'train': {1: {'r2': 0.9257890589417794, 'rmse...</td>\n",
       "      <td>0.92662</td>\n",
       "      <td>...</td>\n",
       "      <td>3.113843</td>\n",
       "      <td>0.712657</td>\n",
       "      <td>2837.511095</td>\n",
       "      <td>2325.352098</td>\n",
       "      <td>3.131266</td>\n",
       "      <td>3.165313e+10</td>\n",
       "      <td>{\"r2\": 0.9255345804892087, \"rmse\": 2136.427626...</td>\n",
       "      <td>{\"pearson_r\": -0.0014954966078752142, \"spearma...</td>\n",
       "      <td>{\"r2\": 0.7719652792738894, \"rmse\": 2849.449093...</td>\n",
       "      <td>{\"pearson_r\": 0.016084754569606532, \"spearman_...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows × 43 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       model_id_hash  random_state  \\\n",
       "0  ff562dc468b7f0d2d7dadb624dec64eb545fdfe735279f...            12   \n",
       "\n",
       "                                         params_json  \\\n",
       "0  {\"Feature_Addition\":\"Pipeline(steps=[('Add_Dat...   \n",
       "\n",
       "                     log_time          model_name  \\\n",
       "0  2025-08-16T23:32:20.812324  GroupwiseRegressor   \n",
       "\n",
       "                                      pipeline_steps  pipeline_n_steps  \\\n",
       "0  [Feature_Addition, Multi_Median_Binner, Groupw...                 3   \n",
       "\n",
       "                                           x_columns  \\\n",
       "0  [demand_met, demand_met_sqrd, month, hour, wee...   \n",
       "\n",
       "                                    metrics_by_group  r2_train  ...  \\\n",
       "0  {'train': {1: {'r2': 0.9257890589417794, 'rmse...   0.92662  ...   \n",
       "\n",
       "   mape_validation_micro  r2_validation_energy_micro  \\\n",
       "0               3.113843                    0.712657   \n",
       "\n",
       "   rmse_validation_energy_micro  mae_validation_energy_micro  \\\n",
       "0                   2837.511095                  2325.352098   \n",
       "\n",
       "   mape_validation_energy_micro  energy_MWh_validation_total  \\\n",
       "0                      3.131266                 3.165313e+10   \n",
       "\n",
       "                                    pooled_co2_train  \\\n",
       "0  {\"r2\": 0.9255345804892087, \"rmse\": 2136.427626...   \n",
       "\n",
       "                                         fd_me_train  \\\n",
       "0  {\"pearson_r\": -0.0014954966078752142, \"spearma...   \n",
       "\n",
       "                               pooled_co2_validation  \\\n",
       "0  {\"r2\": 0.7719652792738894, \"rmse\": 2849.449093...   \n",
       "\n",
       "                                    fd_me_validation  \n",
       "0  {\"pearson_r\": 0.016084754569606532, \"spearman_...  \n",
       "\n",
       "[1 rows x 43 columns]"
      ]
     },
     "execution_count": 194,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ORCHESTRATORS\n",
    "regressor_orchestrator(\n",
    "    user_pipeline=original_median_regressor_pipeline,\n",
    "    x_splits={\n",
    "        \"train\": train_pdf_x_all,\n",
    "        \"validation\": validation_pdf_x_all,\n",
    "        \"test\": test_pdf_x_all\n",
    "    },\n",
    "    y_splits={\n",
    "        \"train\": train_pdf_y,\n",
    "        \"validation\": validation_pdf_y,\n",
    "        \"test\": test_pdf_y\n",
    "    },\n",
    "    interval_hours=0.5,\n",
    "    random_state=12,\n",
    "    group_col_name=\"median_group_id\",\n",
    "    force_run=True,\n",
    "    force_overwrite=True,\n",
    "    compute_test=False,\n",
    "    results_dir=marginal_emissions_logs_directory,\n",
    "    file_prefix=marginal_emissions_prefix,\n",
    "    max_log_mb=95,\n",
    "    fsync=False      # set to True when running on HPC\n",
    ")\n",
    "\n",
    "regressor_orchestrator(\n",
    "    user_pipeline=median_regressor_pipeline_v1,\n",
    "    x_splits={\n",
    "        \"train\": train_pdf_x_all,\n",
    "        \"validation\": validation_pdf_x_all,\n",
    "        \"test\": test_pdf_x_all\n",
    "    },\n",
    "    y_splits={\n",
    "        \"train\": train_pdf_y,\n",
    "        \"validation\": validation_pdf_y,\n",
    "        \"test\": test_pdf_y\n",
    "    },\n",
    "    interval_hours=0.5,\n",
    "    random_state=12,\n",
    "    group_col_name=\"median_group_id\",\n",
    "    force_run=True,\n",
    "    force_overwrite=True,\n",
    "    compute_test=False,\n",
    "    results_dir=marginal_emissions_logs_directory,\n",
    "    file_prefix=marginal_emissions_prefix,\n",
    "    max_log_mb=95,\n",
    "    fsync=False      # set to True when running on HPC\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "553d1aa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "testing_output_file = os.path.join(marginal_emissions_results_directory, \"original_quantile_bins_marginal_emissions_timeseries.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "id": "e5a023b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (9, 35)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>statistic</th><th>timestamp</th><th>city</th><th>land_latitude</th><th>land_longitude</th><th>world_latitude</th><th>world_longitude</th><th>wind_speed_mps</th><th>wind_direction_meteorological</th><th>temperature_celsius</th><th>precipitation_mm</th><th>surface_net_solar_radiation_kWh_per_m2</th><th>surface_solar_radiation_downwards_kWh_per_m2</th><th>surface_net_solar_radiation_joules_per_m2</th><th>surface_solar_radiation_downwards_joules_per_m2</th><th>total_cloud_cover</th><th>high_cloud_cover</th><th>medium_cloud_cover</th><th>low_cloud_cover</th><th>distance_between_locations_meters</th><th>processing_operations_log</th><th>thermal_generation</th><th>gas_generation</th><th>hydro_generation</th><th>nuclear_generation</th><th>renewable_generation</th><th>total_generation</th><th>demand_met</th><th>non_renewable_generation</th><th>tons_co2</th><th>g_co2_per_kwh</th><th>tons_co2_per_mwh</th><th>wind_dir_cardinal_8</th><th>wind_dir_cardinal_16</th><th>wind_dir_cardinal_4</th></tr><tr><td>str</td><td>str</td><td>str</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>str</td><td>str</td><td>str</td></tr></thead><tbody><tr><td>&quot;count&quot;</td><td>&quot;3481920&quot;</td><td>&quot;3481920&quot;</td><td>3.48192e6</td><td>3.48192e6</td><td>3.48192e6</td><td>3.48192e6</td><td>3.48192e6</td><td>3.48192e6</td><td>3.48192e6</td><td>3.48192e6</td><td>3.48192e6</td><td>3.48192e6</td><td>3.48192e6</td><td>3.48192e6</td><td>3.48192e6</td><td>3.48192e6</td><td>3.48192e6</td><td>3.48192e6</td><td>3.48192e6</td><td>1.74096e6</td><td>3.48192e6</td><td>3.48192e6</td><td>3.48192e6</td><td>3.48192e6</td><td>3.48192e6</td><td>3.48192e6</td><td>3.48192e6</td><td>3.48192e6</td><td>3.48192e6</td><td>3.48192e6</td><td>3.48192e6</td><td>&quot;3481920&quot;</td><td>&quot;3481920&quot;</td><td>&quot;3481920&quot;</td></tr><tr><td>&quot;null_count&quot;</td><td>&quot;0&quot;</td><td>&quot;0&quot;</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>1.74096e6</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>&quot;0&quot;</td><td>&quot;0&quot;</td><td>&quot;0&quot;</td></tr><tr><td>&quot;mean&quot;</td><td>&quot;2023-03-17 23:45:00+05:30&quot;</td><td>null</td><td>25.393333</td><td>75.701111</td><td>25.393333</td><td>75.684448</td><td>2.63187</td><td>209.982468</td><td>25.316455</td><td>0.071264</td><td>82469.322585</td><td>98989.168512</td><td>326390.622348</td><td>390807.372846</td><td>0.359816</td><td>0.264307</td><td>0.152118</td><td>0.107959</td><td>0.036667</td><td>null</td><td>133038.493588</td><td>3488.657373</td><td>17616.983083</td><td>5208.884506</td><td>19337.656218</td><td>178684.081988</td><td>177287.744297</td><td>159353.018551</td><td>65601.301985</td><td>738.359913</td><td>0.73836</td><td>null</td><td>null</td><td>null</td></tr><tr><td>&quot;std&quot;</td><td>null</td><td>null</td><td>4.538596</td><td>1.969453</td><td>4.538596</td><td>1.936544</td><td>1.382159</td><td>109.031609</td><td>6.983781</td><td>0.309506</td><td>120344.556357</td><td>144109.410049</td><td>443361.492633</td><td>529310.34544</td><td>0.392301</td><td>0.381599</td><td>0.245868</td><td>0.22307</td><td>0.004713</td><td>null</td><td>17973.982837</td><td>1459.834657</td><td>8805.004409</td><td>789.721015</td><td>15436.03158</td><td>23615.234305</td><td>23535.203493</td><td>19941.80274</td><td>8839.633217</td><td>78.292944</td><td>0.078293</td><td>null</td><td>null</td><td>null</td></tr><tr><td>&quot;min&quot;</td><td>&quot;2021-01-01 00:00:00+05:30&quot;</td><td>null</td><td>18.5</td><td>72.77</td><td>18.5</td><td>72.800003</td><td>0.002416</td><td>0.000122</td><td>2.149261</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.03</td><td>null</td><td>75325.666667</td><td>109.483135</td><td>2516.0</td><td>2281.083333</td><td>0.0</td><td>106969.666667</td><td>105140.916667</td><td>95292.666667</td><td>37529.27105</td><td>483.357945</td><td>0.483358</td><td>&quot;E&quot;</td><td>&quot;E&quot;</td><td>&quot;E&quot;</td></tr><tr><td>&quot;25%&quot;</td><td>&quot;2022-02-08 00:00:00+05:30&quot;</td><td>null</td><td>19.2</td><td>72.97</td><td>19.200001</td><td>73.0</td><td>1.691567</td><td>108.210403</td><td>21.588165</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.030002</td><td>null</td><td>120050.666667</td><td>2438.416667</td><td>9761.166667</td><td>4668.166667</td><td>7047.833333</td><td>162359.083333</td><td>160999.75</td><td>145527.666667</td><td>59239.9249</td><td>685.649915</td><td>0.68565</td><td>null</td><td>null</td><td>null</td></tr><tr><td>&quot;50%&quot;</td><td>&quot;2023-03-18 00:00:00+05:30&quot;</td><td>null</td><td>28.5</td><td>76.94</td><td>28.5</td><td>76.900002</td><td>2.411643</td><td>246.204437</td><td>26.381775</td><td>0.0</td><td>0.307726</td><td>0.360172</td><td>12536.50589</td><td>15013.505001</td><td>0.175415</td><td>0.0</td><td>0.01825</td><td>0.0</td><td>0.039997</td><td>null</td><td>134184.5</td><td>3060.5</td><td>16623.25</td><td>5214.083333</td><td>13690.583333</td><td>179752.833333</td><td>178227.333325</td><td>159159.833333</td><td>66137.34125</td><td>740.288715</td><td>0.740289</td><td>null</td><td>null</td><td>null</td></tr><tr><td>&quot;75%&quot;</td><td>&quot;2024-04-23 23:30:00+05:30&quot;</td><td>null</td><td>28.7</td><td>77.14</td><td>28.700001</td><td>77.099998</td><td>3.309765</td><td>304.86676</td><td>29.635712</td><td>0.002179</td><td>157134.975143</td><td>189414.069345</td><td>649476.992699</td><td>779902.687006</td><td>0.776154</td><td>0.551407</td><td>0.21228</td><td>0.09198</td><td>0.04</td><td>null</td><td>145811.583333</td><td>4216.25</td><td>24520.083333</td><td>5757.75</td><td>28750.416667</td><td>196273.0</td><td>194806.833333</td><td>173136.916667</td><td>71813.8573</td><td>802.991573</td><td>0.802992</td><td>null</td><td>null</td><td>null</td></tr><tr><td>&quot;max&quot;</td><td>&quot;2025-05-31 23:30:00+05:30&quot;</td><td>null</td><td>28.8</td><td>77.34</td><td>28.799999</td><td>77.300003</td><td>19.006941</td><td>359.999786</td><td>46.374786</td><td>16.104477</td><td>465916.895282</td><td>530511.99554</td><td>1.6773e6</td><td>1.9098e6</td><td>1.0</td><td>1.0</td><td>1.0</td><td>1.0</td><td>0.040003</td><td>null</td><td>177508.083333</td><td>12915.75</td><td>43162.083333</td><td>7070.0</td><td>75828.666667</td><td>251763.666667</td><td>249591.916667</td><td>217778.833333</td><td>88742.75695</td><td>905.09159</td><td>0.905092</td><td>&quot;W&quot;</td><td>&quot;WSW&quot;</td><td>&quot;W&quot;</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (9, 35)\n",
       "┌────────────┬───────────┬─────────┬───────────┬───┬───────────┬───────────┬───────────┬───────────┐\n",
       "│ statistic  ┆ timestamp ┆ city    ┆ land_lati ┆ … ┆ tons_co2_ ┆ wind_dir_ ┆ wind_dir_ ┆ wind_dir_ │\n",
       "│ ---        ┆ ---       ┆ ---     ┆ tude      ┆   ┆ per_mwh   ┆ cardinal_ ┆ cardinal_ ┆ cardinal_ │\n",
       "│ str        ┆ str       ┆ str     ┆ ---       ┆   ┆ ---       ┆ 8         ┆ 16        ┆ 4         │\n",
       "│            ┆           ┆         ┆ f64       ┆   ┆ f64       ┆ ---       ┆ ---       ┆ ---       │\n",
       "│            ┆           ┆         ┆           ┆   ┆           ┆ str       ┆ str       ┆ str       │\n",
       "╞════════════╪═══════════╪═════════╪═══════════╪═══╪═══════════╪═══════════╪═══════════╪═══════════╡\n",
       "│ count      ┆ 3481920   ┆ 3481920 ┆ 3.48192e6 ┆ … ┆ 3.48192e6 ┆ 3481920   ┆ 3481920   ┆ 3481920   │\n",
       "│ null_count ┆ 0         ┆ 0       ┆ 0.0       ┆ … ┆ 0.0       ┆ 0         ┆ 0         ┆ 0         │\n",
       "│ mean       ┆ 2023-03-1 ┆ null    ┆ 25.393333 ┆ … ┆ 0.73836   ┆ null      ┆ null      ┆ null      │\n",
       "│            ┆ 7 23:45:0 ┆         ┆           ┆   ┆           ┆           ┆           ┆           │\n",
       "│            ┆ 0+05:30   ┆         ┆           ┆   ┆           ┆           ┆           ┆           │\n",
       "│ std        ┆ null      ┆ null    ┆ 4.538596  ┆ … ┆ 0.078293  ┆ null      ┆ null      ┆ null      │\n",
       "│ min        ┆ 2021-01-0 ┆ null    ┆ 18.5      ┆ … ┆ 0.483358  ┆ E         ┆ E         ┆ E         │\n",
       "│            ┆ 1 00:00:0 ┆         ┆           ┆   ┆           ┆           ┆           ┆           │\n",
       "│            ┆ 0+05:30   ┆         ┆           ┆   ┆           ┆           ┆           ┆           │\n",
       "│ 25%        ┆ 2022-02-0 ┆ null    ┆ 19.2      ┆ … ┆ 0.68565   ┆ null      ┆ null      ┆ null      │\n",
       "│            ┆ 8 00:00:0 ┆         ┆           ┆   ┆           ┆           ┆           ┆           │\n",
       "│            ┆ 0+05:30   ┆         ┆           ┆   ┆           ┆           ┆           ┆           │\n",
       "│ 50%        ┆ 2023-03-1 ┆ null    ┆ 28.5      ┆ … ┆ 0.740289  ┆ null      ┆ null      ┆ null      │\n",
       "│            ┆ 8 00:00:0 ┆         ┆           ┆   ┆           ┆           ┆           ┆           │\n",
       "│            ┆ 0+05:30   ┆         ┆           ┆   ┆           ┆           ┆           ┆           │\n",
       "│ 75%        ┆ 2024-04-2 ┆ null    ┆ 28.7      ┆ … ┆ 0.802992  ┆ null      ┆ null      ┆ null      │\n",
       "│            ┆ 3 23:30:0 ┆         ┆           ┆   ┆           ┆           ┆           ┆           │\n",
       "│            ┆ 0+05:30   ┆         ┆           ┆   ┆           ┆           ┆           ┆           │\n",
       "│ max        ┆ 2025-05-3 ┆ null    ┆ 28.8      ┆ … ┆ 0.905092  ┆ W         ┆ WSW       ┆ W         │\n",
       "│            ┆ 1 23:30:0 ┆         ┆           ┆   ┆           ┆           ┆           ┆           │\n",
       "│            ┆ 0+05:30   ┆         ┆           ┆   ┆           ┆           ┆           ┆           │\n",
       "└────────────┴───────────┴─────────┴───────────┴───┴───────────┴───────────┴───────────┴───────────┘"
      ]
     },
     "execution_count": 221,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base_pldf.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "id": "d4894bf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_df = base_pldf.to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "id": "2163d7ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_df_x = full_df.drop(columns=[\"tons_co2\"])\n",
    "full_df_y = full_df[\"tons_co2\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "id": "cfd54380",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Skipping group 15: only 1 < 20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SAVE] Wrote marginal emissions to data/marginal_emissions_development/results/testing_marginal_emissions_timeseries.parquet (rows=3,481,920)\n"
     ]
    }
   ],
   "source": [
    "me_out = fit_and_export_marginal_emissions_full(\n",
    "    pipeline=original_multi_binner_regressor_pipeline,\n",
    "    X_full=full_df_x,\n",
    "    y_full=full_df_y,\n",
    "    out_parquet_path=testing_output_file,\n",
    "    id_cols=[\"timestamp\", \"city\", \"land_longitude\", \"land_latitude\"],            # include any other IDs you care about\n",
    "    include_params=True,                      # include alpha1/alpha2 for auditability\n",
    "    keep_cols=[\"demand_met\", \"tons_co2\"],     # optional handy columns\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "id": "a217e414",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The type of the [me_out] DataFrame is: <class 'pandas.core.frame.DataFrame'>\n",
      "The type of the [me_out_pldf] DataFrame is: <class 'polars.dataframe.frame.DataFrame'>\n"
     ]
    }
   ],
   "source": [
    "print(f\"The type of the [me_out] DataFrame is: {type(me_out)}\")\n",
    "# convert to polars dataframe\n",
    "me_out_pldf = pl.DataFrame(me_out)\n",
    "print(f\"The type of the [me_out_pldf] DataFrame is: {type(me_out_pldf)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "id": "da63d4bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (10, 9)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>timestamp</th><th>city</th><th>land_longitude</th><th>land_latitude</th><th>ME</th><th>alpha1</th><th>alpha2</th><th>original_quantile_group_id</th><th>demand_met</th></tr><tr><td>datetime[μs, Asia/Kolkata]</td><td>cat</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>i64</td><td>f64</td></tr></thead><tbody><tr><td>2024-06-11 08:00:00 IST</td><td>&quot;delhi&quot;</td><td>76.84</td><td>28.4</td><td>0.281945</td><td>1.026967</td><td>-0.000002</td><td>33</td><td>202137.583333</td></tr><tr><td>2023-10-14 14:00:00 IST</td><td>&quot;delhi&quot;</td><td>76.94</td><td>28.8</td><td>0.245039</td><td>0.798551</td><td>-0.000001</td><td>19</td><td>211772.75</td></tr><tr><td>2021-11-22 19:00:00 IST</td><td>&quot;mumbai&quot;</td><td>72.97</td><td>19.1</td><td>0.433756</td><td>0.805138</td><td>-0.000001</td><td>3</td><td>162786.25</td></tr><tr><td>2023-01-27 10:30:00 IST</td><td>&quot;delhi&quot;</td><td>76.94</td><td>28.5</td><td>0.291099</td><td>0.757302</td><td>-0.000001</td><td>20</td><td>202182.25</td></tr><tr><td>2023-06-19 19:30:00 IST</td><td>&quot;delhi&quot;</td><td>77.14</td><td>28.7</td><td>0.35504</td><td>0.805138</td><td>-0.000001</td><td>3</td><td>197289.666667</td></tr><tr><td>2022-07-17 21:00:00 IST</td><td>&quot;mumbai&quot;</td><td>72.97</td><td>18.9</td><td>0.398066</td><td>0.682504</td><td>-8.1658e-7</td><td>6</td><td>174163.833333</td></tr><tr><td>2021-11-06 03:00:00 IST</td><td>&quot;delhi&quot;</td><td>76.84</td><td>28.8</td><td>0.509305</td><td>0.798551</td><td>-0.000001</td><td>19</td><td>110665.0</td></tr><tr><td>2022-09-03 16:30:00 IST</td><td>&quot;delhi&quot;</td><td>77.04</td><td>28.8</td><td>0.363809</td><td>0.805138</td><td>-0.000001</td><td>3</td><td>193446.0</td></tr><tr><td>2021-11-22 04:30:00 IST</td><td>&quot;delhi&quot;</td><td>76.84</td><td>28.4</td><td>0.566409</td><td>0.975689</td><td>-0.000002</td><td>26</td><td>114676.25</td></tr><tr><td>2023-05-31 19:00:00 IST</td><td>&quot;delhi&quot;</td><td>76.84</td><td>28.7</td><td>0.389098</td><td>0.784999</td><td>-0.000001</td><td>2</td><td>183418.25</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (10, 9)\n",
       "┌────────────┬────────┬────────────┬────────────┬───┬──────────┬───────────┬───────────┬───────────┐\n",
       "│ timestamp  ┆ city   ┆ land_longi ┆ land_latit ┆ … ┆ alpha1   ┆ alpha2    ┆ original_ ┆ demand_me │\n",
       "│ ---        ┆ ---    ┆ tude       ┆ ude        ┆   ┆ ---      ┆ ---       ┆ quantile_ ┆ t         │\n",
       "│ datetime[μ ┆ cat    ┆ ---        ┆ ---        ┆   ┆ f64      ┆ f64       ┆ group_id  ┆ ---       │\n",
       "│ s, Asia/Ko ┆        ┆ f64        ┆ f64        ┆   ┆          ┆           ┆ ---       ┆ f64       │\n",
       "│ lkata]     ┆        ┆            ┆            ┆   ┆          ┆           ┆ i64       ┆           │\n",
       "╞════════════╪════════╪════════════╪════════════╪═══╪══════════╪═══════════╪═══════════╪═══════════╡\n",
       "│ 2024-06-11 ┆ delhi  ┆ 76.84      ┆ 28.4       ┆ … ┆ 1.026967 ┆ -0.000002 ┆ 33        ┆ 202137.58 │\n",
       "│ 08:00:00   ┆        ┆            ┆            ┆   ┆          ┆           ┆           ┆ 3333      │\n",
       "│ IST        ┆        ┆            ┆            ┆   ┆          ┆           ┆           ┆           │\n",
       "│ 2023-10-14 ┆ delhi  ┆ 76.94      ┆ 28.8       ┆ … ┆ 0.798551 ┆ -0.000001 ┆ 19        ┆ 211772.75 │\n",
       "│ 14:00:00   ┆        ┆            ┆            ┆   ┆          ┆           ┆           ┆           │\n",
       "│ IST        ┆        ┆            ┆            ┆   ┆          ┆           ┆           ┆           │\n",
       "│ 2021-11-22 ┆ mumbai ┆ 72.97      ┆ 19.1       ┆ … ┆ 0.805138 ┆ -0.000001 ┆ 3         ┆ 162786.25 │\n",
       "│ 19:00:00   ┆        ┆            ┆            ┆   ┆          ┆           ┆           ┆           │\n",
       "│ IST        ┆        ┆            ┆            ┆   ┆          ┆           ┆           ┆           │\n",
       "│ 2023-01-27 ┆ delhi  ┆ 76.94      ┆ 28.5       ┆ … ┆ 0.757302 ┆ -0.000001 ┆ 20        ┆ 202182.25 │\n",
       "│ 10:30:00   ┆        ┆            ┆            ┆   ┆          ┆           ┆           ┆           │\n",
       "│ IST        ┆        ┆            ┆            ┆   ┆          ┆           ┆           ┆           │\n",
       "│ 2023-06-19 ┆ delhi  ┆ 77.14      ┆ 28.7       ┆ … ┆ 0.805138 ┆ -0.000001 ┆ 3         ┆ 197289.66 │\n",
       "│ 19:30:00   ┆        ┆            ┆            ┆   ┆          ┆           ┆           ┆ 6667      │\n",
       "│ IST        ┆        ┆            ┆            ┆   ┆          ┆           ┆           ┆           │\n",
       "│ 2022-07-17 ┆ mumbai ┆ 72.97      ┆ 18.9       ┆ … ┆ 0.682504 ┆ -8.1658e- ┆ 6         ┆ 174163.83 │\n",
       "│ 21:00:00   ┆        ┆            ┆            ┆   ┆          ┆ 7         ┆           ┆ 3333      │\n",
       "│ IST        ┆        ┆            ┆            ┆   ┆          ┆           ┆           ┆           │\n",
       "│ 2021-11-06 ┆ delhi  ┆ 76.84      ┆ 28.8       ┆ … ┆ 0.798551 ┆ -0.000001 ┆ 19        ┆ 110665.0  │\n",
       "│ 03:00:00   ┆        ┆            ┆            ┆   ┆          ┆           ┆           ┆           │\n",
       "│ IST        ┆        ┆            ┆            ┆   ┆          ┆           ┆           ┆           │\n",
       "│ 2022-09-03 ┆ delhi  ┆ 77.04      ┆ 28.8       ┆ … ┆ 0.805138 ┆ -0.000001 ┆ 3         ┆ 193446.0  │\n",
       "│ 16:30:00   ┆        ┆            ┆            ┆   ┆          ┆           ┆           ┆           │\n",
       "│ IST        ┆        ┆            ┆            ┆   ┆          ┆           ┆           ┆           │\n",
       "│ 2021-11-22 ┆ delhi  ┆ 76.84      ┆ 28.4       ┆ … ┆ 0.975689 ┆ -0.000002 ┆ 26        ┆ 114676.25 │\n",
       "│ 04:30:00   ┆        ┆            ┆            ┆   ┆          ┆           ┆           ┆           │\n",
       "│ IST        ┆        ┆            ┆            ┆   ┆          ┆           ┆           ┆           │\n",
       "│ 2023-05-31 ┆ delhi  ┆ 76.84      ┆ 28.7       ┆ … ┆ 0.784999 ┆ -0.000001 ┆ 2         ┆ 183418.25 │\n",
       "│ 19:00:00   ┆        ┆            ┆            ┆   ┆          ┆           ┆           ┆           │\n",
       "│ IST        ┆        ┆            ┆            ┆   ┆          ┆           ┆           ┆           │\n",
       "└────────────┴────────┴────────────┴────────────┴───┴──────────┴───────────┴───────────┴───────────┘"
      ]
     },
     "execution_count": 209,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "me_out_pldf.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "id": "94fd121b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (9, 10)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>statistic</th><th>timestamp</th><th>city</th><th>land_longitude</th><th>land_latitude</th><th>ME</th><th>alpha1</th><th>alpha2</th><th>original_quantile_group_id</th><th>demand_met</th></tr><tr><td>str</td><td>str</td><td>str</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td></tr></thead><tbody><tr><td>&quot;count&quot;</td><td>&quot;3481920&quot;</td><td>&quot;3481920&quot;</td><td>3.48192e6</td><td>3.48192e6</td><td>3.481919e6</td><td>3.481919e6</td><td>3.481919e6</td><td>3.48192e6</td><td>3.48192e6</td></tr><tr><td>&quot;null_count&quot;</td><td>&quot;0&quot;</td><td>&quot;0&quot;</td><td>0.0</td><td>0.0</td><td>1.0</td><td>1.0</td><td>1.0</td><td>0.0</td><td>0.0</td></tr><tr><td>&quot;mean&quot;</td><td>&quot;2023-03-17 23:45:00+05:30&quot;</td><td>null</td><td>75.701111</td><td>25.393333</td><td>0.373358</td><td>0.887762</td><td>-0.000001</td><td>16.530667</td><td>177287.744297</td></tr><tr><td>&quot;std&quot;</td><td>null</td><td>null</td><td>1.969453</td><td>4.538596</td><td>0.073292</td><td>0.112286</td><td>3.4511e-7</td><td>11.399828</td><td>23535.203493</td></tr><tr><td>&quot;min&quot;</td><td>&quot;2021-01-01 00:00:00+05:30&quot;</td><td>null</td><td>72.77</td><td>18.5</td><td>0.101938</td><td>0.682504</td><td>-0.000002</td><td>2.0</td><td>105140.916667</td></tr><tr><td>&quot;25%&quot;</td><td>&quot;2022-02-08 00:00:00+05:30&quot;</td><td>null</td><td>72.97</td><td>19.2</td><td>0.326649</td><td>0.798551</td><td>-0.000002</td><td>4.0</td><td>160999.75</td></tr><tr><td>&quot;50%&quot;</td><td>&quot;2023-03-18 00:00:00+05:30&quot;</td><td>null</td><td>76.94</td><td>28.5</td><td>0.373625</td><td>0.869218</td><td>-0.000001</td><td>17.0</td><td>178227.333325</td></tr><tr><td>&quot;75%&quot;</td><td>&quot;2024-04-23 23:30:00+05:30&quot;</td><td>null</td><td>77.14</td><td>28.7</td><td>0.423545</td><td>1.009042</td><td>-0.000001</td><td>26.0</td><td>194806.833333</td></tr><tr><td>&quot;max&quot;</td><td>&quot;2025-05-31 23:30:00+05:30&quot;</td><td>null</td><td>77.34</td><td>28.8</td><td>0.635273</td><td>1.06163</td><td>-8.1658e-7</td><td>34.0</td><td>249591.916667</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (9, 10)\n",
       "┌────────────┬───────────┬─────────┬───────────┬───┬───────────┬───────────┬───────────┬───────────┐\n",
       "│ statistic  ┆ timestamp ┆ city    ┆ land_long ┆ … ┆ alpha1    ┆ alpha2    ┆ original_ ┆ demand_me │\n",
       "│ ---        ┆ ---       ┆ ---     ┆ itude     ┆   ┆ ---       ┆ ---       ┆ quantile_ ┆ t         │\n",
       "│ str        ┆ str       ┆ str     ┆ ---       ┆   ┆ f64       ┆ f64       ┆ group_id  ┆ ---       │\n",
       "│            ┆           ┆         ┆ f64       ┆   ┆           ┆           ┆ ---       ┆ f64       │\n",
       "│            ┆           ┆         ┆           ┆   ┆           ┆           ┆ f64       ┆           │\n",
       "╞════════════╪═══════════╪═════════╪═══════════╪═══╪═══════════╪═══════════╪═══════════╪═══════════╡\n",
       "│ count      ┆ 3481920   ┆ 3481920 ┆ 3.48192e6 ┆ … ┆ 3.481919e ┆ 3.481919e ┆ 3.48192e6 ┆ 3.48192e6 │\n",
       "│            ┆           ┆         ┆           ┆   ┆ 6         ┆ 6         ┆           ┆           │\n",
       "│ null_count ┆ 0         ┆ 0       ┆ 0.0       ┆ … ┆ 1.0       ┆ 1.0       ┆ 0.0       ┆ 0.0       │\n",
       "│ mean       ┆ 2023-03-1 ┆ null    ┆ 75.701111 ┆ … ┆ 0.887762  ┆ -0.000001 ┆ 16.530667 ┆ 177287.74 │\n",
       "│            ┆ 7 23:45:0 ┆         ┆           ┆   ┆           ┆           ┆           ┆ 4297      │\n",
       "│            ┆ 0+05:30   ┆         ┆           ┆   ┆           ┆           ┆           ┆           │\n",
       "│ std        ┆ null      ┆ null    ┆ 1.969453  ┆ … ┆ 0.112286  ┆ 3.4511e-7 ┆ 11.399828 ┆ 23535.203 │\n",
       "│            ┆           ┆         ┆           ┆   ┆           ┆           ┆           ┆ 493       │\n",
       "│ min        ┆ 2021-01-0 ┆ null    ┆ 72.77     ┆ … ┆ 0.682504  ┆ -0.000002 ┆ 2.0       ┆ 105140.91 │\n",
       "│            ┆ 1 00:00:0 ┆         ┆           ┆   ┆           ┆           ┆           ┆ 6667      │\n",
       "│            ┆ 0+05:30   ┆         ┆           ┆   ┆           ┆           ┆           ┆           │\n",
       "│ 25%        ┆ 2022-02-0 ┆ null    ┆ 72.97     ┆ … ┆ 0.798551  ┆ -0.000002 ┆ 4.0       ┆ 160999.75 │\n",
       "│            ┆ 8 00:00:0 ┆         ┆           ┆   ┆           ┆           ┆           ┆           │\n",
       "│            ┆ 0+05:30   ┆         ┆           ┆   ┆           ┆           ┆           ┆           │\n",
       "│ 50%        ┆ 2023-03-1 ┆ null    ┆ 76.94     ┆ … ┆ 0.869218  ┆ -0.000001 ┆ 17.0      ┆ 178227.33 │\n",
       "│            ┆ 8 00:00:0 ┆         ┆           ┆   ┆           ┆           ┆           ┆ 3325      │\n",
       "│            ┆ 0+05:30   ┆         ┆           ┆   ┆           ┆           ┆           ┆           │\n",
       "│ 75%        ┆ 2024-04-2 ┆ null    ┆ 77.14     ┆ … ┆ 1.009042  ┆ -0.000001 ┆ 26.0      ┆ 194806.83 │\n",
       "│            ┆ 3 23:30:0 ┆         ┆           ┆   ┆           ┆           ┆           ┆ 3333      │\n",
       "│            ┆ 0+05:30   ┆         ┆           ┆   ┆           ┆           ┆           ┆           │\n",
       "│ max        ┆ 2025-05-3 ┆ null    ┆ 77.34     ┆ … ┆ 1.06163   ┆ -8.1658e- ┆ 34.0      ┆ 249591.91 │\n",
       "│            ┆ 1 23:30:0 ┆         ┆           ┆   ┆           ┆ 7         ┆           ┆ 6667      │\n",
       "│            ┆ 0+05:30   ┆         ┆           ┆   ┆           ┆           ┆           ┆           │\n",
       "└────────────┴───────────┴─────────┴───────────┴───┴───────────┴───────────┴───────────┴───────────┘"
      ]
     },
     "execution_count": 210,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "me_out_pldf.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "id": "8ee400f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (1, 9)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>timestamp</th><th>city</th><th>land_longitude</th><th>land_latitude</th><th>ME</th><th>alpha1</th><th>alpha2</th><th>original_quantile_group_id</th><th>demand_met</th></tr><tr><td>datetime[μs, Asia/Kolkata]</td><td>cat</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>i64</td><td>f64</td></tr></thead><tbody><tr><td>2024-05-31 02:00:00 IST</td><td>&quot;mumbai&quot;</td><td>72.87</td><td>19.3</td><td>null</td><td>null</td><td>null</td><td>15</td><td>218235.916667</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (1, 9)\n",
       "┌─────────────┬────────┬─────────────┬─────────────┬───┬────────┬────────┬────────────┬────────────┐\n",
       "│ timestamp   ┆ city   ┆ land_longit ┆ land_latitu ┆ … ┆ alpha1 ┆ alpha2 ┆ original_q ┆ demand_met │\n",
       "│ ---         ┆ ---    ┆ ude         ┆ de          ┆   ┆ ---    ┆ ---    ┆ uantile_gr ┆ ---        │\n",
       "│ datetime[μs ┆ cat    ┆ ---         ┆ ---         ┆   ┆ f64    ┆ f64    ┆ oup_id     ┆ f64        │\n",
       "│ , Asia/Kolk ┆        ┆ f64         ┆ f64         ┆   ┆        ┆        ┆ ---        ┆            │\n",
       "│ ata]        ┆        ┆             ┆             ┆   ┆        ┆        ┆ i64        ┆            │\n",
       "╞═════════════╪════════╪═════════════╪═════════════╪═══╪════════╪════════╪════════════╪════════════╡\n",
       "│ 2024-05-31  ┆ mumbai ┆ 72.87       ┆ 19.3        ┆ … ┆ null   ┆ null   ┆ 15         ┆ 218235.916 │\n",
       "│ 02:00:00    ┆        ┆             ┆             ┆   ┆        ┆        ┆            ┆ 667        │\n",
       "│ IST         ┆        ┆             ┆             ┆   ┆        ┆        ┆            ┆            │\n",
       "└─────────────┴────────┴─────────────┴─────────────┴───┴────────┴────────┴────────────┴────────────┘"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(me_out_pldf.filter(\n",
    "    pl.col(\"ME\").is_null()\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "id": "0e284520",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (10, 9)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>timestamp</th><th>city</th><th>land_longitude</th><th>land_latitude</th><th>ME</th><th>alpha1</th><th>alpha2</th><th>original_quantile_group_id</th><th>demand_met</th></tr><tr><td>datetime[μs, Asia/Kolkata]</td><td>cat</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>i64</td><td>f64</td></tr></thead><tbody><tr><td>2024-05-31 00:30:00 IST</td><td>&quot;mumbai&quot;</td><td>72.87</td><td>19.3</td><td>0.276165</td><td>0.784577</td><td>-0.000001</td><td>9</td><td>226588.5</td></tr><tr><td>2024-05-31 01:00:00 IST</td><td>&quot;mumbai&quot;</td><td>72.87</td><td>19.3</td><td>0.258134</td><td>0.796921</td><td>-0.000001</td><td>16</td><td>223699.916667</td></tr><tr><td>2024-05-31 01:30:00 IST</td><td>&quot;mumbai&quot;</td><td>72.87</td><td>19.3</td><td>0.265286</td><td>0.796921</td><td>-0.000001</td><td>16</td><td>220730.75</td></tr><tr><td>2024-05-31 02:00:00 IST</td><td>&quot;mumbai&quot;</td><td>72.87</td><td>19.3</td><td>null</td><td>null</td><td>null</td><td>15</td><td>218235.916667</td></tr><tr><td>2024-05-31 02:30:00 IST</td><td>&quot;mumbai&quot;</td><td>72.87</td><td>19.3</td><td>0.215375</td><td>1.024058</td><td>-0.000002</td><td>23</td><td>216169.333333</td></tr><tr><td>2024-05-31 03:00:00 IST</td><td>&quot;mumbai&quot;</td><td>72.87</td><td>19.3</td><td>0.222551</td><td>1.024058</td><td>-0.000002</td><td>23</td><td>214251.166667</td></tr><tr><td>2024-05-31 03:30:00 IST</td><td>&quot;mumbai&quot;</td><td>72.87</td><td>19.3</td><td>0.22889</td><td>1.024058</td><td>-0.000002</td><td>23</td><td>212556.583333</td></tr><tr><td>2024-05-31 04:00:00 IST</td><td>&quot;mumbai&quot;</td><td>72.87</td><td>19.3</td><td>0.235012</td><td>1.024058</td><td>-0.000002</td><td>23</td><td>210920.083333</td></tr><tr><td>2024-05-31 04:30:00 IST</td><td>&quot;mumbai&quot;</td><td>72.87</td><td>19.3</td><td>0.241282</td><td>0.991575</td><td>-0.000002</td><td>24</td><td>210233.916667</td></tr><tr><td>2024-05-31 05:00:00 IST</td><td>&quot;mumbai&quot;</td><td>72.87</td><td>19.3</td><td>0.256864</td><td>1.055478</td><td>-0.000002</td><td>32</td><td>210322.416667</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (10, 9)\n",
       "┌────────────┬────────┬────────────┬────────────┬───┬──────────┬───────────┬───────────┬───────────┐\n",
       "│ timestamp  ┆ city   ┆ land_longi ┆ land_latit ┆ … ┆ alpha1   ┆ alpha2    ┆ original_ ┆ demand_me │\n",
       "│ ---        ┆ ---    ┆ tude       ┆ ude        ┆   ┆ ---      ┆ ---       ┆ quantile_ ┆ t         │\n",
       "│ datetime[μ ┆ cat    ┆ ---        ┆ ---        ┆   ┆ f64      ┆ f64       ┆ group_id  ┆ ---       │\n",
       "│ s, Asia/Ko ┆        ┆ f64        ┆ f64        ┆   ┆          ┆           ┆ ---       ┆ f64       │\n",
       "│ lkata]     ┆        ┆            ┆            ┆   ┆          ┆           ┆ i64       ┆           │\n",
       "╞════════════╪════════╪════════════╪════════════╪═══╪══════════╪═══════════╪═══════════╪═══════════╡\n",
       "│ 2024-05-31 ┆ mumbai ┆ 72.87      ┆ 19.3       ┆ … ┆ 0.784577 ┆ -0.000001 ┆ 9         ┆ 226588.5  │\n",
       "│ 00:30:00   ┆        ┆            ┆            ┆   ┆          ┆           ┆           ┆           │\n",
       "│ IST        ┆        ┆            ┆            ┆   ┆          ┆           ┆           ┆           │\n",
       "│ 2024-05-31 ┆ mumbai ┆ 72.87      ┆ 19.3       ┆ … ┆ 0.796921 ┆ -0.000001 ┆ 16        ┆ 223699.91 │\n",
       "│ 01:00:00   ┆        ┆            ┆            ┆   ┆          ┆           ┆           ┆ 6667      │\n",
       "│ IST        ┆        ┆            ┆            ┆   ┆          ┆           ┆           ┆           │\n",
       "│ 2024-05-31 ┆ mumbai ┆ 72.87      ┆ 19.3       ┆ … ┆ 0.796921 ┆ -0.000001 ┆ 16        ┆ 220730.75 │\n",
       "│ 01:30:00   ┆        ┆            ┆            ┆   ┆          ┆           ┆           ┆           │\n",
       "│ IST        ┆        ┆            ┆            ┆   ┆          ┆           ┆           ┆           │\n",
       "│ 2024-05-31 ┆ mumbai ┆ 72.87      ┆ 19.3       ┆ … ┆ null     ┆ null      ┆ 15        ┆ 218235.91 │\n",
       "│ 02:00:00   ┆        ┆            ┆            ┆   ┆          ┆           ┆           ┆ 6667      │\n",
       "│ IST        ┆        ┆            ┆            ┆   ┆          ┆           ┆           ┆           │\n",
       "│ 2024-05-31 ┆ mumbai ┆ 72.87      ┆ 19.3       ┆ … ┆ 1.024058 ┆ -0.000002 ┆ 23        ┆ 216169.33 │\n",
       "│ 02:30:00   ┆        ┆            ┆            ┆   ┆          ┆           ┆           ┆ 3333      │\n",
       "│ IST        ┆        ┆            ┆            ┆   ┆          ┆           ┆           ┆           │\n",
       "│ 2024-05-31 ┆ mumbai ┆ 72.87      ┆ 19.3       ┆ … ┆ 1.024058 ┆ -0.000002 ┆ 23        ┆ 214251.16 │\n",
       "│ 03:00:00   ┆        ┆            ┆            ┆   ┆          ┆           ┆           ┆ 6667      │\n",
       "│ IST        ┆        ┆            ┆            ┆   ┆          ┆           ┆           ┆           │\n",
       "│ 2024-05-31 ┆ mumbai ┆ 72.87      ┆ 19.3       ┆ … ┆ 1.024058 ┆ -0.000002 ┆ 23        ┆ 212556.58 │\n",
       "│ 03:30:00   ┆        ┆            ┆            ┆   ┆          ┆           ┆           ┆ 3333      │\n",
       "│ IST        ┆        ┆            ┆            ┆   ┆          ┆           ┆           ┆           │\n",
       "│ 2024-05-31 ┆ mumbai ┆ 72.87      ┆ 19.3       ┆ … ┆ 1.024058 ┆ -0.000002 ┆ 23        ┆ 210920.08 │\n",
       "│ 04:00:00   ┆        ┆            ┆            ┆   ┆          ┆           ┆           ┆ 3333      │\n",
       "│ IST        ┆        ┆            ┆            ┆   ┆          ┆           ┆           ┆           │\n",
       "│ 2024-05-31 ┆ mumbai ┆ 72.87      ┆ 19.3       ┆ … ┆ 0.991575 ┆ -0.000002 ┆ 24        ┆ 210233.91 │\n",
       "│ 04:30:00   ┆        ┆            ┆            ┆   ┆          ┆           ┆           ┆ 6667      │\n",
       "│ IST        ┆        ┆            ┆            ┆   ┆          ┆           ┆           ┆           │\n",
       "│ 2024-05-31 ┆ mumbai ┆ 72.87      ┆ 19.3       ┆ … ┆ 1.055478 ┆ -0.000002 ┆ 32        ┆ 210322.41 │\n",
       "│ 05:00:00   ┆        ┆            ┆            ┆   ┆          ┆           ┆           ┆ 6667      │\n",
       "│ IST        ┆        ┆            ┆            ┆   ┆          ┆           ┆           ┆           │\n",
       "└────────────┴────────┴────────────┴────────────┴───┴──────────┴───────────┴───────────┴───────────┘"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(me_out_pldf.filter(\n",
    "    pl.col(\"timestamp\") > datetime(2024,5,31,0,0, tzinfo=ZoneInfo(\"Asia/Kolkata\")),\n",
    "    pl.col(\"land_longitude\").round(2) == 72.87,\n",
    "    pl.col(\"land_latitude\").round(2) == 19.30\n",
    "    ).head(10)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "54ead4da",
   "metadata": {},
   "outputs": [],
   "source": [
    "group_cols = [\"land_longitude\", \"land_latitude\"]  # exact location match\n",
    "time_col = \"timestamp\"\n",
    "window_min = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "id": "87ae7396",
   "metadata": {},
   "outputs": [],
   "source": [
    "group_cols = [\"land_longitude\", \"land_latitude\"]  # exact location match\n",
    "time_col = \"timestamp\"\n",
    "window_min = 30\n",
    "window = pl.duration(minutes=window_min)  # <-- compare durations to durations\n",
    "\n",
    "# Only interpolate the columns that should be continuous\n",
    "cols_to_fill = [\"ME\", \"alpha1\", \"alpha2\",]\n",
    "\n",
    "me_out_pldf_sorted = me_out_pldf.sort(group_cols + [time_col])\n",
    "\n",
    "def fill_col(c: str) -> pl.Expr:\n",
    "    # nearest non-null timestamps/values within each location\n",
    "    prev_ts  = (\n",
    "        pl.when(pl.col(c).is_not_null()).then(pl.col(time_col)).otherwise(None)\n",
    "        .forward_fill()\n",
    "        .over(group_cols)\n",
    "    )\n",
    "    next_ts  = (\n",
    "        pl.when(pl.col(c).is_not_null()).then(pl.col(time_col)).otherwise(None)\n",
    "        .backward_fill()\n",
    "        .over(group_cols)\n",
    "    )\n",
    "\n",
    "    prev_val = pl.col(c).forward_fill().over(group_cols)\n",
    "    next_val = pl.col(c).backward_fill().over(group_cols)\n",
    "\n",
    "    # linear interpolation (cast to float for safety)\n",
    "    interp   = pl.col(c).cast(pl.Float64).interpolate().over(group_cols)\n",
    "\n",
    "    # only fill if the gap on each side is within the window\n",
    "    ok_prev = ((pl.col(time_col) - prev_ts) <= window).fill_null(False)\n",
    "    ok_next = ((next_ts - pl.col(time_col)) <= window).fill_null(False)\n",
    "\n",
    "    return (\n",
    "        pl.when(pl.col(c).is_null() & ok_prev & ok_next).then(interp)     # interior gap -> linear\n",
    "         .when(pl.col(c).is_null() & ok_prev & ~ok_next).then(prev_val)  # edge gap -> forward-fill\n",
    "         .when(pl.col(c).is_null() & ~ok_prev & ok_next).then(next_val)  # edge gap -> backward-fill\n",
    "         .otherwise(pl.col(c))\n",
    "         .alias(c)\n",
    "    )\n",
    "\n",
    "me_out_pldf_filled = me_out_pldf_sorted.with_columns([fill_col(c) for c in cols_to_fill])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "id": "e349ce67",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (9, 10)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>statistic</th><th>timestamp</th><th>city</th><th>land_longitude</th><th>land_latitude</th><th>ME</th><th>alpha1</th><th>alpha2</th><th>original_quantile_group_id</th><th>demand_met</th></tr><tr><td>str</td><td>str</td><td>str</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td></tr></thead><tbody><tr><td>&quot;count&quot;</td><td>&quot;3481920&quot;</td><td>&quot;3481920&quot;</td><td>3.48192e6</td><td>3.48192e6</td><td>3.48192e6</td><td>3.48192e6</td><td>3.48192e6</td><td>3.48192e6</td><td>3.48192e6</td></tr><tr><td>&quot;null_count&quot;</td><td>&quot;0&quot;</td><td>&quot;0&quot;</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td></tr><tr><td>&quot;mean&quot;</td><td>&quot;2023-03-17 23:45:00+05:30&quot;</td><td>null</td><td>75.701111</td><td>25.393333</td><td>0.373358</td><td>0.887762</td><td>-0.000001</td><td>16.530667</td><td>177287.744297</td></tr><tr><td>&quot;std&quot;</td><td>null</td><td>null</td><td>1.969453</td><td>4.538596</td><td>0.073292</td><td>0.112286</td><td>3.4511e-7</td><td>11.399828</td><td>23535.203493</td></tr><tr><td>&quot;min&quot;</td><td>&quot;2021-01-01 00:00:00+05:30&quot;</td><td>null</td><td>72.77</td><td>18.5</td><td>0.101938</td><td>0.682504</td><td>-0.000002</td><td>2.0</td><td>105140.916667</td></tr><tr><td>&quot;25%&quot;</td><td>&quot;2022-02-08 00:00:00+05:30&quot;</td><td>null</td><td>72.97</td><td>19.2</td><td>0.326648</td><td>0.798551</td><td>-0.000002</td><td>4.0</td><td>160999.75</td></tr><tr><td>&quot;50%&quot;</td><td>&quot;2023-03-18 00:00:00+05:30&quot;</td><td>null</td><td>76.94</td><td>28.5</td><td>0.373625</td><td>0.869218</td><td>-0.000001</td><td>17.0</td><td>178227.333325</td></tr><tr><td>&quot;75%&quot;</td><td>&quot;2024-04-23 23:30:00+05:30&quot;</td><td>null</td><td>77.14</td><td>28.7</td><td>0.423545</td><td>1.009042</td><td>-0.000001</td><td>26.0</td><td>194806.833333</td></tr><tr><td>&quot;max&quot;</td><td>&quot;2025-05-31 23:30:00+05:30&quot;</td><td>null</td><td>77.34</td><td>28.8</td><td>0.635273</td><td>1.06163</td><td>-8.1658e-7</td><td>34.0</td><td>249591.916667</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (9, 10)\n",
       "┌────────────┬───────────┬─────────┬───────────┬───┬───────────┬───────────┬───────────┬───────────┐\n",
       "│ statistic  ┆ timestamp ┆ city    ┆ land_long ┆ … ┆ alpha1    ┆ alpha2    ┆ original_ ┆ demand_me │\n",
       "│ ---        ┆ ---       ┆ ---     ┆ itude     ┆   ┆ ---       ┆ ---       ┆ quantile_ ┆ t         │\n",
       "│ str        ┆ str       ┆ str     ┆ ---       ┆   ┆ f64       ┆ f64       ┆ group_id  ┆ ---       │\n",
       "│            ┆           ┆         ┆ f64       ┆   ┆           ┆           ┆ ---       ┆ f64       │\n",
       "│            ┆           ┆         ┆           ┆   ┆           ┆           ┆ f64       ┆           │\n",
       "╞════════════╪═══════════╪═════════╪═══════════╪═══╪═══════════╪═══════════╪═══════════╪═══════════╡\n",
       "│ count      ┆ 3481920   ┆ 3481920 ┆ 3.48192e6 ┆ … ┆ 3.48192e6 ┆ 3.48192e6 ┆ 3.48192e6 ┆ 3.48192e6 │\n",
       "│ null_count ┆ 0         ┆ 0       ┆ 0.0       ┆ … ┆ 0.0       ┆ 0.0       ┆ 0.0       ┆ 0.0       │\n",
       "│ mean       ┆ 2023-03-1 ┆ null    ┆ 75.701111 ┆ … ┆ 0.887762  ┆ -0.000001 ┆ 16.530667 ┆ 177287.74 │\n",
       "│            ┆ 7 23:45:0 ┆         ┆           ┆   ┆           ┆           ┆           ┆ 4297      │\n",
       "│            ┆ 0+05:30   ┆         ┆           ┆   ┆           ┆           ┆           ┆           │\n",
       "│ std        ┆ null      ┆ null    ┆ 1.969453  ┆ … ┆ 0.112286  ┆ 3.4511e-7 ┆ 11.399828 ┆ 23535.203 │\n",
       "│            ┆           ┆         ┆           ┆   ┆           ┆           ┆           ┆ 493       │\n",
       "│ min        ┆ 2021-01-0 ┆ null    ┆ 72.77     ┆ … ┆ 0.682504  ┆ -0.000002 ┆ 2.0       ┆ 105140.91 │\n",
       "│            ┆ 1 00:00:0 ┆         ┆           ┆   ┆           ┆           ┆           ┆ 6667      │\n",
       "│            ┆ 0+05:30   ┆         ┆           ┆   ┆           ┆           ┆           ┆           │\n",
       "│ 25%        ┆ 2022-02-0 ┆ null    ┆ 72.97     ┆ … ┆ 0.798551  ┆ -0.000002 ┆ 4.0       ┆ 160999.75 │\n",
       "│            ┆ 8 00:00:0 ┆         ┆           ┆   ┆           ┆           ┆           ┆           │\n",
       "│            ┆ 0+05:30   ┆         ┆           ┆   ┆           ┆           ┆           ┆           │\n",
       "│ 50%        ┆ 2023-03-1 ┆ null    ┆ 76.94     ┆ … ┆ 0.869218  ┆ -0.000001 ┆ 17.0      ┆ 178227.33 │\n",
       "│            ┆ 8 00:00:0 ┆         ┆           ┆   ┆           ┆           ┆           ┆ 3325      │\n",
       "│            ┆ 0+05:30   ┆         ┆           ┆   ┆           ┆           ┆           ┆           │\n",
       "│ 75%        ┆ 2024-04-2 ┆ null    ┆ 77.14     ┆ … ┆ 1.009042  ┆ -0.000001 ┆ 26.0      ┆ 194806.83 │\n",
       "│            ┆ 3 23:30:0 ┆         ┆           ┆   ┆           ┆           ┆           ┆ 3333      │\n",
       "│            ┆ 0+05:30   ┆         ┆           ┆   ┆           ┆           ┆           ┆           │\n",
       "│ max        ┆ 2025-05-3 ┆ null    ┆ 77.34     ┆ … ┆ 1.06163   ┆ -8.1658e- ┆ 34.0      ┆ 249591.91 │\n",
       "│            ┆ 1 23:30:0 ┆         ┆           ┆   ┆           ┆ 7         ┆           ┆ 6667      │\n",
       "│            ┆ 0+05:30   ┆         ┆           ┆   ┆           ┆           ┆           ┆           │\n",
       "└────────────┴───────────┴─────────┴───────────┴───┴───────────┴───────────┴───────────┴───────────┘"
      ]
     },
     "execution_count": 223,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "me_out_pldf_filled.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "id": "c137410a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (10, 9)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>timestamp</th><th>city</th><th>land_longitude</th><th>land_latitude</th><th>ME</th><th>alpha1</th><th>alpha2</th><th>original_quantile_group_id</th><th>demand_met</th></tr><tr><td>datetime[μs, Asia/Kolkata]</td><td>cat</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>i64</td><td>f64</td></tr></thead><tbody><tr><td>2024-05-31 00:30:00 IST</td><td>&quot;mumbai&quot;</td><td>72.87</td><td>19.3</td><td>0.276165</td><td>0.784577</td><td>-0.000001</td><td>9</td><td>226588.5</td></tr><tr><td>2024-05-31 01:00:00 IST</td><td>&quot;mumbai&quot;</td><td>72.87</td><td>19.3</td><td>0.258134</td><td>0.796921</td><td>-0.000001</td><td>16</td><td>223699.916667</td></tr><tr><td>2024-05-31 01:30:00 IST</td><td>&quot;mumbai&quot;</td><td>72.87</td><td>19.3</td><td>0.265286</td><td>0.796921</td><td>-0.000001</td><td>16</td><td>220730.75</td></tr><tr><td>2024-05-31 02:00:00 IST</td><td>&quot;mumbai&quot;</td><td>72.87</td><td>19.3</td><td>0.24033</td><td>0.910489</td><td>-0.000002</td><td>15</td><td>218235.916667</td></tr><tr><td>2024-05-31 02:30:00 IST</td><td>&quot;mumbai&quot;</td><td>72.87</td><td>19.3</td><td>0.215375</td><td>1.024058</td><td>-0.000002</td><td>23</td><td>216169.333333</td></tr><tr><td>2024-05-31 03:00:00 IST</td><td>&quot;mumbai&quot;</td><td>72.87</td><td>19.3</td><td>0.222551</td><td>1.024058</td><td>-0.000002</td><td>23</td><td>214251.166667</td></tr><tr><td>2024-05-31 03:30:00 IST</td><td>&quot;mumbai&quot;</td><td>72.87</td><td>19.3</td><td>0.22889</td><td>1.024058</td><td>-0.000002</td><td>23</td><td>212556.583333</td></tr><tr><td>2024-05-31 04:00:00 IST</td><td>&quot;mumbai&quot;</td><td>72.87</td><td>19.3</td><td>0.235012</td><td>1.024058</td><td>-0.000002</td><td>23</td><td>210920.083333</td></tr><tr><td>2024-05-31 04:30:00 IST</td><td>&quot;mumbai&quot;</td><td>72.87</td><td>19.3</td><td>0.241282</td><td>0.991575</td><td>-0.000002</td><td>24</td><td>210233.916667</td></tr><tr><td>2024-05-31 05:00:00 IST</td><td>&quot;mumbai&quot;</td><td>72.87</td><td>19.3</td><td>0.256864</td><td>1.055478</td><td>-0.000002</td><td>32</td><td>210322.416667</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (10, 9)\n",
       "┌────────────┬────────┬────────────┬────────────┬───┬──────────┬───────────┬───────────┬───────────┐\n",
       "│ timestamp  ┆ city   ┆ land_longi ┆ land_latit ┆ … ┆ alpha1   ┆ alpha2    ┆ original_ ┆ demand_me │\n",
       "│ ---        ┆ ---    ┆ tude       ┆ ude        ┆   ┆ ---      ┆ ---       ┆ quantile_ ┆ t         │\n",
       "│ datetime[μ ┆ cat    ┆ ---        ┆ ---        ┆   ┆ f64      ┆ f64       ┆ group_id  ┆ ---       │\n",
       "│ s, Asia/Ko ┆        ┆ f64        ┆ f64        ┆   ┆          ┆           ┆ ---       ┆ f64       │\n",
       "│ lkata]     ┆        ┆            ┆            ┆   ┆          ┆           ┆ i64       ┆           │\n",
       "╞════════════╪════════╪════════════╪════════════╪═══╪══════════╪═══════════╪═══════════╪═══════════╡\n",
       "│ 2024-05-31 ┆ mumbai ┆ 72.87      ┆ 19.3       ┆ … ┆ 0.784577 ┆ -0.000001 ┆ 9         ┆ 226588.5  │\n",
       "│ 00:30:00   ┆        ┆            ┆            ┆   ┆          ┆           ┆           ┆           │\n",
       "│ IST        ┆        ┆            ┆            ┆   ┆          ┆           ┆           ┆           │\n",
       "│ 2024-05-31 ┆ mumbai ┆ 72.87      ┆ 19.3       ┆ … ┆ 0.796921 ┆ -0.000001 ┆ 16        ┆ 223699.91 │\n",
       "│ 01:00:00   ┆        ┆            ┆            ┆   ┆          ┆           ┆           ┆ 6667      │\n",
       "│ IST        ┆        ┆            ┆            ┆   ┆          ┆           ┆           ┆           │\n",
       "│ 2024-05-31 ┆ mumbai ┆ 72.87      ┆ 19.3       ┆ … ┆ 0.796921 ┆ -0.000001 ┆ 16        ┆ 220730.75 │\n",
       "│ 01:30:00   ┆        ┆            ┆            ┆   ┆          ┆           ┆           ┆           │\n",
       "│ IST        ┆        ┆            ┆            ┆   ┆          ┆           ┆           ┆           │\n",
       "│ 2024-05-31 ┆ mumbai ┆ 72.87      ┆ 19.3       ┆ … ┆ 0.910489 ┆ -0.000002 ┆ 15        ┆ 218235.91 │\n",
       "│ 02:00:00   ┆        ┆            ┆            ┆   ┆          ┆           ┆           ┆ 6667      │\n",
       "│ IST        ┆        ┆            ┆            ┆   ┆          ┆           ┆           ┆           │\n",
       "│ 2024-05-31 ┆ mumbai ┆ 72.87      ┆ 19.3       ┆ … ┆ 1.024058 ┆ -0.000002 ┆ 23        ┆ 216169.33 │\n",
       "│ 02:30:00   ┆        ┆            ┆            ┆   ┆          ┆           ┆           ┆ 3333      │\n",
       "│ IST        ┆        ┆            ┆            ┆   ┆          ┆           ┆           ┆           │\n",
       "│ 2024-05-31 ┆ mumbai ┆ 72.87      ┆ 19.3       ┆ … ┆ 1.024058 ┆ -0.000002 ┆ 23        ┆ 214251.16 │\n",
       "│ 03:00:00   ┆        ┆            ┆            ┆   ┆          ┆           ┆           ┆ 6667      │\n",
       "│ IST        ┆        ┆            ┆            ┆   ┆          ┆           ┆           ┆           │\n",
       "│ 2024-05-31 ┆ mumbai ┆ 72.87      ┆ 19.3       ┆ … ┆ 1.024058 ┆ -0.000002 ┆ 23        ┆ 212556.58 │\n",
       "│ 03:30:00   ┆        ┆            ┆            ┆   ┆          ┆           ┆           ┆ 3333      │\n",
       "│ IST        ┆        ┆            ┆            ┆   ┆          ┆           ┆           ┆           │\n",
       "│ 2024-05-31 ┆ mumbai ┆ 72.87      ┆ 19.3       ┆ … ┆ 1.024058 ┆ -0.000002 ┆ 23        ┆ 210920.08 │\n",
       "│ 04:00:00   ┆        ┆            ┆            ┆   ┆          ┆           ┆           ┆ 3333      │\n",
       "│ IST        ┆        ┆            ┆            ┆   ┆          ┆           ┆           ┆           │\n",
       "│ 2024-05-31 ┆ mumbai ┆ 72.87      ┆ 19.3       ┆ … ┆ 0.991575 ┆ -0.000002 ┆ 24        ┆ 210233.91 │\n",
       "│ 04:30:00   ┆        ┆            ┆            ┆   ┆          ┆           ┆           ┆ 6667      │\n",
       "│ IST        ┆        ┆            ┆            ┆   ┆          ┆           ┆           ┆           │\n",
       "│ 2024-05-31 ┆ mumbai ┆ 72.87      ┆ 19.3       ┆ … ┆ 1.055478 ┆ -0.000002 ┆ 32        ┆ 210322.41 │\n",
       "│ 05:00:00   ┆        ┆            ┆            ┆   ┆          ┆           ┆           ┆ 6667      │\n",
       "│ IST        ┆        ┆            ┆            ┆   ┆          ┆           ┆           ┆           │\n",
       "└────────────┴────────┴────────────┴────────────┴───┴──────────┴───────────┴───────────┴───────────┘"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(me_out_pldf_filled.filter(\n",
    "    pl.col(\"timestamp\") > datetime(2024,5,31,0,0, tzinfo=ZoneInfo(\"Asia/Kolkata\")),\n",
    "    pl.col(\"land_longitude\").round(2) == 72.87,\n",
    "    pl.col(\"land_latitude\").round(2) == 19.30\n",
    "    ).head(10)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "id": "3b2b515b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filled data written successfully to data/marginal_emissions_development/results/original_quantile_bins_marginal_emissions_timeseries.parquet\n"
     ]
    }
   ],
   "source": [
    "# rewrite file to include filled data\n",
    "try:\n",
    "    me_out_pldf_filled.write_parquet(\n",
    "        testing_output_file,\n",
    "        compression=\"snappy\",\n",
    "        statistics=True\n",
    "    )\n",
    "    print(f\"Filled data written successfully to {testing_output_file}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error writing filled data: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46fca1e7",
   "metadata": {},
   "source": [
    "#### Grid Search"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e04db21e",
   "metadata": {},
   "source": [
    "##### Quantile Binning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a03bf49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting Up configurations\n",
    "multi_quantile_param_grid = build_quantile_grid_configs(\n",
    "    candidate_binning_vars=[\"surface_net_solar_radiation_kWh_per_m2\", \"wind_speed_mps\", \"temperature_celsius\", \"precipitation_mm\", \"total_cloud_cover\"],\n",
    "    candidate_bin_counts=[3, 5, 10, 20, 50,100],\n",
    "    candidate_x_vars=[\"demand_met\", \"demand_met_sqrd\"],\n",
    "    candidate_fe_vars=[\"month\", \"hour\", \"week_of_year\", \"day_of_week\", \"half_hour\"],\n",
    "    x_var_length=2,\n",
    "    binner_extra_grid={\"oob_policy\": [\"clip\"], \"max_oob_rate\": [0.05, 0.03, None], \"retain_flags\": [True]}\n",
    ")\n",
    "\n",
    "regressor_kwargs_q = {\n",
    "    \"y_var\": \"tons_co2\",\n",
    "    \"x_vars\": [\"demand_met\", \"demand_met_sqrd\"],  # default; overwritten per-config anyway\n",
    "    \"fe_vars\": [\"month\", \"hour\"],\n",
    "    \"group_col\": \"quantile_group_id\",\n",
    "    \"min_group_size\": 20,\n",
    "    \"track_metrics\": True,\n",
    "    \"verbose\": False,\n",
    "    \"random_state\": 12,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00dd9277",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[R0/0] [GRID 1/17298] qbin_3_surface_net_solar_radiation_kWh_per_m2__x_demand_met-demand_met_sqrd__fe_month__oobclip_rate0.05\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[503], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mrun_grid_search\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbase_feature_pipeline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfeature_addition_pipeline\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43mregressor_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mGroupwiseRegressor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mregressor_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mregressor_kwargs_q\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrid_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmulti_quantile_param_grid\u001b[49m\u001b[43m,\u001b[49m\u001b[43m           \u001b[49m\u001b[38;5;66;43;03m# pass the WHOLE list once\u001b[39;49;00m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mresults_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmarginal_emissions_logs_directory\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m# rolling shards\u001b[39;49;00m\n\u001b[1;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfile_prefix\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmarginal_emissions_prefix\u001b[49m\u001b[43m,\u001b[49m\u001b[43m            \u001b[49m\u001b[38;5;66;43;03m# e.g. \"me_grid\"\u001b[39;49;00m\n\u001b[1;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43mglobal_extra_info\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmodel_type\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmulti_binner\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43mforce_run\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43mforce_overwrite\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbase_feature_pipeline_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mFeatureAdditionPipeline\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m    \u001b[49m\u001b[43meval_splits\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtrain\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mvalidation\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m               \u001b[49m\u001b[38;5;66;43;03m# explicit: no test during tuning\u001b[39;49;00m\n\u001b[1;32m     13\u001b[0m \u001b[43m    \u001b[49m\u001b[43mx_splits\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtrain\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_pdf_x_all\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mvalidation\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidation_pdf_x_all\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtest\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_pdf_x_all\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[43m    \u001b[49m\u001b[43my_splits\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtrain\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_pdf_y\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mvalidation\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidation_pdf_y\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtest\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_pdf_y\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_log_mb\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m95\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfsync\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m   \u001b[49m\u001b[38;5;66;43;03m# set to True when running on HPC\u001b[39;49;00m\n\u001b[1;32m     17\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlog_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# legacy path unused when using rotating logs\u001b[39;49;00m\n\u001b[1;32m     18\u001b[0m \u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[440], line 116\u001b[0m, in \u001b[0;36mrun_grid_search\u001b[0;34m(base_feature_pipeline, regressor_cls, regressor_kwargs, grid_config, x_splits, y_splits, log_path, global_extra_info, force_run, force_overwrite, base_feature_pipeline_name, eval_splits, results_dir, file_prefix, max_log_mb, fsync)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mrank_tag\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m[GRID \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtotal\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m] \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlabel\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    115\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 116\u001b[0m     summary_df \u001b[38;5;241m=\u001b[39m \u001b[43mregressor_orchestrator\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    117\u001b[0m \u001b[43m        \u001b[49m\u001b[43muser_pipeline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfull_pipeline\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    118\u001b[0m \u001b[43m        \u001b[49m\u001b[43mx_splits\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mx_splits\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    119\u001b[0m \u001b[43m        \u001b[49m\u001b[43my_splits\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43my_splits\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    120\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlog_csv_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlog_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m            \u001b[49m\u001b[38;5;66;43;03m# legacy OK\u001b[39;49;00m\n\u001b[1;32m    121\u001b[0m \u001b[43m        \u001b[49m\u001b[43mextra_info\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_info\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    122\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforce_run\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_run\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    123\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforce_overwrite\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_overwrite\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    124\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrandom_state\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreg_kwargs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrandom_state\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    125\u001b[0m \u001b[43m        \u001b[49m\u001b[43meval_splits\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meval_splits\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    126\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# NEW\u001b[39;49;00m\n\u001b[1;32m    127\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresults_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresults_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    128\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfile_prefix\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfile_prefix\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    129\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_log_mb\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_log_mb\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    130\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfsync\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfsync\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    131\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    132\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m summary_df \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    133\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[GRID] Logged: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlabel\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[439], line 96\u001b[0m, in \u001b[0;36mregressor_orchestrator\u001b[0;34m(user_pipeline, x_splits, y_splits, log_csv_path, extra_info, force_run, force_overwrite, random_state, group_col_name, interval_hours, eval_splits, compute_test, results_dir, file_prefix, max_log_mb, fsync)\u001b[0m\n\u001b[1;32m     93\u001b[0m x_cols_used: \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mstr\u001b[39m] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     95\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m split \u001b[38;5;129;01min\u001b[39;00m eval_splits:\n\u001b[0;32m---> 96\u001b[0m     metrics_df, x_cols_used, extras \u001b[38;5;241m=\u001b[39m \u001b[43mrun_regressor_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     97\u001b[0m \u001b[43m        \u001b[49m\u001b[43muser_pipeline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muser_pipeline\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     98\u001b[0m \u001b[43m        \u001b[49m\u001b[43mx_df\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mx_splits\u001b[49m\u001b[43m[\u001b[49m\u001b[43msplit\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     99\u001b[0m \u001b[43m        \u001b[49m\u001b[43my_df\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43my_splits\u001b[49m\u001b[43m[\u001b[49m\u001b[43msplit\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    100\u001b[0m \u001b[43m        \u001b[49m\u001b[43msplit_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msplit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    101\u001b[0m \u001b[43m        \u001b[49m\u001b[43mextra_info\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_info\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    102\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_model\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    103\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrandom_state\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrandom_state\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    104\u001b[0m \u001b[43m        \u001b[49m\u001b[43minterval_hours\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minterval_hours\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    105\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel_id_hash\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_key\u001b[49m\u001b[43m,\u001b[49m\u001b[43m          \u001b[49m\u001b[38;5;66;43;03m# shared ID across splits\u001b[39;49;00m\n\u001b[1;32m    106\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparams_json_str\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparams_json_str\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# shared params JSON\u001b[39;49;00m\n\u001b[1;32m    107\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    108\u001b[0m     logs[split] \u001b[38;5;241m=\u001b[39m metrics_df\n\u001b[1;32m    109\u001b[0m     pooled_extras[split] \u001b[38;5;241m=\u001b[39m extras\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpooled_co2\u001b[39m\u001b[38;5;124m\"\u001b[39m, {})\n",
      "Cell \u001b[0;32mIn[438], line 91\u001b[0m, in \u001b[0;36mrun_regressor_model\u001b[0;34m(user_pipeline, x_df, y_df, split_name, extra_info, return_model, random_state, interval_hours, model_id_hash, params_json_str)\u001b[0m\n\u001b[1;32m     87\u001b[0m extras: \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Any] \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m     89\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m split_name \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m     90\u001b[0m     \u001b[38;5;66;03m# Fit → metrics from regressor\u001b[39;00m\n\u001b[0;32m---> 91\u001b[0m     _ \u001b[38;5;241m=\u001b[39m \u001b[43muser_pipeline\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_ser\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     92\u001b[0m     model \u001b[38;5;241m=\u001b[39m user_pipeline\u001b[38;5;241m.\u001b[39m_final_estimator  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[1;32m     93\u001b[0m     metrics_df \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mget_metrics(summarise\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\u001b[38;5;241m.\u001b[39mreset_index(drop\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/irpenv_4/lib/python3.10/site-packages/sklearn/base.py:1365\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1358\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[1;32m   1360\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[1;32m   1361\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m   1362\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[1;32m   1363\u001b[0m     )\n\u001b[1;32m   1364\u001b[0m ):\n\u001b[0;32m-> 1365\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/irpenv_4/lib/python3.10/site-packages/sklearn/pipeline.py:719\u001b[0m, in \u001b[0;36mPipeline.fit_transform\u001b[0;34m(self, X, y, **params)\u001b[0m\n\u001b[1;32m    680\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Fit the model and transform with the final estimator.\u001b[39;00m\n\u001b[1;32m    681\u001b[0m \n\u001b[1;32m    682\u001b[0m \u001b[38;5;124;03mFit all the transformers one after the other and sequentially transform\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    716\u001b[0m \u001b[38;5;124;03m    Transformed samples.\u001b[39;00m\n\u001b[1;32m    717\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    718\u001b[0m routed_params \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_method_params(method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfit_transform\u001b[39m\u001b[38;5;124m\"\u001b[39m, props\u001b[38;5;241m=\u001b[39mparams)\n\u001b[0;32m--> 719\u001b[0m Xt \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrouted_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    721\u001b[0m last_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_final_estimator\n\u001b[1;32m    722\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m _print_elapsed_time(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPipeline\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_log_message(\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msteps) \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m)):\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/irpenv_4/lib/python3.10/site-packages/sklearn/pipeline.py:589\u001b[0m, in \u001b[0;36mPipeline._fit\u001b[0;34m(self, X, y, routed_params, raw_params)\u001b[0m\n\u001b[1;32m    582\u001b[0m \u001b[38;5;66;03m# Fit or load from cache the current transformer\u001b[39;00m\n\u001b[1;32m    583\u001b[0m step_params \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_metadata_for_step(\n\u001b[1;32m    584\u001b[0m     step_idx\u001b[38;5;241m=\u001b[39mstep_idx,\n\u001b[1;32m    585\u001b[0m     step_params\u001b[38;5;241m=\u001b[39mrouted_params[name],\n\u001b[1;32m    586\u001b[0m     all_params\u001b[38;5;241m=\u001b[39mraw_params,\n\u001b[1;32m    587\u001b[0m )\n\u001b[0;32m--> 589\u001b[0m X, fitted_transformer \u001b[38;5;241m=\u001b[39m \u001b[43mfit_transform_one_cached\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    590\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcloned_transformer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    591\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    592\u001b[0m \u001b[43m    \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    593\u001b[0m \u001b[43m    \u001b[49m\u001b[43mweight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    594\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmessage_clsname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mPipeline\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    595\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmessage\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_log_message\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstep_idx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    596\u001b[0m \u001b[43m    \u001b[49m\u001b[43mparams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstep_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    597\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    598\u001b[0m \u001b[38;5;66;03m# Replace the transformer of the step with the fitted\u001b[39;00m\n\u001b[1;32m    599\u001b[0m \u001b[38;5;66;03m# transformer. This is necessary when loading the transformer\u001b[39;00m\n\u001b[1;32m    600\u001b[0m \u001b[38;5;66;03m# from the cache.\u001b[39;00m\n\u001b[1;32m    601\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msteps[step_idx] \u001b[38;5;241m=\u001b[39m (name, fitted_transformer)\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/irpenv_4/lib/python3.10/site-packages/joblib/memory.py:326\u001b[0m, in \u001b[0;36mNotMemorizedFunc.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 326\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/irpenv_4/lib/python3.10/site-packages/sklearn/pipeline.py:1540\u001b[0m, in \u001b[0;36m_fit_transform_one\u001b[0;34m(transformer, X, y, weight, message_clsname, message, params)\u001b[0m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m _print_elapsed_time(message_clsname, message):\n\u001b[1;32m   1539\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(transformer, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfit_transform\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m-> 1540\u001b[0m         res \u001b[38;5;241m=\u001b[39m \u001b[43mtransformer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfit_transform\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1541\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1542\u001b[0m         res \u001b[38;5;241m=\u001b[39m transformer\u001b[38;5;241m.\u001b[39mfit(X, y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfit\u001b[39m\u001b[38;5;124m\"\u001b[39m, {}))\u001b[38;5;241m.\u001b[39mtransform(\n\u001b[1;32m   1543\u001b[0m             X, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtransform\u001b[39m\u001b[38;5;124m\"\u001b[39m, {})\n\u001b[1;32m   1544\u001b[0m         )\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/irpenv_4/lib/python3.10/site-packages/sklearn/base.py:1365\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1358\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[1;32m   1360\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[1;32m   1361\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m   1362\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[1;32m   1363\u001b[0m     )\n\u001b[1;32m   1364\u001b[0m ):\n\u001b[0;32m-> 1365\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/irpenv_4/lib/python3.10/site-packages/sklearn/pipeline.py:731\u001b[0m, in \u001b[0;36mPipeline.fit_transform\u001b[0;34m(self, X, y, **params)\u001b[0m\n\u001b[1;32m    725\u001b[0m last_step_params \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_metadata_for_step(\n\u001b[1;32m    726\u001b[0m     step_idx\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m,\n\u001b[1;32m    727\u001b[0m     step_params\u001b[38;5;241m=\u001b[39mrouted_params[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msteps[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m][\u001b[38;5;241m0\u001b[39m]],\n\u001b[1;32m    728\u001b[0m     all_params\u001b[38;5;241m=\u001b[39mparams,\n\u001b[1;32m    729\u001b[0m )\n\u001b[1;32m    730\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(last_step, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfit_transform\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m--> 731\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mlast_step\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_transform\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    732\u001b[0m \u001b[43m        \u001b[49m\u001b[43mXt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mlast_step_params\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfit_transform\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[1;32m    733\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    734\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    735\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m last_step\u001b[38;5;241m.\u001b[39mfit(Xt, y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mlast_step_params[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfit\u001b[39m\u001b[38;5;124m\"\u001b[39m])\u001b[38;5;241m.\u001b[39mtransform(\n\u001b[1;32m    736\u001b[0m         Xt, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mlast_step_params[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtransform\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m    737\u001b[0m     )\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/irpenv_4/lib/python3.10/site-packages/sklearn/utils/_set_output.py:316\u001b[0m, in \u001b[0;36m_wrap_method_output.<locals>.wrapped\u001b[0;34m(self, X, *args, **kwargs)\u001b[0m\n\u001b[1;32m    314\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(f)\n\u001b[1;32m    315\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mwrapped\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 316\u001b[0m     data_to_wrap \u001b[38;5;241m=\u001b[39m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    317\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data_to_wrap, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[1;32m    318\u001b[0m         \u001b[38;5;66;03m# only wrap the first output for cross decomposition\u001b[39;00m\n\u001b[1;32m    319\u001b[0m         return_tuple \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    320\u001b[0m             _wrap_data_with_container(method, data_to_wrap[\u001b[38;5;241m0\u001b[39m], X, \u001b[38;5;28mself\u001b[39m),\n\u001b[1;32m    321\u001b[0m             \u001b[38;5;241m*\u001b[39mdata_to_wrap[\u001b[38;5;241m1\u001b[39m:],\n\u001b[1;32m    322\u001b[0m         )\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/irpenv_4/lib/python3.10/site-packages/sklearn/base.py:897\u001b[0m, in \u001b[0;36mTransformerMixin.fit_transform\u001b[0;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[1;32m    894\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfit(X, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_params)\u001b[38;5;241m.\u001b[39mtransform(X)\n\u001b[1;32m    895\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    896\u001b[0m     \u001b[38;5;66;03m# fit method of arity 2 (supervised transformation)\u001b[39;00m\n\u001b[0;32m--> 897\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfit_params\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/irpenv_4/lib/python3.10/site-packages/sklearn/utils/_set_output.py:316\u001b[0m, in \u001b[0;36m_wrap_method_output.<locals>.wrapped\u001b[0;34m(self, X, *args, **kwargs)\u001b[0m\n\u001b[1;32m    314\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(f)\n\u001b[1;32m    315\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mwrapped\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 316\u001b[0m     data_to_wrap \u001b[38;5;241m=\u001b[39m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    317\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data_to_wrap, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[1;32m    318\u001b[0m         \u001b[38;5;66;03m# only wrap the first output for cross decomposition\u001b[39;00m\n\u001b[1;32m    319\u001b[0m         return_tuple \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    320\u001b[0m             _wrap_data_with_container(method, data_to_wrap[\u001b[38;5;241m0\u001b[39m], X, \u001b[38;5;28mself\u001b[39m),\n\u001b[1;32m    321\u001b[0m             \u001b[38;5;241m*\u001b[39mdata_to_wrap[\u001b[38;5;241m1\u001b[39m:],\n\u001b[1;32m    322\u001b[0m         )\n",
      "Cell \u001b[0;32mIn[428], line 82\u001b[0m, in \u001b[0;36mAnalysisFeatureAdder.transform\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m     79\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mColumn \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtimestamp_col\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m contains non-parseable datetimes\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     81\u001b[0m \u001b[38;5;66;03m# temporal\u001b[39;00m\n\u001b[0;32m---> 82\u001b[0m df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtime_id\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mdf\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtimestamp_col\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstrftime\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m%\u001b[39;49m\u001b[38;5;124;43mH-\u001b[39;49m\u001b[38;5;124;43m%\u001b[39;49m\u001b[38;5;124;43mM\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstring\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     84\u001b[0m \u001b[38;5;66;03m# quantitative\u001b[39;00m\n\u001b[1;32m     85\u001b[0m q \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdemand_met_col\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/irpenv_4/lib/python3.10/site-packages/pandas/core/accessor.py:112\u001b[0m, in \u001b[0;36mPandasDelegate._add_delegate_accessors.<locals>._create_delegator_method.<locals>.f\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    111\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mf\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 112\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_delegate_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/irpenv_4/lib/python3.10/site-packages/pandas/core/indexes/accessors.py:132\u001b[0m, in \u001b[0;36mProperties._delegate_method\u001b[0;34m(self, name, *args, **kwargs)\u001b[0m\n\u001b[1;32m    129\u001b[0m values \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_values()\n\u001b[1;32m    131\u001b[0m method \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(values, name)\n\u001b[0;32m--> 132\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mmethod\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    134\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_list_like(result):\n\u001b[1;32m    135\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/irpenv_4/lib/python3.10/site-packages/pandas/core/indexes/datetimes.py:278\u001b[0m, in \u001b[0;36mDatetimeIndex.strftime\u001b[0;34m(self, date_format)\u001b[0m\n\u001b[1;32m    276\u001b[0m \u001b[38;5;129m@doc\u001b[39m(DatetimeArray\u001b[38;5;241m.\u001b[39mstrftime)\n\u001b[1;32m    277\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mstrftime\u001b[39m(\u001b[38;5;28mself\u001b[39m, date_format) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Index:\n\u001b[0;32m--> 278\u001b[0m     arr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_data\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstrftime\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdate_format\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    279\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m Index(arr, name\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname, dtype\u001b[38;5;241m=\u001b[39marr\u001b[38;5;241m.\u001b[39mdtype)\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/irpenv_4/lib/python3.10/site-packages/pandas/core/arrays/datetimelike.py:1814\u001b[0m, in \u001b[0;36mDatelikeOps.strftime\u001b[0;34m(self, date_format)\u001b[0m\n\u001b[1;32m   1765\u001b[0m \u001b[38;5;129m@Substitution\u001b[39m(\n\u001b[1;32m   1766\u001b[0m     URL\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://docs.python.org/3/library/datetime.html\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1767\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m#strftime-and-strptime-behavior\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1768\u001b[0m )\n\u001b[1;32m   1769\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mstrftime\u001b[39m(\u001b[38;5;28mself\u001b[39m, date_format: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m npt\u001b[38;5;241m.\u001b[39mNDArray[np\u001b[38;5;241m.\u001b[39mobject_]:\n\u001b[1;32m   1770\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1771\u001b[0m \u001b[38;5;124;03m    Convert to Index using specified date_format.\u001b[39;00m\n\u001b[1;32m   1772\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1812\u001b[0m \u001b[38;5;124;03m          dtype='object')\u001b[39;00m\n\u001b[1;32m   1813\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1814\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_format_native_types\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdate_format\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdate_format\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mna_rep\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnan\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1815\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m using_string_dtype():\n\u001b[1;32m   1816\u001b[0m         \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m StringDtype\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/irpenv_4/lib/python3.10/site-packages/pandas/core/arrays/datetimes.py:753\u001b[0m, in \u001b[0;36mDatetimeArray._format_native_types\u001b[0;34m(self, na_rep, date_format, **kwargs)\u001b[0m\n\u001b[1;32m    749\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m date_format \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_is_dates_only:\n\u001b[1;32m    750\u001b[0m     \u001b[38;5;66;03m# Only dates and no timezone: provide a default format\u001b[39;00m\n\u001b[1;32m    751\u001b[0m     date_format \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mY-\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mm-\u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 753\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtslib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mformat_array_from_datetime\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    754\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43masi8\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtz\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtz\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mformat\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdate_format\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mna_rep\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mna_rep\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreso\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_creso\u001b[49m\n\u001b[1;32m    755\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32mpandas/_libs/tslib.pyx:222\u001b[0m, in \u001b[0;36mpandas._libs.tslib.format_array_from_datetime\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/tslibs/timestamps.pyx:1488\u001b[0m, in \u001b[0;36mpandas._libs.tslibs.timestamps.Timestamp.strftime\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/irpenv_4/lib/python3.10/site-packages/pytz/tzinfo.py:430\u001b[0m, in \u001b[0;36mDstTzInfo.dst\u001b[0;34m(self, dt, is_dst)\u001b[0m\n\u001b[1;32m    427\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    428\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_utcoffset\n\u001b[0;32m--> 430\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mdst\u001b[39m(\u001b[38;5;28mself\u001b[39m, dt, is_dst\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m    431\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m'''See datetime.tzinfo.dst\u001b[39;00m\n\u001b[1;32m    432\u001b[0m \n\u001b[1;32m    433\u001b[0m \u001b[38;5;124;03m    The is_dst parameter may be used to remove ambiguity during DST\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    459\u001b[0m \n\u001b[1;32m    460\u001b[0m \u001b[38;5;124;03m    '''\u001b[39;00m\n\u001b[1;32m    461\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m dt \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "run_grid_search(\n",
    "    base_feature_pipeline=feature_addition_pipeline,\n",
    "    regressor_cls=GroupwiseRegressor,\n",
    "    regressor_kwargs=regressor_kwargs_q,\n",
    "    grid_config=multi_quantile_param_grid,           # pass the WHOLE list once\n",
    "    results_dir=marginal_emissions_logs_directory, # rolling shards\n",
    "    file_prefix=marginal_emissions_prefix,            # e.g. \"me_grid\"\n",
    "    global_extra_info={\"model_type\": \"multi_binner\"},\n",
    "    force_run=False,\n",
    "    force_overwrite=False,\n",
    "    base_feature_pipeline_name=\"FeatureAdditionPipeline\",\n",
    "    eval_splits=(\"train\",\"validation\"),               # explicit: no test during tuning\n",
    "    x_splits={\"train\": train_pdf_x_all, \"validation\": validation_pdf_x_all, \"test\": test_pdf_x_all},\n",
    "    y_splits={\"train\": train_pdf_y, \"validation\": validation_pdf_y, \"test\": test_pdf_y},\n",
    "    max_log_mb=95,\n",
    "    fsync = False,   # set to True when running on HPC\n",
    "    log_path=None,  # legacy path unused when using rotating logs\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e74dea3d",
   "metadata": {},
   "source": [
    "##### Median Binning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "391cd123",
   "metadata": {},
   "outputs": [],
   "source": [
    "candidate_binning_vars = [\"surface_net_solar_radiation_kWh_per_m2\", \"wind_speed_mps\", \"temperature_celsius\", \"precipitation_mm\", \"total_cloud_cover\"]\n",
    "candidate_x_vars = [\"demand_met\", \"demand_met_sqrd\"]\n",
    "candidate_fe_vars = [\"month\", \"hour\", \"week_of_year\", \"day_of_week\", \"half_hour\"]\n",
    "\n",
    "multi_median_param_grid = build_median_binner_configs(\n",
    "    candidate_binning_vars=candidate_binning_vars,\n",
    "    candidate_x_vars=candidate_x_vars,\n",
    "    candidate_fe_vars=candidate_fe_vars,\n",
    "    x_var_length=2,\n",
    ")\n",
    "regressor_kwargs_m = {\n",
    "    \"y_var\": \"tons_co2\",\n",
    "    \"x_vars\": [\"demand_met\", \"demand_met_sqrd\"],  # default; grid entries can override\n",
    "    \"fe_vars\": [\"month\", \"hour\"],                  # default; grid entries can override\n",
    "    \"group_col\": \"median_group_id\",                # MUST match binner's group_col_name\n",
    "    \"min_group_size\": 20,\n",
    "    \"track_metrics\": True,\n",
    "    \"verbose\": False,\n",
    "    \"random_state\": 12\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efd01461",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[R0/0] [GRID 1/961] median_surface_net_solar_radiation_kWh_per_m2__x_demand_met-demand_met_sqrd__fe_month\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[505], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mrun_grid_search\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbase_feature_pipeline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfeature_addition_pipeline\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43mregressor_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mGroupwiseRegressor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mregressor_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mregressor_kwargs_m\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrid_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmulti_median_param_grid\u001b[49m\u001b[43m,\u001b[49m\u001b[43m           \u001b[49m\u001b[38;5;66;43;03m# pass the entire list\u001b[39;49;00m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mx_splits\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtrain\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_pdf_x_all\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mvalidation\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidation_pdf_x_all\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtest\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_pdf_x_all\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43my_splits\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtrain\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_pdf_y\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mvalidation\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidation_pdf_y\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtest\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_pdf_y\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43mresults_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmarginal_emissions_logs_directory\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfile_prefix\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmarginal_emissions_prefix\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlog_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# keep None when using rotating logs\u001b[39;49;00m\n\u001b[1;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[43mglobal_extra_info\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmodel_type\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmedian_binner\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m    \u001b[49m\u001b[43mforce_run\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[43m    \u001b[49m\u001b[43mforce_overwrite\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbase_feature_pipeline_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mFeatureAdditionPipeline\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[43m    \u001b[49m\u001b[43meval_splits\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtrain\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mvalidation\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_log_mb\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m95\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfsync\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m   \u001b[49m\u001b[38;5;66;43;03m# set True on HPC if you want durable writes\u001b[39;49;00m\n\u001b[1;32m     18\u001b[0m \u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[440], line 116\u001b[0m, in \u001b[0;36mrun_grid_search\u001b[0;34m(base_feature_pipeline, regressor_cls, regressor_kwargs, grid_config, x_splits, y_splits, log_path, global_extra_info, force_run, force_overwrite, base_feature_pipeline_name, eval_splits, results_dir, file_prefix, max_log_mb, fsync)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mrank_tag\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m[GRID \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtotal\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m] \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlabel\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    115\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 116\u001b[0m     summary_df \u001b[38;5;241m=\u001b[39m \u001b[43mregressor_orchestrator\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    117\u001b[0m \u001b[43m        \u001b[49m\u001b[43muser_pipeline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfull_pipeline\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    118\u001b[0m \u001b[43m        \u001b[49m\u001b[43mx_splits\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mx_splits\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    119\u001b[0m \u001b[43m        \u001b[49m\u001b[43my_splits\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43my_splits\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    120\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlog_csv_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlog_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m            \u001b[49m\u001b[38;5;66;43;03m# legacy OK\u001b[39;49;00m\n\u001b[1;32m    121\u001b[0m \u001b[43m        \u001b[49m\u001b[43mextra_info\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_info\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    122\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforce_run\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_run\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    123\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforce_overwrite\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_overwrite\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    124\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrandom_state\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreg_kwargs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrandom_state\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    125\u001b[0m \u001b[43m        \u001b[49m\u001b[43meval_splits\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meval_splits\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    126\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# NEW\u001b[39;49;00m\n\u001b[1;32m    127\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresults_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresults_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    128\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfile_prefix\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfile_prefix\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    129\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_log_mb\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_log_mb\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    130\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfsync\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfsync\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    131\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    132\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m summary_df \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    133\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[GRID] Logged: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlabel\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[439], line 96\u001b[0m, in \u001b[0;36mregressor_orchestrator\u001b[0;34m(user_pipeline, x_splits, y_splits, log_csv_path, extra_info, force_run, force_overwrite, random_state, group_col_name, interval_hours, eval_splits, compute_test, results_dir, file_prefix, max_log_mb, fsync)\u001b[0m\n\u001b[1;32m     93\u001b[0m x_cols_used: \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mstr\u001b[39m] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     95\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m split \u001b[38;5;129;01min\u001b[39;00m eval_splits:\n\u001b[0;32m---> 96\u001b[0m     metrics_df, x_cols_used, extras \u001b[38;5;241m=\u001b[39m \u001b[43mrun_regressor_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     97\u001b[0m \u001b[43m        \u001b[49m\u001b[43muser_pipeline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muser_pipeline\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     98\u001b[0m \u001b[43m        \u001b[49m\u001b[43mx_df\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mx_splits\u001b[49m\u001b[43m[\u001b[49m\u001b[43msplit\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     99\u001b[0m \u001b[43m        \u001b[49m\u001b[43my_df\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43my_splits\u001b[49m\u001b[43m[\u001b[49m\u001b[43msplit\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    100\u001b[0m \u001b[43m        \u001b[49m\u001b[43msplit_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msplit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    101\u001b[0m \u001b[43m        \u001b[49m\u001b[43mextra_info\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_info\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    102\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_model\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    103\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrandom_state\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrandom_state\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    104\u001b[0m \u001b[43m        \u001b[49m\u001b[43minterval_hours\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minterval_hours\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    105\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel_id_hash\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_key\u001b[49m\u001b[43m,\u001b[49m\u001b[43m          \u001b[49m\u001b[38;5;66;43;03m# shared ID across splits\u001b[39;49;00m\n\u001b[1;32m    106\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparams_json_str\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparams_json_str\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# shared params JSON\u001b[39;49;00m\n\u001b[1;32m    107\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    108\u001b[0m     logs[split] \u001b[38;5;241m=\u001b[39m metrics_df\n\u001b[1;32m    109\u001b[0m     pooled_extras[split] \u001b[38;5;241m=\u001b[39m extras\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpooled_co2\u001b[39m\u001b[38;5;124m\"\u001b[39m, {})\n",
      "Cell \u001b[0;32mIn[438], line 91\u001b[0m, in \u001b[0;36mrun_regressor_model\u001b[0;34m(user_pipeline, x_df, y_df, split_name, extra_info, return_model, random_state, interval_hours, model_id_hash, params_json_str)\u001b[0m\n\u001b[1;32m     87\u001b[0m extras: \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Any] \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m     89\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m split_name \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m     90\u001b[0m     \u001b[38;5;66;03m# Fit → metrics from regressor\u001b[39;00m\n\u001b[0;32m---> 91\u001b[0m     _ \u001b[38;5;241m=\u001b[39m \u001b[43muser_pipeline\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_ser\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     92\u001b[0m     model \u001b[38;5;241m=\u001b[39m user_pipeline\u001b[38;5;241m.\u001b[39m_final_estimator  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[1;32m     93\u001b[0m     metrics_df \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mget_metrics(summarise\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\u001b[38;5;241m.\u001b[39mreset_index(drop\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/irpenv_4/lib/python3.10/site-packages/sklearn/base.py:1365\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1358\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[1;32m   1360\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[1;32m   1361\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m   1362\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[1;32m   1363\u001b[0m     )\n\u001b[1;32m   1364\u001b[0m ):\n\u001b[0;32m-> 1365\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/irpenv_4/lib/python3.10/site-packages/sklearn/pipeline.py:719\u001b[0m, in \u001b[0;36mPipeline.fit_transform\u001b[0;34m(self, X, y, **params)\u001b[0m\n\u001b[1;32m    680\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Fit the model and transform with the final estimator.\u001b[39;00m\n\u001b[1;32m    681\u001b[0m \n\u001b[1;32m    682\u001b[0m \u001b[38;5;124;03mFit all the transformers one after the other and sequentially transform\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    716\u001b[0m \u001b[38;5;124;03m    Transformed samples.\u001b[39;00m\n\u001b[1;32m    717\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    718\u001b[0m routed_params \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_method_params(method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfit_transform\u001b[39m\u001b[38;5;124m\"\u001b[39m, props\u001b[38;5;241m=\u001b[39mparams)\n\u001b[0;32m--> 719\u001b[0m Xt \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrouted_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    721\u001b[0m last_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_final_estimator\n\u001b[1;32m    722\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m _print_elapsed_time(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPipeline\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_log_message(\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msteps) \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m)):\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/irpenv_4/lib/python3.10/site-packages/sklearn/pipeline.py:589\u001b[0m, in \u001b[0;36mPipeline._fit\u001b[0;34m(self, X, y, routed_params, raw_params)\u001b[0m\n\u001b[1;32m    582\u001b[0m \u001b[38;5;66;03m# Fit or load from cache the current transformer\u001b[39;00m\n\u001b[1;32m    583\u001b[0m step_params \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_metadata_for_step(\n\u001b[1;32m    584\u001b[0m     step_idx\u001b[38;5;241m=\u001b[39mstep_idx,\n\u001b[1;32m    585\u001b[0m     step_params\u001b[38;5;241m=\u001b[39mrouted_params[name],\n\u001b[1;32m    586\u001b[0m     all_params\u001b[38;5;241m=\u001b[39mraw_params,\n\u001b[1;32m    587\u001b[0m )\n\u001b[0;32m--> 589\u001b[0m X, fitted_transformer \u001b[38;5;241m=\u001b[39m \u001b[43mfit_transform_one_cached\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    590\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcloned_transformer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    591\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    592\u001b[0m \u001b[43m    \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    593\u001b[0m \u001b[43m    \u001b[49m\u001b[43mweight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    594\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmessage_clsname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mPipeline\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    595\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmessage\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_log_message\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstep_idx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    596\u001b[0m \u001b[43m    \u001b[49m\u001b[43mparams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstep_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    597\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    598\u001b[0m \u001b[38;5;66;03m# Replace the transformer of the step with the fitted\u001b[39;00m\n\u001b[1;32m    599\u001b[0m \u001b[38;5;66;03m# transformer. This is necessary when loading the transformer\u001b[39;00m\n\u001b[1;32m    600\u001b[0m \u001b[38;5;66;03m# from the cache.\u001b[39;00m\n\u001b[1;32m    601\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msteps[step_idx] \u001b[38;5;241m=\u001b[39m (name, fitted_transformer)\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/irpenv_4/lib/python3.10/site-packages/joblib/memory.py:326\u001b[0m, in \u001b[0;36mNotMemorizedFunc.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 326\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/irpenv_4/lib/python3.10/site-packages/sklearn/pipeline.py:1540\u001b[0m, in \u001b[0;36m_fit_transform_one\u001b[0;34m(transformer, X, y, weight, message_clsname, message, params)\u001b[0m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m _print_elapsed_time(message_clsname, message):\n\u001b[1;32m   1539\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(transformer, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfit_transform\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m-> 1540\u001b[0m         res \u001b[38;5;241m=\u001b[39m \u001b[43mtransformer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfit_transform\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1541\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1542\u001b[0m         res \u001b[38;5;241m=\u001b[39m transformer\u001b[38;5;241m.\u001b[39mfit(X, y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfit\u001b[39m\u001b[38;5;124m\"\u001b[39m, {}))\u001b[38;5;241m.\u001b[39mtransform(\n\u001b[1;32m   1543\u001b[0m             X, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtransform\u001b[39m\u001b[38;5;124m\"\u001b[39m, {})\n\u001b[1;32m   1544\u001b[0m         )\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/irpenv_4/lib/python3.10/site-packages/sklearn/base.py:1365\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1358\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[1;32m   1360\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[1;32m   1361\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m   1362\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[1;32m   1363\u001b[0m     )\n\u001b[1;32m   1364\u001b[0m ):\n\u001b[0;32m-> 1365\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/irpenv_4/lib/python3.10/site-packages/sklearn/pipeline.py:731\u001b[0m, in \u001b[0;36mPipeline.fit_transform\u001b[0;34m(self, X, y, **params)\u001b[0m\n\u001b[1;32m    725\u001b[0m last_step_params \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_metadata_for_step(\n\u001b[1;32m    726\u001b[0m     step_idx\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m,\n\u001b[1;32m    727\u001b[0m     step_params\u001b[38;5;241m=\u001b[39mrouted_params[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msteps[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m][\u001b[38;5;241m0\u001b[39m]],\n\u001b[1;32m    728\u001b[0m     all_params\u001b[38;5;241m=\u001b[39mparams,\n\u001b[1;32m    729\u001b[0m )\n\u001b[1;32m    730\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(last_step, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfit_transform\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m--> 731\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mlast_step\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_transform\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    732\u001b[0m \u001b[43m        \u001b[49m\u001b[43mXt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mlast_step_params\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfit_transform\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[1;32m    733\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    734\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    735\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m last_step\u001b[38;5;241m.\u001b[39mfit(Xt, y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mlast_step_params[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfit\u001b[39m\u001b[38;5;124m\"\u001b[39m])\u001b[38;5;241m.\u001b[39mtransform(\n\u001b[1;32m    736\u001b[0m         Xt, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mlast_step_params[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtransform\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m    737\u001b[0m     )\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/irpenv_4/lib/python3.10/site-packages/sklearn/utils/_set_output.py:316\u001b[0m, in \u001b[0;36m_wrap_method_output.<locals>.wrapped\u001b[0;34m(self, X, *args, **kwargs)\u001b[0m\n\u001b[1;32m    314\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(f)\n\u001b[1;32m    315\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mwrapped\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 316\u001b[0m     data_to_wrap \u001b[38;5;241m=\u001b[39m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    317\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data_to_wrap, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[1;32m    318\u001b[0m         \u001b[38;5;66;03m# only wrap the first output for cross decomposition\u001b[39;00m\n\u001b[1;32m    319\u001b[0m         return_tuple \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    320\u001b[0m             _wrap_data_with_container(method, data_to_wrap[\u001b[38;5;241m0\u001b[39m], X, \u001b[38;5;28mself\u001b[39m),\n\u001b[1;32m    321\u001b[0m             \u001b[38;5;241m*\u001b[39mdata_to_wrap[\u001b[38;5;241m1\u001b[39m:],\n\u001b[1;32m    322\u001b[0m         )\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/irpenv_4/lib/python3.10/site-packages/sklearn/base.py:897\u001b[0m, in \u001b[0;36mTransformerMixin.fit_transform\u001b[0;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[1;32m    894\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfit(X, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_params)\u001b[38;5;241m.\u001b[39mtransform(X)\n\u001b[1;32m    895\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    896\u001b[0m     \u001b[38;5;66;03m# fit method of arity 2 (supervised transformation)\u001b[39;00m\n\u001b[0;32m--> 897\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfit_params\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/irpenv_4/lib/python3.10/site-packages/sklearn/utils/_set_output.py:316\u001b[0m, in \u001b[0;36m_wrap_method_output.<locals>.wrapped\u001b[0;34m(self, X, *args, **kwargs)\u001b[0m\n\u001b[1;32m    314\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(f)\n\u001b[1;32m    315\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mwrapped\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 316\u001b[0m     data_to_wrap \u001b[38;5;241m=\u001b[39m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    317\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data_to_wrap, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[1;32m    318\u001b[0m         \u001b[38;5;66;03m# only wrap the first output for cross decomposition\u001b[39;00m\n\u001b[1;32m    319\u001b[0m         return_tuple \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    320\u001b[0m             _wrap_data_with_container(method, data_to_wrap[\u001b[38;5;241m0\u001b[39m], X, \u001b[38;5;28mself\u001b[39m),\n\u001b[1;32m    321\u001b[0m             \u001b[38;5;241m*\u001b[39mdata_to_wrap[\u001b[38;5;241m1\u001b[39m:],\n\u001b[1;32m    322\u001b[0m         )\n",
      "Cell \u001b[0;32mIn[428], line 82\u001b[0m, in \u001b[0;36mAnalysisFeatureAdder.transform\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m     79\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mColumn \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtimestamp_col\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m contains non-parseable datetimes\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     81\u001b[0m \u001b[38;5;66;03m# temporal\u001b[39;00m\n\u001b[0;32m---> 82\u001b[0m df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtime_id\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mdf\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtimestamp_col\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstrftime\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m%\u001b[39;49m\u001b[38;5;124;43mH-\u001b[39;49m\u001b[38;5;124;43m%\u001b[39;49m\u001b[38;5;124;43mM\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstring\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     84\u001b[0m \u001b[38;5;66;03m# quantitative\u001b[39;00m\n\u001b[1;32m     85\u001b[0m q \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdemand_met_col\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/irpenv_4/lib/python3.10/site-packages/pandas/core/accessor.py:112\u001b[0m, in \u001b[0;36mPandasDelegate._add_delegate_accessors.<locals>._create_delegator_method.<locals>.f\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    111\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mf\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 112\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_delegate_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/irpenv_4/lib/python3.10/site-packages/pandas/core/indexes/accessors.py:132\u001b[0m, in \u001b[0;36mProperties._delegate_method\u001b[0;34m(self, name, *args, **kwargs)\u001b[0m\n\u001b[1;32m    129\u001b[0m values \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_values()\n\u001b[1;32m    131\u001b[0m method \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(values, name)\n\u001b[0;32m--> 132\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mmethod\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    134\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_list_like(result):\n\u001b[1;32m    135\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/irpenv_4/lib/python3.10/site-packages/pandas/core/indexes/datetimes.py:278\u001b[0m, in \u001b[0;36mDatetimeIndex.strftime\u001b[0;34m(self, date_format)\u001b[0m\n\u001b[1;32m    276\u001b[0m \u001b[38;5;129m@doc\u001b[39m(DatetimeArray\u001b[38;5;241m.\u001b[39mstrftime)\n\u001b[1;32m    277\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mstrftime\u001b[39m(\u001b[38;5;28mself\u001b[39m, date_format) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Index:\n\u001b[0;32m--> 278\u001b[0m     arr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_data\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstrftime\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdate_format\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    279\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m Index(arr, name\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname, dtype\u001b[38;5;241m=\u001b[39marr\u001b[38;5;241m.\u001b[39mdtype)\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/irpenv_4/lib/python3.10/site-packages/pandas/core/arrays/datetimelike.py:1814\u001b[0m, in \u001b[0;36mDatelikeOps.strftime\u001b[0;34m(self, date_format)\u001b[0m\n\u001b[1;32m   1765\u001b[0m \u001b[38;5;129m@Substitution\u001b[39m(\n\u001b[1;32m   1766\u001b[0m     URL\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://docs.python.org/3/library/datetime.html\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1767\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m#strftime-and-strptime-behavior\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1768\u001b[0m )\n\u001b[1;32m   1769\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mstrftime\u001b[39m(\u001b[38;5;28mself\u001b[39m, date_format: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m npt\u001b[38;5;241m.\u001b[39mNDArray[np\u001b[38;5;241m.\u001b[39mobject_]:\n\u001b[1;32m   1770\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1771\u001b[0m \u001b[38;5;124;03m    Convert to Index using specified date_format.\u001b[39;00m\n\u001b[1;32m   1772\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1812\u001b[0m \u001b[38;5;124;03m          dtype='object')\u001b[39;00m\n\u001b[1;32m   1813\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1814\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_format_native_types\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdate_format\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdate_format\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mna_rep\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnan\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1815\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m using_string_dtype():\n\u001b[1;32m   1816\u001b[0m         \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m StringDtype\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/irpenv_4/lib/python3.10/site-packages/pandas/core/arrays/datetimes.py:753\u001b[0m, in \u001b[0;36mDatetimeArray._format_native_types\u001b[0;34m(self, na_rep, date_format, **kwargs)\u001b[0m\n\u001b[1;32m    749\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m date_format \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_is_dates_only:\n\u001b[1;32m    750\u001b[0m     \u001b[38;5;66;03m# Only dates and no timezone: provide a default format\u001b[39;00m\n\u001b[1;32m    751\u001b[0m     date_format \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mY-\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mm-\u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 753\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtslib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mformat_array_from_datetime\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    754\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43masi8\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtz\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtz\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mformat\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdate_format\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mna_rep\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mna_rep\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreso\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_creso\u001b[49m\n\u001b[1;32m    755\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32mpandas/_libs/tslib.pyx:222\u001b[0m, in \u001b[0;36mpandas._libs.tslib.format_array_from_datetime\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/tslibs/timestamps.pyx:1488\u001b[0m, in \u001b[0;36mpandas._libs.tslibs.timestamps.Timestamp.strftime\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/irpenv_4/lib/python3.10/site-packages/pytz/tzinfo.py:430\u001b[0m, in \u001b[0;36mDstTzInfo.dst\u001b[0;34m(self, dt, is_dst)\u001b[0m\n\u001b[1;32m    427\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    428\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_utcoffset\n\u001b[0;32m--> 430\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mdst\u001b[39m(\u001b[38;5;28mself\u001b[39m, dt, is_dst\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m    431\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m'''See datetime.tzinfo.dst\u001b[39;00m\n\u001b[1;32m    432\u001b[0m \n\u001b[1;32m    433\u001b[0m \u001b[38;5;124;03m    The is_dst parameter may be used to remove ambiguity during DST\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    459\u001b[0m \n\u001b[1;32m    460\u001b[0m \u001b[38;5;124;03m    '''\u001b[39;00m\n\u001b[1;32m    461\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m dt \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "run_grid_search(\n",
    "    base_feature_pipeline=feature_addition_pipeline,\n",
    "    regressor_cls=GroupwiseRegressor,\n",
    "    regressor_kwargs=regressor_kwargs_m,\n",
    "    grid_config=multi_median_param_grid,           # pass the entire list\n",
    "    x_splits={\"train\": train_pdf_x_all, \"validation\": validation_pdf_x_all, \"test\": test_pdf_x_all},\n",
    "    y_splits={\"train\": train_pdf_y, \"validation\": validation_pdf_y, \"test\": test_pdf_y},\n",
    "    results_dir=marginal_emissions_logs_directory,\n",
    "    file_prefix=marginal_emissions_prefix,\n",
    "    log_path=None,  # keep None when using rotating logs\n",
    "    global_extra_info={\"model_type\": \"median_binner\"},\n",
    "    force_run=False,\n",
    "    force_overwrite=False,\n",
    "    base_feature_pipeline_name=\"FeatureAdditionPipeline\",\n",
    "    eval_splits=(\"train\", \"validation\"),\n",
    "    max_log_mb=95,\n",
    "    fsync=False,   # set True on HPC if you want durable writes\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56c1f1f2",
   "metadata": {},
   "source": [
    "#### HPC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11902261",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[R0/0] [GRID 1/17298] qbin_3_surface_net_solar_radiation_kWh_per_m2__x_demand_met-demand_met_sqrd__fe_month__oobclip_rate0.05\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[506], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mrun_grid_search_auto\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbase_feature_pipeline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfeature_addition_pipeline\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43mregressor_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mGroupwiseRegressor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mregressor_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mregressor_kwargs_q\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrid_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmulti_quantile_param_grid\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mx_splits\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtrain\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_pdf_x_all\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mvalidation\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidation_pdf_x_all\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtest\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_pdf_x_all\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43my_splits\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtrain\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_pdf_y\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mvalidation\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidation_pdf_y\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtest\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_pdf_y\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43mresults_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmarginal_emissions_logs_directory\u001b[49m\u001b[43m,\u001b[49m\u001b[43m             \u001b[49m\u001b[38;5;66;43;03m# shared filesystem path\u001b[39;49;00m\n\u001b[1;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfile_prefix\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmarginal_emissions_prefix\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m_quantile\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m       \u001b[49m\u001b[38;5;66;43;03m# separate shard family\u001b[39;49;00m\n\u001b[1;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_log_mb\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m95\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfsync\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m                                                \u001b[49m\u001b[38;5;66;43;03m# durability on HPC\u001b[39;49;00m\n\u001b[1;32m     12\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbase_feature_pipeline_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mFeatureAdditionPipeline\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[43m    \u001b[49m\u001b[43meval_splits\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtrain\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mvalidation\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m                       \u001b[49m\u001b[38;5;66;43;03m# no test during tuning\u001b[39;49;00m\n\u001b[1;32m     14\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdistribute\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mauto\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m                                         \u001b[49m\u001b[38;5;66;43;03m# \"mpi\" if MPI present, else \"single\"\u001b[39;49;00m\n\u001b[1;32m     15\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdist_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstride\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m                                        \u001b[49m\u001b[38;5;66;43;03m# good default\u001b[39;49;00m\n\u001b[1;32m     16\u001b[0m \u001b[43m    \u001b[49m\u001b[43mforce_run\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[43m    \u001b[49m\u001b[43mforce_overwrite\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     18\u001b[0m \u001b[43m    \u001b[49m\u001b[43mseed\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m12\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     19\u001b[0m \u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[441], line 98\u001b[0m, in \u001b[0;36mrun_grid_search_auto\u001b[0;34m(base_feature_pipeline, regressor_cls, regressor_kwargs, grid_config, x_splits, y_splits, results_dir, file_prefix, max_log_mb, fsync, base_feature_pipeline_name, eval_splits, force_run, force_overwrite, distribute, dist_mode, seed)\u001b[0m\n\u001b[1;32m     95\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m distribute \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmpi\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m     96\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[MPI] rank=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrank\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00msize\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m assigned \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(local_configs)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m configs\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 98\u001b[0m \u001b[43mrun_grid_search\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     99\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbase_feature_pipeline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbase_feature_pipeline\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    100\u001b[0m \u001b[43m    \u001b[49m\u001b[43mregressor_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mregressor_cls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    101\u001b[0m \u001b[43m    \u001b[49m\u001b[43mregressor_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_reg_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    102\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrid_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_configs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    103\u001b[0m \u001b[43m    \u001b[49m\u001b[43mx_splits\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mx_splits\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    104\u001b[0m \u001b[43m    \u001b[49m\u001b[43my_splits\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43my_splits\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    105\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlog_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# legacy path unused when using rotating logs\u001b[39;49;00m\n\u001b[1;32m    106\u001b[0m \u001b[43m    \u001b[49m\u001b[43mglobal_extra_info\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrunner_rank\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mrank\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrunner_size\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43msize\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    107\u001b[0m \u001b[43m    \u001b[49m\u001b[43mforce_run\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_run\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    108\u001b[0m \u001b[43m    \u001b[49m\u001b[43mforce_overwrite\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_overwrite\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    109\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbase_feature_pipeline_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbase_feature_pipeline_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    110\u001b[0m \u001b[43m    \u001b[49m\u001b[43meval_splits\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meval_splits\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    111\u001b[0m \u001b[43m    \u001b[49m\u001b[43mresults_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresults_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    112\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfile_prefix\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfile_prefix\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    113\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_log_mb\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_log_mb\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    114\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfsync\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfsync\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    115\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    117\u001b[0m \u001b[38;5;66;03m# Optional barrier for neat logs\u001b[39;00m\n\u001b[1;32m    118\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "Cell \u001b[0;32mIn[440], line 116\u001b[0m, in \u001b[0;36mrun_grid_search\u001b[0;34m(base_feature_pipeline, regressor_cls, regressor_kwargs, grid_config, x_splits, y_splits, log_path, global_extra_info, force_run, force_overwrite, base_feature_pipeline_name, eval_splits, results_dir, file_prefix, max_log_mb, fsync)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mrank_tag\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m[GRID \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtotal\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m] \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlabel\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    115\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 116\u001b[0m     summary_df \u001b[38;5;241m=\u001b[39m \u001b[43mregressor_orchestrator\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    117\u001b[0m \u001b[43m        \u001b[49m\u001b[43muser_pipeline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfull_pipeline\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    118\u001b[0m \u001b[43m        \u001b[49m\u001b[43mx_splits\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mx_splits\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    119\u001b[0m \u001b[43m        \u001b[49m\u001b[43my_splits\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43my_splits\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    120\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlog_csv_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlog_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m            \u001b[49m\u001b[38;5;66;43;03m# legacy OK\u001b[39;49;00m\n\u001b[1;32m    121\u001b[0m \u001b[43m        \u001b[49m\u001b[43mextra_info\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_info\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    122\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforce_run\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_run\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    123\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforce_overwrite\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_overwrite\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    124\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrandom_state\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreg_kwargs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrandom_state\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    125\u001b[0m \u001b[43m        \u001b[49m\u001b[43meval_splits\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meval_splits\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    126\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# NEW\u001b[39;49;00m\n\u001b[1;32m    127\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresults_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresults_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    128\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfile_prefix\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfile_prefix\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    129\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_log_mb\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_log_mb\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    130\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfsync\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfsync\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    131\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    132\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m summary_df \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    133\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[GRID] Logged: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlabel\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[439], line 96\u001b[0m, in \u001b[0;36mregressor_orchestrator\u001b[0;34m(user_pipeline, x_splits, y_splits, log_csv_path, extra_info, force_run, force_overwrite, random_state, group_col_name, interval_hours, eval_splits, compute_test, results_dir, file_prefix, max_log_mb, fsync)\u001b[0m\n\u001b[1;32m     93\u001b[0m x_cols_used: \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mstr\u001b[39m] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     95\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m split \u001b[38;5;129;01min\u001b[39;00m eval_splits:\n\u001b[0;32m---> 96\u001b[0m     metrics_df, x_cols_used, extras \u001b[38;5;241m=\u001b[39m \u001b[43mrun_regressor_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     97\u001b[0m \u001b[43m        \u001b[49m\u001b[43muser_pipeline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muser_pipeline\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     98\u001b[0m \u001b[43m        \u001b[49m\u001b[43mx_df\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mx_splits\u001b[49m\u001b[43m[\u001b[49m\u001b[43msplit\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     99\u001b[0m \u001b[43m        \u001b[49m\u001b[43my_df\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43my_splits\u001b[49m\u001b[43m[\u001b[49m\u001b[43msplit\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    100\u001b[0m \u001b[43m        \u001b[49m\u001b[43msplit_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msplit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    101\u001b[0m \u001b[43m        \u001b[49m\u001b[43mextra_info\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_info\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    102\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_model\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    103\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrandom_state\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrandom_state\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    104\u001b[0m \u001b[43m        \u001b[49m\u001b[43minterval_hours\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minterval_hours\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    105\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel_id_hash\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_key\u001b[49m\u001b[43m,\u001b[49m\u001b[43m          \u001b[49m\u001b[38;5;66;43;03m# shared ID across splits\u001b[39;49;00m\n\u001b[1;32m    106\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparams_json_str\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparams_json_str\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# shared params JSON\u001b[39;49;00m\n\u001b[1;32m    107\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    108\u001b[0m     logs[split] \u001b[38;5;241m=\u001b[39m metrics_df\n\u001b[1;32m    109\u001b[0m     pooled_extras[split] \u001b[38;5;241m=\u001b[39m extras\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpooled_co2\u001b[39m\u001b[38;5;124m\"\u001b[39m, {})\n",
      "Cell \u001b[0;32mIn[438], line 91\u001b[0m, in \u001b[0;36mrun_regressor_model\u001b[0;34m(user_pipeline, x_df, y_df, split_name, extra_info, return_model, random_state, interval_hours, model_id_hash, params_json_str)\u001b[0m\n\u001b[1;32m     87\u001b[0m extras: \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Any] \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m     89\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m split_name \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m     90\u001b[0m     \u001b[38;5;66;03m# Fit → metrics from regressor\u001b[39;00m\n\u001b[0;32m---> 91\u001b[0m     _ \u001b[38;5;241m=\u001b[39m \u001b[43muser_pipeline\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_ser\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     92\u001b[0m     model \u001b[38;5;241m=\u001b[39m user_pipeline\u001b[38;5;241m.\u001b[39m_final_estimator  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[1;32m     93\u001b[0m     metrics_df \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mget_metrics(summarise\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\u001b[38;5;241m.\u001b[39mreset_index(drop\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/irpenv_4/lib/python3.10/site-packages/sklearn/base.py:1365\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1358\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[1;32m   1360\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[1;32m   1361\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m   1362\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[1;32m   1363\u001b[0m     )\n\u001b[1;32m   1364\u001b[0m ):\n\u001b[0;32m-> 1365\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/irpenv_4/lib/python3.10/site-packages/sklearn/pipeline.py:719\u001b[0m, in \u001b[0;36mPipeline.fit_transform\u001b[0;34m(self, X, y, **params)\u001b[0m\n\u001b[1;32m    680\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Fit the model and transform with the final estimator.\u001b[39;00m\n\u001b[1;32m    681\u001b[0m \n\u001b[1;32m    682\u001b[0m \u001b[38;5;124;03mFit all the transformers one after the other and sequentially transform\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    716\u001b[0m \u001b[38;5;124;03m    Transformed samples.\u001b[39;00m\n\u001b[1;32m    717\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    718\u001b[0m routed_params \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_method_params(method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfit_transform\u001b[39m\u001b[38;5;124m\"\u001b[39m, props\u001b[38;5;241m=\u001b[39mparams)\n\u001b[0;32m--> 719\u001b[0m Xt \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrouted_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    721\u001b[0m last_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_final_estimator\n\u001b[1;32m    722\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m _print_elapsed_time(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPipeline\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_log_message(\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msteps) \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m)):\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/irpenv_4/lib/python3.10/site-packages/sklearn/pipeline.py:589\u001b[0m, in \u001b[0;36mPipeline._fit\u001b[0;34m(self, X, y, routed_params, raw_params)\u001b[0m\n\u001b[1;32m    582\u001b[0m \u001b[38;5;66;03m# Fit or load from cache the current transformer\u001b[39;00m\n\u001b[1;32m    583\u001b[0m step_params \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_metadata_for_step(\n\u001b[1;32m    584\u001b[0m     step_idx\u001b[38;5;241m=\u001b[39mstep_idx,\n\u001b[1;32m    585\u001b[0m     step_params\u001b[38;5;241m=\u001b[39mrouted_params[name],\n\u001b[1;32m    586\u001b[0m     all_params\u001b[38;5;241m=\u001b[39mraw_params,\n\u001b[1;32m    587\u001b[0m )\n\u001b[0;32m--> 589\u001b[0m X, fitted_transformer \u001b[38;5;241m=\u001b[39m \u001b[43mfit_transform_one_cached\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    590\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcloned_transformer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    591\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    592\u001b[0m \u001b[43m    \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    593\u001b[0m \u001b[43m    \u001b[49m\u001b[43mweight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    594\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmessage_clsname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mPipeline\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    595\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmessage\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_log_message\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstep_idx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    596\u001b[0m \u001b[43m    \u001b[49m\u001b[43mparams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstep_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    597\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    598\u001b[0m \u001b[38;5;66;03m# Replace the transformer of the step with the fitted\u001b[39;00m\n\u001b[1;32m    599\u001b[0m \u001b[38;5;66;03m# transformer. This is necessary when loading the transformer\u001b[39;00m\n\u001b[1;32m    600\u001b[0m \u001b[38;5;66;03m# from the cache.\u001b[39;00m\n\u001b[1;32m    601\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msteps[step_idx] \u001b[38;5;241m=\u001b[39m (name, fitted_transformer)\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/irpenv_4/lib/python3.10/site-packages/joblib/memory.py:326\u001b[0m, in \u001b[0;36mNotMemorizedFunc.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 326\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/irpenv_4/lib/python3.10/site-packages/sklearn/pipeline.py:1540\u001b[0m, in \u001b[0;36m_fit_transform_one\u001b[0;34m(transformer, X, y, weight, message_clsname, message, params)\u001b[0m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m _print_elapsed_time(message_clsname, message):\n\u001b[1;32m   1539\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(transformer, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfit_transform\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m-> 1540\u001b[0m         res \u001b[38;5;241m=\u001b[39m \u001b[43mtransformer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfit_transform\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1541\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1542\u001b[0m         res \u001b[38;5;241m=\u001b[39m transformer\u001b[38;5;241m.\u001b[39mfit(X, y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfit\u001b[39m\u001b[38;5;124m\"\u001b[39m, {}))\u001b[38;5;241m.\u001b[39mtransform(\n\u001b[1;32m   1543\u001b[0m             X, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtransform\u001b[39m\u001b[38;5;124m\"\u001b[39m, {})\n\u001b[1;32m   1544\u001b[0m         )\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/irpenv_4/lib/python3.10/site-packages/sklearn/base.py:1365\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1358\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[1;32m   1360\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[1;32m   1361\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m   1362\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[1;32m   1363\u001b[0m     )\n\u001b[1;32m   1364\u001b[0m ):\n\u001b[0;32m-> 1365\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/irpenv_4/lib/python3.10/site-packages/sklearn/pipeline.py:731\u001b[0m, in \u001b[0;36mPipeline.fit_transform\u001b[0;34m(self, X, y, **params)\u001b[0m\n\u001b[1;32m    725\u001b[0m last_step_params \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_metadata_for_step(\n\u001b[1;32m    726\u001b[0m     step_idx\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m,\n\u001b[1;32m    727\u001b[0m     step_params\u001b[38;5;241m=\u001b[39mrouted_params[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msteps[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m][\u001b[38;5;241m0\u001b[39m]],\n\u001b[1;32m    728\u001b[0m     all_params\u001b[38;5;241m=\u001b[39mparams,\n\u001b[1;32m    729\u001b[0m )\n\u001b[1;32m    730\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(last_step, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfit_transform\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m--> 731\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mlast_step\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_transform\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    732\u001b[0m \u001b[43m        \u001b[49m\u001b[43mXt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mlast_step_params\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfit_transform\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[1;32m    733\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    734\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    735\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m last_step\u001b[38;5;241m.\u001b[39mfit(Xt, y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mlast_step_params[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfit\u001b[39m\u001b[38;5;124m\"\u001b[39m])\u001b[38;5;241m.\u001b[39mtransform(\n\u001b[1;32m    736\u001b[0m         Xt, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mlast_step_params[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtransform\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m    737\u001b[0m     )\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/irpenv_4/lib/python3.10/site-packages/sklearn/utils/_set_output.py:316\u001b[0m, in \u001b[0;36m_wrap_method_output.<locals>.wrapped\u001b[0;34m(self, X, *args, **kwargs)\u001b[0m\n\u001b[1;32m    314\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(f)\n\u001b[1;32m    315\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mwrapped\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 316\u001b[0m     data_to_wrap \u001b[38;5;241m=\u001b[39m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    317\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data_to_wrap, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[1;32m    318\u001b[0m         \u001b[38;5;66;03m# only wrap the first output for cross decomposition\u001b[39;00m\n\u001b[1;32m    319\u001b[0m         return_tuple \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    320\u001b[0m             _wrap_data_with_container(method, data_to_wrap[\u001b[38;5;241m0\u001b[39m], X, \u001b[38;5;28mself\u001b[39m),\n\u001b[1;32m    321\u001b[0m             \u001b[38;5;241m*\u001b[39mdata_to_wrap[\u001b[38;5;241m1\u001b[39m:],\n\u001b[1;32m    322\u001b[0m         )\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/irpenv_4/lib/python3.10/site-packages/sklearn/base.py:897\u001b[0m, in \u001b[0;36mTransformerMixin.fit_transform\u001b[0;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[1;32m    894\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfit(X, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_params)\u001b[38;5;241m.\u001b[39mtransform(X)\n\u001b[1;32m    895\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    896\u001b[0m     \u001b[38;5;66;03m# fit method of arity 2 (supervised transformation)\u001b[39;00m\n\u001b[0;32m--> 897\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfit_params\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/irpenv_4/lib/python3.10/site-packages/sklearn/utils/_set_output.py:316\u001b[0m, in \u001b[0;36m_wrap_method_output.<locals>.wrapped\u001b[0;34m(self, X, *args, **kwargs)\u001b[0m\n\u001b[1;32m    314\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(f)\n\u001b[1;32m    315\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mwrapped\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 316\u001b[0m     data_to_wrap \u001b[38;5;241m=\u001b[39m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    317\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data_to_wrap, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[1;32m    318\u001b[0m         \u001b[38;5;66;03m# only wrap the first output for cross decomposition\u001b[39;00m\n\u001b[1;32m    319\u001b[0m         return_tuple \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    320\u001b[0m             _wrap_data_with_container(method, data_to_wrap[\u001b[38;5;241m0\u001b[39m], X, \u001b[38;5;28mself\u001b[39m),\n\u001b[1;32m    321\u001b[0m             \u001b[38;5;241m*\u001b[39mdata_to_wrap[\u001b[38;5;241m1\u001b[39m:],\n\u001b[1;32m    322\u001b[0m         )\n",
      "Cell \u001b[0;32mIn[428], line 82\u001b[0m, in \u001b[0;36mAnalysisFeatureAdder.transform\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m     79\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mColumn \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtimestamp_col\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m contains non-parseable datetimes\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     81\u001b[0m \u001b[38;5;66;03m# temporal\u001b[39;00m\n\u001b[0;32m---> 82\u001b[0m df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtime_id\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mdf\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtimestamp_col\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstrftime\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m%\u001b[39;49m\u001b[38;5;124;43mH-\u001b[39;49m\u001b[38;5;124;43m%\u001b[39;49m\u001b[38;5;124;43mM\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstring\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     84\u001b[0m \u001b[38;5;66;03m# quantitative\u001b[39;00m\n\u001b[1;32m     85\u001b[0m q \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdemand_met_col\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/irpenv_4/lib/python3.10/site-packages/pandas/core/accessor.py:112\u001b[0m, in \u001b[0;36mPandasDelegate._add_delegate_accessors.<locals>._create_delegator_method.<locals>.f\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    111\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mf\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 112\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_delegate_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/irpenv_4/lib/python3.10/site-packages/pandas/core/indexes/accessors.py:132\u001b[0m, in \u001b[0;36mProperties._delegate_method\u001b[0;34m(self, name, *args, **kwargs)\u001b[0m\n\u001b[1;32m    129\u001b[0m values \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_values()\n\u001b[1;32m    131\u001b[0m method \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(values, name)\n\u001b[0;32m--> 132\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mmethod\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    134\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_list_like(result):\n\u001b[1;32m    135\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/irpenv_4/lib/python3.10/site-packages/pandas/core/indexes/datetimes.py:278\u001b[0m, in \u001b[0;36mDatetimeIndex.strftime\u001b[0;34m(self, date_format)\u001b[0m\n\u001b[1;32m    276\u001b[0m \u001b[38;5;129m@doc\u001b[39m(DatetimeArray\u001b[38;5;241m.\u001b[39mstrftime)\n\u001b[1;32m    277\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mstrftime\u001b[39m(\u001b[38;5;28mself\u001b[39m, date_format) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Index:\n\u001b[0;32m--> 278\u001b[0m     arr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_data\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstrftime\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdate_format\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    279\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m Index(arr, name\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname, dtype\u001b[38;5;241m=\u001b[39marr\u001b[38;5;241m.\u001b[39mdtype)\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/irpenv_4/lib/python3.10/site-packages/pandas/core/arrays/datetimelike.py:1814\u001b[0m, in \u001b[0;36mDatelikeOps.strftime\u001b[0;34m(self, date_format)\u001b[0m\n\u001b[1;32m   1765\u001b[0m \u001b[38;5;129m@Substitution\u001b[39m(\n\u001b[1;32m   1766\u001b[0m     URL\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://docs.python.org/3/library/datetime.html\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1767\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m#strftime-and-strptime-behavior\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1768\u001b[0m )\n\u001b[1;32m   1769\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mstrftime\u001b[39m(\u001b[38;5;28mself\u001b[39m, date_format: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m npt\u001b[38;5;241m.\u001b[39mNDArray[np\u001b[38;5;241m.\u001b[39mobject_]:\n\u001b[1;32m   1770\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1771\u001b[0m \u001b[38;5;124;03m    Convert to Index using specified date_format.\u001b[39;00m\n\u001b[1;32m   1772\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1812\u001b[0m \u001b[38;5;124;03m          dtype='object')\u001b[39;00m\n\u001b[1;32m   1813\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1814\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_format_native_types\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdate_format\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdate_format\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mna_rep\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnan\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1815\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m using_string_dtype():\n\u001b[1;32m   1816\u001b[0m         \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m StringDtype\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/irpenv_4/lib/python3.10/site-packages/pandas/core/arrays/datetimes.py:753\u001b[0m, in \u001b[0;36mDatetimeArray._format_native_types\u001b[0;34m(self, na_rep, date_format, **kwargs)\u001b[0m\n\u001b[1;32m    749\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m date_format \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_is_dates_only:\n\u001b[1;32m    750\u001b[0m     \u001b[38;5;66;03m# Only dates and no timezone: provide a default format\u001b[39;00m\n\u001b[1;32m    751\u001b[0m     date_format \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mY-\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mm-\u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 753\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtslib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mformat_array_from_datetime\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    754\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43masi8\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtz\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtz\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mformat\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdate_format\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mna_rep\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mna_rep\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreso\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_creso\u001b[49m\n\u001b[1;32m    755\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32mpandas/_libs/tslib.pyx:222\u001b[0m, in \u001b[0;36mpandas._libs.tslib.format_array_from_datetime\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/tslibs/timestamps.pyx:1488\u001b[0m, in \u001b[0;36mpandas._libs.tslibs.timestamps.Timestamp.strftime\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/irpenv_4/lib/python3.10/site-packages/pytz/tzinfo.py:430\u001b[0m, in \u001b[0;36mDstTzInfo.dst\u001b[0;34m(self, dt, is_dst)\u001b[0m\n\u001b[1;32m    427\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    428\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_utcoffset\n\u001b[0;32m--> 430\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mdst\u001b[39m(\u001b[38;5;28mself\u001b[39m, dt, is_dst\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m    431\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m'''See datetime.tzinfo.dst\u001b[39;00m\n\u001b[1;32m    432\u001b[0m \n\u001b[1;32m    433\u001b[0m \u001b[38;5;124;03m    The is_dst parameter may be used to remove ambiguity during DST\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    459\u001b[0m \n\u001b[1;32m    460\u001b[0m \u001b[38;5;124;03m    '''\u001b[39;00m\n\u001b[1;32m    461\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m dt \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# run_grid_search_auto(\n",
    "#     base_feature_pipeline=feature_addition_pipeline,\n",
    "#     regressor_cls=GroupwiseRegressor,\n",
    "#     regressor_kwargs=regressor_kwargs_q,\n",
    "#     grid_config=multi_quantile_param_grid,\n",
    "#     x_splits={\"train\": train_pdf_x_all, \"validation\": validation_pdf_x_all, \"test\": test_pdf_x_all},\n",
    "#     y_splits={\"train\": train_pdf_y, \"validation\": validation_pdf_y, \"test\": test_pdf_y},\n",
    "#     results_dir=marginal_emissions_logs_directory,             # shared filesystem path\n",
    "#     file_prefix=marginal_emissions_prefix + \"_quantile\",       # separate shard family\n",
    "#     max_log_mb=95,\n",
    "#     fsync=True,                                                # durability on HPC\n",
    "#     base_feature_pipeline_name=\"FeatureAdditionPipeline\",\n",
    "#     eval_splits=(\"train\", \"validation\"),                       # no test during tuning\n",
    "#     distribute=\"auto\",                                         # \"mpi\" if MPI present, else \"single\"\n",
    "#     dist_mode=\"stride\",                                        # good default\n",
    "#     force_run=False,\n",
    "#     force_overwrite=False,\n",
    "#     seed=12,\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25c50dee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[R0/0] [GRID 1/961] median_surface_net_solar_radiation_kWh_per_m2__x_demand_met-demand_met_sqrd__fe_month\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[508], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mrun_grid_search_auto\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbase_feature_pipeline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfeature_addition_pipeline\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43mregressor_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mGroupwiseRegressor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mregressor_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mregressor_kwargs_m\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrid_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmulti_median_param_grid\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mx_splits\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtrain\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_pdf_x_all\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mvalidation\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidation_pdf_x_all\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtest\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_pdf_x_all\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43my_splits\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtrain\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_pdf_y\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mvalidation\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidation_pdf_y\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtest\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_pdf_y\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43mresults_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmarginal_emissions_logs_directory\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfile_prefix\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmarginal_emissions_prefix\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m_median\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_log_mb\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m95\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfsync\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbase_feature_pipeline_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mFeatureAdditionPipeline\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[43m    \u001b[49m\u001b[43meval_splits\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtrain\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mvalidation\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdistribute\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mauto\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdist_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstride\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[43m    \u001b[49m\u001b[43mforce_run\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[43m    \u001b[49m\u001b[43mforce_overwrite\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     18\u001b[0m \u001b[43m    \u001b[49m\u001b[43mseed\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m12\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     19\u001b[0m \u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[441], line 98\u001b[0m, in \u001b[0;36mrun_grid_search_auto\u001b[0;34m(base_feature_pipeline, regressor_cls, regressor_kwargs, grid_config, x_splits, y_splits, results_dir, file_prefix, max_log_mb, fsync, base_feature_pipeline_name, eval_splits, force_run, force_overwrite, distribute, dist_mode, seed)\u001b[0m\n\u001b[1;32m     95\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m distribute \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmpi\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m     96\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[MPI] rank=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrank\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00msize\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m assigned \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(local_configs)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m configs\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 98\u001b[0m \u001b[43mrun_grid_search\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     99\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbase_feature_pipeline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbase_feature_pipeline\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    100\u001b[0m \u001b[43m    \u001b[49m\u001b[43mregressor_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mregressor_cls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    101\u001b[0m \u001b[43m    \u001b[49m\u001b[43mregressor_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_reg_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    102\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrid_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_configs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    103\u001b[0m \u001b[43m    \u001b[49m\u001b[43mx_splits\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mx_splits\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    104\u001b[0m \u001b[43m    \u001b[49m\u001b[43my_splits\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43my_splits\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    105\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlog_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# legacy path unused when using rotating logs\u001b[39;49;00m\n\u001b[1;32m    106\u001b[0m \u001b[43m    \u001b[49m\u001b[43mglobal_extra_info\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrunner_rank\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mrank\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrunner_size\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43msize\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    107\u001b[0m \u001b[43m    \u001b[49m\u001b[43mforce_run\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_run\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    108\u001b[0m \u001b[43m    \u001b[49m\u001b[43mforce_overwrite\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_overwrite\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    109\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbase_feature_pipeline_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbase_feature_pipeline_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    110\u001b[0m \u001b[43m    \u001b[49m\u001b[43meval_splits\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meval_splits\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    111\u001b[0m \u001b[43m    \u001b[49m\u001b[43mresults_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresults_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    112\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfile_prefix\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfile_prefix\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    113\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_log_mb\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_log_mb\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    114\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfsync\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfsync\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    115\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    117\u001b[0m \u001b[38;5;66;03m# Optional barrier for neat logs\u001b[39;00m\n\u001b[1;32m    118\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "Cell \u001b[0;32mIn[440], line 116\u001b[0m, in \u001b[0;36mrun_grid_search\u001b[0;34m(base_feature_pipeline, regressor_cls, regressor_kwargs, grid_config, x_splits, y_splits, log_path, global_extra_info, force_run, force_overwrite, base_feature_pipeline_name, eval_splits, results_dir, file_prefix, max_log_mb, fsync)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mrank_tag\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m[GRID \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtotal\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m] \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlabel\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    115\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 116\u001b[0m     summary_df \u001b[38;5;241m=\u001b[39m \u001b[43mregressor_orchestrator\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    117\u001b[0m \u001b[43m        \u001b[49m\u001b[43muser_pipeline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfull_pipeline\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    118\u001b[0m \u001b[43m        \u001b[49m\u001b[43mx_splits\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mx_splits\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    119\u001b[0m \u001b[43m        \u001b[49m\u001b[43my_splits\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43my_splits\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    120\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlog_csv_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlog_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m            \u001b[49m\u001b[38;5;66;43;03m# legacy OK\u001b[39;49;00m\n\u001b[1;32m    121\u001b[0m \u001b[43m        \u001b[49m\u001b[43mextra_info\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_info\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    122\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforce_run\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_run\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    123\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforce_overwrite\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_overwrite\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    124\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrandom_state\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreg_kwargs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrandom_state\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    125\u001b[0m \u001b[43m        \u001b[49m\u001b[43meval_splits\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meval_splits\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    126\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# NEW\u001b[39;49;00m\n\u001b[1;32m    127\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresults_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresults_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    128\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfile_prefix\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfile_prefix\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    129\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_log_mb\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_log_mb\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    130\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfsync\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfsync\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    131\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    132\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m summary_df \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    133\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[GRID] Logged: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlabel\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[439], line 96\u001b[0m, in \u001b[0;36mregressor_orchestrator\u001b[0;34m(user_pipeline, x_splits, y_splits, log_csv_path, extra_info, force_run, force_overwrite, random_state, group_col_name, interval_hours, eval_splits, compute_test, results_dir, file_prefix, max_log_mb, fsync)\u001b[0m\n\u001b[1;32m     93\u001b[0m x_cols_used: \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mstr\u001b[39m] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     95\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m split \u001b[38;5;129;01min\u001b[39;00m eval_splits:\n\u001b[0;32m---> 96\u001b[0m     metrics_df, x_cols_used, extras \u001b[38;5;241m=\u001b[39m \u001b[43mrun_regressor_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     97\u001b[0m \u001b[43m        \u001b[49m\u001b[43muser_pipeline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muser_pipeline\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     98\u001b[0m \u001b[43m        \u001b[49m\u001b[43mx_df\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mx_splits\u001b[49m\u001b[43m[\u001b[49m\u001b[43msplit\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     99\u001b[0m \u001b[43m        \u001b[49m\u001b[43my_df\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43my_splits\u001b[49m\u001b[43m[\u001b[49m\u001b[43msplit\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    100\u001b[0m \u001b[43m        \u001b[49m\u001b[43msplit_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msplit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    101\u001b[0m \u001b[43m        \u001b[49m\u001b[43mextra_info\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_info\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    102\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_model\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    103\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrandom_state\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrandom_state\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    104\u001b[0m \u001b[43m        \u001b[49m\u001b[43minterval_hours\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minterval_hours\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    105\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel_id_hash\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_key\u001b[49m\u001b[43m,\u001b[49m\u001b[43m          \u001b[49m\u001b[38;5;66;43;03m# shared ID across splits\u001b[39;49;00m\n\u001b[1;32m    106\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparams_json_str\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparams_json_str\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# shared params JSON\u001b[39;49;00m\n\u001b[1;32m    107\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    108\u001b[0m     logs[split] \u001b[38;5;241m=\u001b[39m metrics_df\n\u001b[1;32m    109\u001b[0m     pooled_extras[split] \u001b[38;5;241m=\u001b[39m extras\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpooled_co2\u001b[39m\u001b[38;5;124m\"\u001b[39m, {})\n",
      "Cell \u001b[0;32mIn[438], line 91\u001b[0m, in \u001b[0;36mrun_regressor_model\u001b[0;34m(user_pipeline, x_df, y_df, split_name, extra_info, return_model, random_state, interval_hours, model_id_hash, params_json_str)\u001b[0m\n\u001b[1;32m     87\u001b[0m extras: \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Any] \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m     89\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m split_name \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m     90\u001b[0m     \u001b[38;5;66;03m# Fit → metrics from regressor\u001b[39;00m\n\u001b[0;32m---> 91\u001b[0m     _ \u001b[38;5;241m=\u001b[39m \u001b[43muser_pipeline\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_ser\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     92\u001b[0m     model \u001b[38;5;241m=\u001b[39m user_pipeline\u001b[38;5;241m.\u001b[39m_final_estimator  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[1;32m     93\u001b[0m     metrics_df \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mget_metrics(summarise\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\u001b[38;5;241m.\u001b[39mreset_index(drop\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/irpenv_4/lib/python3.10/site-packages/sklearn/base.py:1365\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1358\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[1;32m   1360\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[1;32m   1361\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m   1362\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[1;32m   1363\u001b[0m     )\n\u001b[1;32m   1364\u001b[0m ):\n\u001b[0;32m-> 1365\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/irpenv_4/lib/python3.10/site-packages/sklearn/pipeline.py:719\u001b[0m, in \u001b[0;36mPipeline.fit_transform\u001b[0;34m(self, X, y, **params)\u001b[0m\n\u001b[1;32m    680\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Fit the model and transform with the final estimator.\u001b[39;00m\n\u001b[1;32m    681\u001b[0m \n\u001b[1;32m    682\u001b[0m \u001b[38;5;124;03mFit all the transformers one after the other and sequentially transform\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    716\u001b[0m \u001b[38;5;124;03m    Transformed samples.\u001b[39;00m\n\u001b[1;32m    717\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    718\u001b[0m routed_params \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_method_params(method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfit_transform\u001b[39m\u001b[38;5;124m\"\u001b[39m, props\u001b[38;5;241m=\u001b[39mparams)\n\u001b[0;32m--> 719\u001b[0m Xt \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrouted_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    721\u001b[0m last_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_final_estimator\n\u001b[1;32m    722\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m _print_elapsed_time(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPipeline\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_log_message(\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msteps) \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m)):\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/irpenv_4/lib/python3.10/site-packages/sklearn/pipeline.py:589\u001b[0m, in \u001b[0;36mPipeline._fit\u001b[0;34m(self, X, y, routed_params, raw_params)\u001b[0m\n\u001b[1;32m    582\u001b[0m \u001b[38;5;66;03m# Fit or load from cache the current transformer\u001b[39;00m\n\u001b[1;32m    583\u001b[0m step_params \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_metadata_for_step(\n\u001b[1;32m    584\u001b[0m     step_idx\u001b[38;5;241m=\u001b[39mstep_idx,\n\u001b[1;32m    585\u001b[0m     step_params\u001b[38;5;241m=\u001b[39mrouted_params[name],\n\u001b[1;32m    586\u001b[0m     all_params\u001b[38;5;241m=\u001b[39mraw_params,\n\u001b[1;32m    587\u001b[0m )\n\u001b[0;32m--> 589\u001b[0m X, fitted_transformer \u001b[38;5;241m=\u001b[39m \u001b[43mfit_transform_one_cached\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    590\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcloned_transformer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    591\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    592\u001b[0m \u001b[43m    \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    593\u001b[0m \u001b[43m    \u001b[49m\u001b[43mweight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    594\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmessage_clsname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mPipeline\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    595\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmessage\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_log_message\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstep_idx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    596\u001b[0m \u001b[43m    \u001b[49m\u001b[43mparams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstep_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    597\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    598\u001b[0m \u001b[38;5;66;03m# Replace the transformer of the step with the fitted\u001b[39;00m\n\u001b[1;32m    599\u001b[0m \u001b[38;5;66;03m# transformer. This is necessary when loading the transformer\u001b[39;00m\n\u001b[1;32m    600\u001b[0m \u001b[38;5;66;03m# from the cache.\u001b[39;00m\n\u001b[1;32m    601\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msteps[step_idx] \u001b[38;5;241m=\u001b[39m (name, fitted_transformer)\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/irpenv_4/lib/python3.10/site-packages/joblib/memory.py:326\u001b[0m, in \u001b[0;36mNotMemorizedFunc.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 326\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/irpenv_4/lib/python3.10/site-packages/sklearn/pipeline.py:1540\u001b[0m, in \u001b[0;36m_fit_transform_one\u001b[0;34m(transformer, X, y, weight, message_clsname, message, params)\u001b[0m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m _print_elapsed_time(message_clsname, message):\n\u001b[1;32m   1539\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(transformer, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfit_transform\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m-> 1540\u001b[0m         res \u001b[38;5;241m=\u001b[39m \u001b[43mtransformer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfit_transform\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1541\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1542\u001b[0m         res \u001b[38;5;241m=\u001b[39m transformer\u001b[38;5;241m.\u001b[39mfit(X, y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfit\u001b[39m\u001b[38;5;124m\"\u001b[39m, {}))\u001b[38;5;241m.\u001b[39mtransform(\n\u001b[1;32m   1543\u001b[0m             X, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtransform\u001b[39m\u001b[38;5;124m\"\u001b[39m, {})\n\u001b[1;32m   1544\u001b[0m         )\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/irpenv_4/lib/python3.10/site-packages/sklearn/base.py:1365\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1358\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[1;32m   1360\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[1;32m   1361\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m   1362\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[1;32m   1363\u001b[0m     )\n\u001b[1;32m   1364\u001b[0m ):\n\u001b[0;32m-> 1365\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/irpenv_4/lib/python3.10/site-packages/sklearn/pipeline.py:731\u001b[0m, in \u001b[0;36mPipeline.fit_transform\u001b[0;34m(self, X, y, **params)\u001b[0m\n\u001b[1;32m    725\u001b[0m last_step_params \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_metadata_for_step(\n\u001b[1;32m    726\u001b[0m     step_idx\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m,\n\u001b[1;32m    727\u001b[0m     step_params\u001b[38;5;241m=\u001b[39mrouted_params[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msteps[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m][\u001b[38;5;241m0\u001b[39m]],\n\u001b[1;32m    728\u001b[0m     all_params\u001b[38;5;241m=\u001b[39mparams,\n\u001b[1;32m    729\u001b[0m )\n\u001b[1;32m    730\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(last_step, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfit_transform\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m--> 731\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mlast_step\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_transform\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    732\u001b[0m \u001b[43m        \u001b[49m\u001b[43mXt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mlast_step_params\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfit_transform\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[1;32m    733\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    734\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    735\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m last_step\u001b[38;5;241m.\u001b[39mfit(Xt, y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mlast_step_params[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfit\u001b[39m\u001b[38;5;124m\"\u001b[39m])\u001b[38;5;241m.\u001b[39mtransform(\n\u001b[1;32m    736\u001b[0m         Xt, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mlast_step_params[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtransform\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m    737\u001b[0m     )\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/irpenv_4/lib/python3.10/site-packages/sklearn/utils/_set_output.py:316\u001b[0m, in \u001b[0;36m_wrap_method_output.<locals>.wrapped\u001b[0;34m(self, X, *args, **kwargs)\u001b[0m\n\u001b[1;32m    314\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(f)\n\u001b[1;32m    315\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mwrapped\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 316\u001b[0m     data_to_wrap \u001b[38;5;241m=\u001b[39m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    317\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data_to_wrap, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[1;32m    318\u001b[0m         \u001b[38;5;66;03m# only wrap the first output for cross decomposition\u001b[39;00m\n\u001b[1;32m    319\u001b[0m         return_tuple \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    320\u001b[0m             _wrap_data_with_container(method, data_to_wrap[\u001b[38;5;241m0\u001b[39m], X, \u001b[38;5;28mself\u001b[39m),\n\u001b[1;32m    321\u001b[0m             \u001b[38;5;241m*\u001b[39mdata_to_wrap[\u001b[38;5;241m1\u001b[39m:],\n\u001b[1;32m    322\u001b[0m         )\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/irpenv_4/lib/python3.10/site-packages/sklearn/base.py:897\u001b[0m, in \u001b[0;36mTransformerMixin.fit_transform\u001b[0;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[1;32m    894\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfit(X, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_params)\u001b[38;5;241m.\u001b[39mtransform(X)\n\u001b[1;32m    895\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    896\u001b[0m     \u001b[38;5;66;03m# fit method of arity 2 (supervised transformation)\u001b[39;00m\n\u001b[0;32m--> 897\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfit_params\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/irpenv_4/lib/python3.10/site-packages/sklearn/utils/_set_output.py:316\u001b[0m, in \u001b[0;36m_wrap_method_output.<locals>.wrapped\u001b[0;34m(self, X, *args, **kwargs)\u001b[0m\n\u001b[1;32m    314\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(f)\n\u001b[1;32m    315\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mwrapped\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 316\u001b[0m     data_to_wrap \u001b[38;5;241m=\u001b[39m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    317\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data_to_wrap, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[1;32m    318\u001b[0m         \u001b[38;5;66;03m# only wrap the first output for cross decomposition\u001b[39;00m\n\u001b[1;32m    319\u001b[0m         return_tuple \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    320\u001b[0m             _wrap_data_with_container(method, data_to_wrap[\u001b[38;5;241m0\u001b[39m], X, \u001b[38;5;28mself\u001b[39m),\n\u001b[1;32m    321\u001b[0m             \u001b[38;5;241m*\u001b[39mdata_to_wrap[\u001b[38;5;241m1\u001b[39m:],\n\u001b[1;32m    322\u001b[0m         )\n",
      "Cell \u001b[0;32mIn[428], line 82\u001b[0m, in \u001b[0;36mAnalysisFeatureAdder.transform\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m     79\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mColumn \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtimestamp_col\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m contains non-parseable datetimes\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     81\u001b[0m \u001b[38;5;66;03m# temporal\u001b[39;00m\n\u001b[0;32m---> 82\u001b[0m df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtime_id\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mdf\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtimestamp_col\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstrftime\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m%\u001b[39;49m\u001b[38;5;124;43mH-\u001b[39;49m\u001b[38;5;124;43m%\u001b[39;49m\u001b[38;5;124;43mM\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstring\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     84\u001b[0m \u001b[38;5;66;03m# quantitative\u001b[39;00m\n\u001b[1;32m     85\u001b[0m q \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdemand_met_col\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/irpenv_4/lib/python3.10/site-packages/pandas/core/accessor.py:112\u001b[0m, in \u001b[0;36mPandasDelegate._add_delegate_accessors.<locals>._create_delegator_method.<locals>.f\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    111\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mf\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 112\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_delegate_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/irpenv_4/lib/python3.10/site-packages/pandas/core/indexes/accessors.py:132\u001b[0m, in \u001b[0;36mProperties._delegate_method\u001b[0;34m(self, name, *args, **kwargs)\u001b[0m\n\u001b[1;32m    129\u001b[0m values \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_values()\n\u001b[1;32m    131\u001b[0m method \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(values, name)\n\u001b[0;32m--> 132\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mmethod\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    134\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_list_like(result):\n\u001b[1;32m    135\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/irpenv_4/lib/python3.10/site-packages/pandas/core/indexes/datetimes.py:278\u001b[0m, in \u001b[0;36mDatetimeIndex.strftime\u001b[0;34m(self, date_format)\u001b[0m\n\u001b[1;32m    276\u001b[0m \u001b[38;5;129m@doc\u001b[39m(DatetimeArray\u001b[38;5;241m.\u001b[39mstrftime)\n\u001b[1;32m    277\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mstrftime\u001b[39m(\u001b[38;5;28mself\u001b[39m, date_format) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Index:\n\u001b[0;32m--> 278\u001b[0m     arr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_data\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstrftime\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdate_format\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    279\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m Index(arr, name\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname, dtype\u001b[38;5;241m=\u001b[39marr\u001b[38;5;241m.\u001b[39mdtype)\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/irpenv_4/lib/python3.10/site-packages/pandas/core/arrays/datetimelike.py:1814\u001b[0m, in \u001b[0;36mDatelikeOps.strftime\u001b[0;34m(self, date_format)\u001b[0m\n\u001b[1;32m   1765\u001b[0m \u001b[38;5;129m@Substitution\u001b[39m(\n\u001b[1;32m   1766\u001b[0m     URL\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://docs.python.org/3/library/datetime.html\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1767\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m#strftime-and-strptime-behavior\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1768\u001b[0m )\n\u001b[1;32m   1769\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mstrftime\u001b[39m(\u001b[38;5;28mself\u001b[39m, date_format: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m npt\u001b[38;5;241m.\u001b[39mNDArray[np\u001b[38;5;241m.\u001b[39mobject_]:\n\u001b[1;32m   1770\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1771\u001b[0m \u001b[38;5;124;03m    Convert to Index using specified date_format.\u001b[39;00m\n\u001b[1;32m   1772\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1812\u001b[0m \u001b[38;5;124;03m          dtype='object')\u001b[39;00m\n\u001b[1;32m   1813\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1814\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_format_native_types\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdate_format\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdate_format\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mna_rep\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnan\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1815\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m using_string_dtype():\n\u001b[1;32m   1816\u001b[0m         \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m StringDtype\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/irpenv_4/lib/python3.10/site-packages/pandas/core/arrays/datetimes.py:753\u001b[0m, in \u001b[0;36mDatetimeArray._format_native_types\u001b[0;34m(self, na_rep, date_format, **kwargs)\u001b[0m\n\u001b[1;32m    749\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m date_format \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_is_dates_only:\n\u001b[1;32m    750\u001b[0m     \u001b[38;5;66;03m# Only dates and no timezone: provide a default format\u001b[39;00m\n\u001b[1;32m    751\u001b[0m     date_format \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mY-\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mm-\u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 753\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtslib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mformat_array_from_datetime\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    754\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43masi8\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtz\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtz\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mformat\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdate_format\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mna_rep\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mna_rep\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreso\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_creso\u001b[49m\n\u001b[1;32m    755\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32mpandas/_libs/tslib.pyx:222\u001b[0m, in \u001b[0;36mpandas._libs.tslib.format_array_from_datetime\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/tslibs/timestamps.pyx:1488\u001b[0m, in \u001b[0;36mpandas._libs.tslibs.timestamps.Timestamp.strftime\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/irpenv_4/lib/python3.10/site-packages/pytz/tzinfo.py:430\u001b[0m, in \u001b[0;36mDstTzInfo.dst\u001b[0;34m(self, dt, is_dst)\u001b[0m\n\u001b[1;32m    427\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    428\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_utcoffset\n\u001b[0;32m--> 430\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mdst\u001b[39m(\u001b[38;5;28mself\u001b[39m, dt, is_dst\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m    431\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m'''See datetime.tzinfo.dst\u001b[39;00m\n\u001b[1;32m    432\u001b[0m \n\u001b[1;32m    433\u001b[0m \u001b[38;5;124;03m    The is_dst parameter may be used to remove ambiguity during DST\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    459\u001b[0m \n\u001b[1;32m    460\u001b[0m \u001b[38;5;124;03m    '''\u001b[39;00m\n\u001b[1;32m    461\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m dt \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# run_grid_search_auto(\n",
    "#     base_feature_pipeline=feature_addition_pipeline,\n",
    "#     regressor_cls=GroupwiseRegressor,\n",
    "#     regressor_kwargs=regressor_kwargs_m,\n",
    "#     grid_config=multi_median_param_grid,\n",
    "#     x_splits={\"train\": train_pdf_x_all, \"validation\": validation_pdf_x_all, \"test\": test_pdf_x_all},\n",
    "#     y_splits={\"train\": train_pdf_y, \"validation\": validation_pdf_y, \"test\": test_pdf_y},\n",
    "#     results_dir=marginal_emissions_logs_directory,\n",
    "#     file_prefix=marginal_emissions_prefix + \"_median\",\n",
    "#     max_log_mb=95,\n",
    "#     fsync=True,\n",
    "#     base_feature_pipeline_name=\"FeatureAdditionPipeline\",\n",
    "#     eval_splits=(\"train\", \"validation\"),\n",
    "#     distribute=\"auto\",\n",
    "#     dist_mode=\"stride\",\n",
    "#     force_run=False,\n",
    "#     force_overwrite=False,\n",
    "#     seed=12,\n",
    "# )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "irpenv_4",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
