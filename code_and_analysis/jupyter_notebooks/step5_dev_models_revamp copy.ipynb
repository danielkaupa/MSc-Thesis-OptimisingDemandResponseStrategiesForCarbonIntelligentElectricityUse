{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9458ba17",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "78ee6334",
   "metadata": {},
   "source": [
    "# Developing Optimisation Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be48e05c",
   "metadata": {},
   "source": [
    "### Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eabd094f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ────────────────────────────────────────────────────────────────────────────\n",
    "# Future (must be first)\n",
    "# ────────────────────────────────────────────────────────────────────────────\n",
    "from __future__ import annotations\n",
    "\n",
    "# ────────────────────────────────────────────────────────────────────────────\n",
    "# Jupyter/Notebook Setup\n",
    "# ────────────────────────────────────────────────────────────────────────────\n",
    "%matplotlib inline\n",
    "from IPython.display import display\n",
    "\n",
    "# ────────────────────────────────────────────────────────────────────────────\n",
    "# Standard Library\n",
    "# ────────────────────────────────────────────────────────────────────────────\n",
    "import os\n",
    "import time\n",
    "from copy import deepcopy\n",
    "from dataclasses import dataclass, asdict\n",
    "from datetime import datetime, timedelta\n",
    "from functools import lru_cache, partial\n",
    "from typing import (\n",
    "    Any, Callable, Dict, List, Optional, Sequence, Tuple, Union, Literal\n",
    ")\n",
    "\n",
    "# ────────────────────────────────────────────────────────────────────────────\n",
    "# Optimisation / OR libraries\n",
    "# ────────────────────────────────────────────────────────────────────────────\n",
    "import pyomo.environ as pyo\n",
    "# import cvxpy as cp   # only if you plan to run LP relaxations\n",
    "\n",
    "# ────────────────────────────────────────────────────────────────────────────\n",
    "# Core Data Handling\n",
    "# ────────────────────────────────────────────────────────────────────────────\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import polars as pl\n",
    "\n",
    "# ────────────────────────────────────────────────────────────────────────────\n",
    "# Visualization (optional)\n",
    "# ────────────────────────────────────────────────────────────────────────────\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b515dc3",
   "metadata": {},
   "source": [
    "Awesome—here’s your **clean, updated constraint set** reflecting the changes you just decided:\n",
    "\n",
    "# Behavioral constraints\n",
    "\n",
    "1. **Max moves per customer per day**\n",
    "\n",
    "* **Constraint:** `moves_per_customer_per_day ≤ X` (default X=1)\n",
    "* **Why:** Minimise disruption and alert fatigue.\n",
    "* **Params:** `X`, `timezone`, `day_boundaries` (default 00:00–24:00), `week_boundaries` (Mon–Sun or ISO)\n",
    "\n",
    "2. **Max moves per customer per week**\n",
    "\n",
    "* **Constraint:** `moves_per_customer_per_week ≤ Y` (default Y=3)\n",
    "* **Why:** Prevent repeated nudges.\n",
    "* **Params:** `Y`, `timezone`, `week_boundaries`\n",
    "\n",
    "3. **Max shift window**\n",
    "\n",
    "* **Constraint:** `|t_original − t_shifted| ≤ H` (inclusive)\n",
    "* **Why:** Keep shifts realistic.\n",
    "* **Params:** `H_hours` (e.g., 2h), `slot_length_minutes` (default 30), `inclusive=True`\n",
    "\n",
    "4. **Peak-hour comfort (per-slot or per-hour grouping)**\n",
    "\n",
    "* **Constraint (per-slot form):** For each peak slot `s`,\n",
    "  `post_usage_customer(s) ≥ (1 − Z) · baseline_customer(s)`\n",
    "  (i.e., **no more than Z% reduction** in that customer’s usage in that peak slot/hour)\n",
    "* **Why:** Maintain comfort in peak periods.\n",
    "* **Params:**\n",
    "\n",
    "  * `Z_percent` (default 30%)\n",
    "  * `scope=\"per_customer\"` (your choice)\n",
    "  * `peak_hours` defined **per city** and **weekday** as full hours; helper expands to 30-min slots\n",
    "  * `cap_mode ∈ { \"slot\", \"hour\" }` (if `\"hour\"`, treat both half-hour slots within the hour jointly)\n",
    "\n",
    "# Practical technical constraints\n",
    "\n",
    "1. **Regional maximum shift (per city-day)**\n",
    "\n",
    "* **Constraint:** `total_moved_kWh(city, day) ≤ P% × regional_total_daily_average_load_kWh(city)`\n",
    "* **Why:** Avoid excessive system perturbation.\n",
    "* **Implementation:** **City-day budget with running residual**; each customer-day solve receives a `moved_kwh_cap = residual_budget`.\n",
    "* **Params:** `P_percent` (default 10%), `regional_total_daily_average_load_kWh={city: kWh}`\n",
    "\n",
    "2. **Household minimum usage per slot**\n",
    "\n",
    "* **Constraint:** `post_usage_customer(s) ≥ max( min_baseline(customer,s), R% × robust_max(customer,s) )`\n",
    "* **Why:** Preserve essential usage.\n",
    "* **Computation:** Per **customer** and **slot** (stratify by hour-of-day & day-of-week). Recommend **precompute** in Polars and attach `floor_kwh`.\n",
    "* **Params:**\n",
    "\n",
    "  * `baseline_period ∈ {year, month, week, day}`\n",
    "  * `baseline_type ∈ {average, absolute_min}`\n",
    "  * `robust_max_percentile` (default 95)\n",
    "  * `R_percent` (default 10)\n",
    "  * `epsilon_floor_kWh` (small positive to avoid zero)\n",
    "\n",
    "3. **No spiking (per-city per-slot cap vs baseline)**\n",
    "\n",
    "* **Constraint:** For each slot `s` in a city-day,\n",
    "  `post_city_usage(s) ≤ (1 + α%) × baseline_city_usage(s)`\n",
    "* **Why:** Prevent creating new peaks by piling into a single low-MEF slot.\n",
    "* **Implementation:** **City-day per-slot residual capacity**. Compute `baseline_city_usage(s)` once, set cap to `(1+α%)·baseline`, maintain a **running residual** and pass **per-destination upper bounds** into each customer solve.\n",
    "* **Params:** `alpha_peak_cap_percent` (default 25%)\n",
    "\n",
    "# Hardcoded constraints\n",
    "\n",
    "1. **Total consumption conservation (per customer)**\n",
    "\n",
    "* **Constraint:** `Σ_s post_usage_customer(s) = Σ_s original_usage_customer(s)` within the conservation horizon (default **day**).\n",
    "* **Params:** `id_field=\"ca_id\"`, `conservation_horizon ∈ {day, week, month, year}`\n",
    "\n",
    "2. **Intra-customer conservation**\n",
    "\n",
    "* **Constraint:** Energy cannot be traded between households; all shifts remain within the same `ca_id`.\n",
    "\n",
    "# Orchestration & model levers\n",
    "\n",
    "1. **High-usage focus (ordering, not a math constraint)**\n",
    "\n",
    "* **Behaviour:** Rank **customers by daily kWh within each city-day** (e.g., percentile).\n",
    "\n",
    "  * If `shuffle_high_usage_order=False` (default): process **highest usage first**.\n",
    "  * If `True`: compute the percentile anyway (record it), but **randomly shuffle** the processing order.\n",
    "* **Params:** `percentile_threshold` (optional for tagging), `shuffle_high_usage_order ∈ {True, False}` (default False)\n",
    "\n",
    "2. **Parallelisation guidance**\n",
    "\n",
    "* Parallelise **across city-days** (and/or cities/weeks).\n",
    "* Do **not** parallelise **within** a single city-day if enforcing regional/anti-spike budgets (they rely on a shared residual).\n",
    "\n",
    "# Units & accounting\n",
    "\n",
    "* **Base unit:** `gCO₂/kWh` (both **marginal** and **average**).\n",
    "* **Objective:** minimise `∑ post_usage[s] · MEF[s]` subject to constraints.\n",
    "* **Reporting:** show savings in **marginal** terms (what the optimiser optimises) and **average** terms (for context).\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "042dc02b",
   "metadata": {},
   "source": [
    "## Loading Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da58faec",
   "metadata": {},
   "source": [
    "### Directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "592c4f75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DIRECTORIES AND PATHS\n",
    "base_data_directory = \"data\"    # Base directory where the dataframes will be saved\n",
    "hitachi_data_directory = os.path.join(base_data_directory, \"hitachi_copy\")      # Directory where the dataframes will be saved\n",
    "meter_save_directory = os.path.join(hitachi_data_directory, \"meter_primary_files\")       # Directory for meter readings\n",
    "\n",
    "marginal_emissions_development_directory = os.path.join(base_data_directory, \"marginal_emissions_development\")  # Directory for marginal emissions development data\n",
    "marginal_emissions_results_directory = os.path.join(marginal_emissions_development_directory, \"results\")\n",
    "marginal_emissions_logs_directory = os.path.join(marginal_emissions_development_directory, \"logs\")\n",
    "\n",
    "optimisation_development_directory = os.path.join(base_data_directory, \"optimisation_development\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9bb9e9a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "Contents of 'data/optimisation_development' and subdirectories:\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "  - ../optimisation_development/.DS_Store\n",
      "  - ../optimisation_development/average_emissions_2022-05-04_to_2022-05-18.parquet\n",
      "  - ../optimisation_development/customers_ids_with_emissions.parquet\n",
      "  - ../optimisation_development/customers_ids_with_marginal_emissions.parquet\n",
      "  - ../optimisation_development/marginal_and_average_emissions_2022-05-04_to_2022-05-18.parquet\n",
      "  - ../optimisation_development/marginal_emissions_2022-05-04_to_2022-05-18.parquet\n",
      "  - ../optimisation_development/meter_readings_2022-05-04_to_2022-05-18.parquet\n",
      "  - ../optimisation_development/meter_readings_2022-05-04_to_2022-05-18_with_marginal_emissions.parquet\n",
      "  - ../optimisation_development/results/.DS_Store\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"-\" * 120)\n",
    "print(f\"Contents of '{optimisation_development_directory}' and subdirectories:\\n\" + \"-\" * 120)\n",
    "for root, dirs, files in os.walk(optimisation_development_directory):\n",
    "    for f in sorted(files):\n",
    "        rel_dir = os.path.relpath(root, hitachi_data_directory)\n",
    "        rel_file = os.path.join(rel_dir, f) if rel_dir != \".\" else f\n",
    "        print(f\"  - {rel_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb5e360e",
   "metadata": {},
   "source": [
    "### File Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1655da7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining Full files\n",
    "marginal_emissions_filename = \"meter_readings_2022-05-04_to_2022-05-18_with_marginal_emissions\"\n",
    "marginal_emissions_filepath = os.path.join(optimisation_development_directory, marginal_emissions_filename + \".parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8c14533",
   "metadata": {},
   "source": [
    "### Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "431db42f",
   "metadata": {},
   "outputs": [],
   "source": [
    "marginal_emissions_pldf = pl.read_parquet(marginal_emissions_filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ed034db5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------------------------------------------------------------------------\n",
      "Schema of marginal_emissions_pldf\n",
      "------------------------------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Schema([('ca_id', String),\n",
       "        ('date', Datetime(time_unit='us', time_zone='Asia/Kolkata')),\n",
       "        ('city', Categorical(ordering='physical')),\n",
       "        ('customer_longitude', Float64),\n",
       "        ('customer_latitude', Float64),\n",
       "        ('value', Float64),\n",
       "        ('demand_met_kWh', Float64),\n",
       "        ('marginal_emissions_grams_co2_per_kWh', Float64),\n",
       "        ('average_emissions_grams_co2_per_kWh', Float64)])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(\"-\"  *120)\n",
    "print(\"Schema of marginal_emissions_pldf\\n\"+ \"-\"  *120)\n",
    "display(marginal_emissions_pldf.schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d8d0388b",
   "metadata": {},
   "outputs": [],
   "source": [
    "marginal_emissions_pldf = marginal_emissions_pldf.rename({\n",
    "    \"marginal_emissions_grams_co2_per_kWh\": \"marginal_emissions_factor_grams_co2_per_kWh\",\n",
    "    \"average_emissions_grams_co2_per_kWh\": \"average_emissions_factor_grams_co2_per_kWh\"\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "390abfb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------------------------------------------------------------------------\n",
      "Schema of marginal_emissions_pldf\n",
      "------------------------------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Schema([('ca_id', String),\n",
       "        ('date', Datetime(time_unit='us', time_zone='Asia/Kolkata')),\n",
       "        ('city', Categorical(ordering='physical')),\n",
       "        ('customer_longitude', Float64),\n",
       "        ('customer_latitude', Float64),\n",
       "        ('value', Float64),\n",
       "        ('demand_met_kWh', Float64),\n",
       "        ('marginal_emissions_factor_grams_co2_per_kWh', Float64),\n",
       "        ('average_emissions_factor_grams_co2_per_kWh', Float64)])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(\"-\"  *120)\n",
    "print(\"Schema of marginal_emissions_pldf\\n\"+ \"-\"  *120)\n",
    "display(marginal_emissions_pldf.schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c77cbb57",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (9, 10)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>statistic</th><th>ca_id</th><th>date</th><th>city</th><th>customer_longitude</th><th>customer_latitude</th><th>value</th><th>demand_met_kWh</th><th>marginal_emissions_factor_grams_co2_per_kWh</th><th>average_emissions_factor_grams_co2_per_kWh</th></tr><tr><td>str</td><td>str</td><td>str</td><td>str</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td></tr></thead><tbody><tr><td>&quot;count&quot;</td><td>&quot;118643003&quot;</td><td>&quot;118643003&quot;</td><td>&quot;118643003&quot;</td><td>1.17974327e8</td><td>1.17974327e8</td><td>1.18643003e8</td><td>1.18643003e8</td><td>1.18643003e8</td><td>1.18643003e8</td></tr><tr><td>&quot;null_count&quot;</td><td>&quot;0&quot;</td><td>&quot;0&quot;</td><td>&quot;0&quot;</td><td>668676.0</td><td>668676.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td></tr><tr><td>&quot;mean&quot;</td><td>null</td><td>&quot;2022-05-10 23:31:16.797125+05:…</td><td>null</td><td>77.291045</td><td>28.759742</td><td>0.349623</td><td>9.2286e7</td><td>708.509083</td><td>733.241389</td></tr><tr><td>&quot;std&quot;</td><td>null</td><td>null</td><td>null</td><td>3.262976</td><td>1.216057</td><td>0.541503</td><td>4.1689e6</td><td>66.359649</td><td>44.958014</td></tr><tr><td>&quot;min&quot;</td><td>&quot;60000005516&quot;</td><td>&quot;2022-05-04 00:00:00+05:30&quot;</td><td>null</td><td>76.954941</td><td>28.58</td><td>0.0</td><td>8.3034e7</td><td>536.134053</td><td>624.711512</td></tr><tr><td>&quot;25%&quot;</td><td>null</td><td>&quot;2022-05-07 11:30:00+05:30&quot;</td><td>null</td><td>77.12764</td><td>28.688147</td><td>0.069</td><td>8.8729e7</td><td>673.338588</td><td>696.592516</td></tr><tr><td>&quot;50%&quot;</td><td>null</td><td>&quot;2022-05-10 23:00:00+05:30&quot;</td><td>null</td><td>77.15129</td><td>28.703647</td><td>0.166</td><td>9.2563e7</td><td>720.94116</td><td>738.809205</td></tr><tr><td>&quot;75%&quot;</td><td>null</td><td>&quot;2022-05-14 13:30:00+05:30&quot;</td><td>null</td><td>77.193402</td><td>28.723953</td><td>0.417</td><td>9.5808e7</td><td>756.948061</td><td>764.851089</td></tr><tr><td>&quot;max&quot;</td><td>&quot;60029920067&quot;</td><td>&quot;2022-05-18 00:00:00+05:30&quot;</td><td>null</td><td>231.484813</td><td>86.320485</td><td>49.438</td><td>1.0071e8</td><td>835.939878</td><td>827.083443</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (9, 10)\n",
       "┌───────────┬───────────┬───────────┬───────────┬───┬───────────┬───────────┬───────────┬──────────┐\n",
       "│ statistic ┆ ca_id     ┆ date      ┆ city      ┆ … ┆ value     ┆ demand_me ┆ marginal_ ┆ average_ │\n",
       "│ ---       ┆ ---       ┆ ---       ┆ ---       ┆   ┆ ---       ┆ t_kWh     ┆ emissions ┆ emission │\n",
       "│ str       ┆ str       ┆ str       ┆ str       ┆   ┆ f64       ┆ ---       ┆ _factor_g ┆ s_factor │\n",
       "│           ┆           ┆           ┆           ┆   ┆           ┆ f64       ┆ ram…      ┆ _grams…  │\n",
       "│           ┆           ┆           ┆           ┆   ┆           ┆           ┆ ---       ┆ ---      │\n",
       "│           ┆           ┆           ┆           ┆   ┆           ┆           ┆ f64       ┆ f64      │\n",
       "╞═══════════╪═══════════╪═══════════╪═══════════╪═══╪═══════════╪═══════════╪═══════════╪══════════╡\n",
       "│ count     ┆ 118643003 ┆ 118643003 ┆ 118643003 ┆ … ┆ 1.1864300 ┆ 1.1864300 ┆ 1.1864300 ┆ 1.186430 │\n",
       "│           ┆           ┆           ┆           ┆   ┆ 3e8       ┆ 3e8       ┆ 3e8       ┆ 03e8     │\n",
       "│ null_coun ┆ 0         ┆ 0         ┆ 0         ┆ … ┆ 0.0       ┆ 0.0       ┆ 0.0       ┆ 0.0      │\n",
       "│ t         ┆           ┆           ┆           ┆   ┆           ┆           ┆           ┆          │\n",
       "│ mean      ┆ null      ┆ 2022-05-1 ┆ null      ┆ … ┆ 0.349623  ┆ 9.2286e7  ┆ 708.50908 ┆ 733.2413 │\n",
       "│           ┆           ┆ 0 23:31:1 ┆           ┆   ┆           ┆           ┆ 3         ┆ 89       │\n",
       "│           ┆           ┆ 6.797125+ ┆           ┆   ┆           ┆           ┆           ┆          │\n",
       "│           ┆           ┆ 05:…      ┆           ┆   ┆           ┆           ┆           ┆          │\n",
       "│ std       ┆ null      ┆ null      ┆ null      ┆ … ┆ 0.541503  ┆ 4.1689e6  ┆ 66.359649 ┆ 44.95801 │\n",
       "│           ┆           ┆           ┆           ┆   ┆           ┆           ┆           ┆ 4        │\n",
       "│ min       ┆ 600000055 ┆ 2022-05-0 ┆ null      ┆ … ┆ 0.0       ┆ 8.3034e7  ┆ 536.13405 ┆ 624.7115 │\n",
       "│           ┆ 16        ┆ 4 00:00:0 ┆           ┆   ┆           ┆           ┆ 3         ┆ 12       │\n",
       "│           ┆           ┆ 0+05:30   ┆           ┆   ┆           ┆           ┆           ┆          │\n",
       "│ 25%       ┆ null      ┆ 2022-05-0 ┆ null      ┆ … ┆ 0.069     ┆ 8.8729e7  ┆ 673.33858 ┆ 696.5925 │\n",
       "│           ┆           ┆ 7 11:30:0 ┆           ┆   ┆           ┆           ┆ 8         ┆ 16       │\n",
       "│           ┆           ┆ 0+05:30   ┆           ┆   ┆           ┆           ┆           ┆          │\n",
       "│ 50%       ┆ null      ┆ 2022-05-1 ┆ null      ┆ … ┆ 0.166     ┆ 9.2563e7  ┆ 720.94116 ┆ 738.8092 │\n",
       "│           ┆           ┆ 0 23:00:0 ┆           ┆   ┆           ┆           ┆           ┆ 05       │\n",
       "│           ┆           ┆ 0+05:30   ┆           ┆   ┆           ┆           ┆           ┆          │\n",
       "│ 75%       ┆ null      ┆ 2022-05-1 ┆ null      ┆ … ┆ 0.417     ┆ 9.5808e7  ┆ 756.94806 ┆ 764.8510 │\n",
       "│           ┆           ┆ 4 13:30:0 ┆           ┆   ┆           ┆           ┆ 1         ┆ 89       │\n",
       "│           ┆           ┆ 0+05:30   ┆           ┆   ┆           ┆           ┆           ┆          │\n",
       "│ max       ┆ 600299200 ┆ 2022-05-1 ┆ null      ┆ … ┆ 49.438    ┆ 1.0071e8  ┆ 835.93987 ┆ 827.0834 │\n",
       "│           ┆ 67        ┆ 8 00:00:0 ┆           ┆   ┆           ┆           ┆ 8         ┆ 43       │\n",
       "│           ┆           ┆ 0+05:30   ┆           ┆   ┆           ┆           ┆           ┆          │\n",
       "└───────────┴───────────┴───────────┴───────────┴───┴───────────┴───────────┴───────────┴──────────┘"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "marginal_emissions_pldf.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61e113ee",
   "metadata": {},
   "source": [
    "## Developing Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfcdaa5d",
   "metadata": {},
   "source": [
    "### Classes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a128127f",
   "metadata": {},
   "source": [
    "#### Customer & Behavior Constraint Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a93437ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class PeakHoursReductionLimitConfig:\n",
    "    \"\"\"\n",
    "    Configuration for limiting peak hours reduction.\n",
    "\n",
    "    This class allows you to define the scope and percentage limit for reducing peak hours\n",
    "    in a specific region or for a specific customer.\n",
    "    The purpose for this configuration is to allow for preserving comfort during peak hours by not reducing the consumption too much.\n",
    "\n",
    "    Attributes:\n",
    "    -----------\n",
    "    peak_hours_reduction_scope: Literal[\"per_customer\", \"per_city\"]\n",
    "        The scope of the reduction limit (per customer or per city).\n",
    "    peak_hours_reduction_percent_limit: float\n",
    "        The percentage limit for    peak hours reduction.\n",
    "    peak_hours_dict: Optional[Dict[str, Dict[str, List[str]]]]\n",
    "        A dictionary defining peak hours for reduction by city, day, and time.\n",
    "    limit_scope: Literal[\"slot\", \"hour\"]\n",
    "        The scope of the limit (per slot or per hour).\n",
    "    \"\"\"\n",
    "    peak_hours_reduction_scope: Literal[\"per_customer\", \"per_city\"] = \"per_city\"\n",
    "    peak_hours_reduction_percent_limit: float = 30.0\n",
    "    # Dict structure: { delhi: { \"Mon\": [ 9,10,20, ...], \"Tue\": [9,10,11, ...] } }\n",
    "    peak_hours_dict: Optional[Dict[str, Dict[str, List[int]]]] = None\n",
    "    limit_scope: Literal[\"slot\", \"hour\"] = \"hour\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4ccead0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class CustomerAdoptionBehavioralConfig:\n",
    "    \"\"\"\n",
    "    Configuration for customer adoption behavior in the energy management system.\n",
    "\n",
    "    This class allows you to define the behavioral parameters that influence how customers\n",
    "    interact with the energy management system, including their usage patterns and preferences.\n",
    "\n",
    "    Attributes:\n",
    "    -----------\n",
    "    customer_power_moves_per_day : int\n",
    "        The number of shifts a customer is allowed to make per day\n",
    "    customer_power_moves_per_week : int\n",
    "        The number of shifts a customer is allowed to make per week.\n",
    "    timezone: str\n",
    "        The timezone in which the customer operates.\n",
    "    day_boundaries: str\n",
    "        The hours in the day which can be evaluated for shifts (e.g., \"08:00-20:00\")\n",
    "    week_boundaries: str\n",
    "        The boundaries of the week for the customer's schedule.\n",
    "    shift_hours_window: float\n",
    "        The number of hours on either side of usage to look for shifts.\n",
    "    slot_length_minutes: int\n",
    "        The length of each time slot in minutes.\n",
    "    shift_window_inclusive: bool\n",
    "        Whether the shift window is inclusive or exclusive.\n",
    "    peak_hours_reduction_limit_config: Optional[PeakHoursReductionLimitConfig]\n",
    "        Configuration for peak hours reduction limits.\n",
    "    \"\"\"\n",
    "    customer_power_moves_per_day: int = 1\n",
    "    customer_power_moves_per_week: int = 3\n",
    "    timezone: str = \"Asia/Kolkata\"\n",
    "    day_boundaries: str = \"00:00-24:00\"\n",
    "    week_boundaries: Literal[\"Mon-Sun\",\"ISO\"] = \"Mon-Sun\"\n",
    "    shift_hours_window: float = 2.0\n",
    "    slot_length_minutes: int = 30\n",
    "    shift_window_inclusive: bool = True\n",
    "    peak_hours_reduction_limit_config: Optional[PeakHoursReductionLimitConfig] = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18e6519f",
   "metadata": {},
   "source": [
    "#### Technical Constraint Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3b9509d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class HouseholdMinimumConsumptionLimitConfig:\n",
    "    \"\"\"\n",
    "    Configuration for minimum energy consumption limits at the household level.\n",
    "\n",
    "    This class allows you to define the minimum energy consumption limits for households,\n",
    "    ensuring that essential energy needs are met even during demand response events.\n",
    "\n",
    "    The attributes of this class will be used to configure the minimum consumption limits.\n",
    "    remaining_usage ≥ max(min_baseline(customer,t), R% * robust_max(customer,t))\n",
    "\n",
    "    Attributes:\n",
    "    -----------\n",
    "    household_minimum_baseline_period: Literal[\"year\",\"month\",\"week\",\"day\"]\n",
    "        The period over which to calculate the baseline consumption.\n",
    "    household_minimum_baseline_type: Literal[\"average\",\"absolute_min\"]\n",
    "        The type of baseline calculation to use (average or absolute minimum).\n",
    "    household_minimum_robust_max_percentile: float\n",
    "        The percentile to use for robust maximum calculation.\n",
    "    household_minimum_R_percent: float\n",
    "        The percentage limit for minimum consumption (as a fraction of robust max).\n",
    "    household_minimum_epsilon_floor_kWh: float\n",
    "        A small value to avoid zero consumption limits.\n",
    "    \"\"\"\n",
    "    household_minimum_baseline_period: Literal[\"year\",\"month\",\"week\",\"day\"] = \"year\"\n",
    "    household_minimum_baseline_type: Literal[\"average\",\"absolute_min\"] = \"average\"\n",
    "    household_minimum_robust_max_percentile: float = 95.0\n",
    "    household_minimum_R_percent: float = 10.0  # fraction of robust max - defaults to 10\n",
    "    # internal: epsilon floor to avoid zeros/outages\n",
    "    household_minimum_epsilon_floor_kWh: float = 0.001 # small value (1 Wh) to avoid zero consumption limits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "386f5d6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class RegionalLoadShiftingLimitConfig:\n",
    "    \"\"\"\n",
    "    Configuration for regional load shifting capabilities.\n",
    "\n",
    "    The purpose of this configuration is to define the upper limit for load shifting at a regional level,\n",
    "    as large fluctuations in the energy demand can impact the overall stability of the grid.\n",
    "\n",
    "    Attributes:\n",
    "    -----------\n",
    "    regional_load_shift_percent_limit: float\n",
    "        The percentage limit for load shifting (per city).\n",
    "    regional_total_daily_average_load_kWh: float\n",
    "        The total daily average load in kWh for each city.\n",
    "    \"\"\"\n",
    "    regional_load_shift_percent_limit: float = 10.0  # per city\n",
    "    regional_total_daily_average_load_kWh: Optional[Dict[str, float]] = None  # {city: kWh/day}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "74665448",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class ShiftWithoutSpikeLimitConfig:\n",
    "    \"\"\"\n",
    "    Configuration to limit the amount of energy that can be moved into a single time slot\n",
    "    in order to avoid causing a spike in usage.\n",
    "\n",
    "    Attributes:\n",
    "    -----------\n",
    "    city: The city for which the configuration applies.\n",
    "    alpha_peak_cap_percent: The per-slot upper cap vs baseline (city level).\n",
    "    \"\"\"\n",
    "    city: str = \"default_city\" # The city for which the configuration applies\n",
    "    alpha_peak_cap_percent: float = 25.0  # per-slot upper cap vs baseline (city level)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdc67907",
   "metadata": {},
   "source": [
    "#### Utility Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6d7c2aa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class ParallelConfig:\n",
    "    \"\"\"\n",
    "    Configuration for parallel processing in the energy management system.\n",
    "\n",
    "    This class allows you to define the parameters for parallel execution of tasks,\n",
    "    including the method of parallelism and the number of workers to use.\n",
    "\n",
    "    Attributes:\n",
    "    -----------\n",
    "    enabled: bool\n",
    "        Whether parallel processing is enabled.\n",
    "    method: Optional[Literal[\"local\",\"mpi\"]]\n",
    "        The method of parallelism to use (e.g., local or MPI).\n",
    "    workers: Optional[int]\n",
    "        The number of workers to use for local parallelism.\n",
    "    show_progress: bool\n",
    "        Whether to show progress bars during execution.\n",
    "    \"\"\"\n",
    "    enabled: bool = False\n",
    "    method: Optional[Literal[\"local\",\"mpi\"]] = None\n",
    "    workers: Optional[int] = None  # for local\n",
    "    show_progress: bool = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "82a6c045",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class ShiftPolicy:\n",
    "    \"\"\"\n",
    "    Policy configuration for load shifting in the energy management system.\n",
    "\n",
    "    This class allows you to consolidate the configurations available through\n",
    "    the classes defined above in order to apply constraints and limitations to\n",
    "    load shifting strategies, including behavioral, regional, household, and spike caps.\n",
    "\n",
    "    Attributes:\n",
    "    -----------\n",
    "    behavioral: CustomerAdoptionBehavioralConfig\n",
    "        Behavioral configuration for customer adoption.\n",
    "    regional_cap: RegionalLoadShiftingLimitConfig\n",
    "        Regional load shifting limit configuration.\n",
    "    household_min: HouseholdMinimumConsumptionLimitConfig\n",
    "        Household minimum consumption limit configuration.\n",
    "    spike_cap: ShiftWithoutSpikeLimitConfig\n",
    "        Shift without spike limit configuration.\n",
    "    \"\"\"\n",
    "    behavioral: CustomerAdoptionBehavioralConfig = CustomerAdoptionBehavioralConfig()\n",
    "    regional_cap: Optional[RegionalLoadShiftingLimitConfig] = None\n",
    "    household_min: Optional[HouseholdMinimumConsumptionLimitConfig] = None\n",
    "    spike_cap: Optional[ShiftWithoutSpikeLimitConfig] = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "69c3173c",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class SolverConfig:\n",
    "    \"\"\"\n",
    "    Configuration for the optimization solver in the energy management system.\n",
    "\n",
    "    This class allows you to define the parameters for the solver used in the optimization\n",
    "    process, including the solver family and specific options for each solver type.\n",
    "\n",
    "    Attributes:\n",
    "    -----------\n",
    "    solver_family : Literal[\"lp\",\"milp\",\"greedy\"]\n",
    "        The family of the solver to use (e.g., LP, MILP, greedy).\n",
    "    lp_solver : Optional[str]\n",
    "        The specific LP solver to use (if family is \"lp\").\n",
    "    lp_solver_opts : Optional[Dict[str, Any]]\n",
    "        Options for the LP solver.\n",
    "    milp_solver : Optional[str]\n",
    "        The specific MILP solver to use (if family is \"milp\").\n",
    "    milp_solver_opts : Optional[Dict[str, Any]]\n",
    "        Options for the MILP solver.\n",
    "    greedy_min_fraction_of_day_to_move : Optional[float]\n",
    "        Minimum fraction of the day to move for greedy strategies.\n",
    "    \"\"\"\n",
    "    solver_family: Literal[\"lp\",\"milp\",\"greedy\"] = \"lp\"\n",
    "    # LP: choose cvxpy solver name if desired\n",
    "    lp_solver: Optional[str] = \"GLPK\"        # e.g., \"GLPK\",\"ECOS\",\"OSQP\",\"GUROBI\",\"CPLEX\"\n",
    "    lp_solver_opts: Optional[Dict[str, Any]] = None\n",
    "    # MILP: choose pyomo solver name\n",
    "    milp_solver: Optional[str] = \"cbc\"       # e.g., \"glpk\",\"cbc\",\"highs\",\"gurobi\"\n",
    "    milp_solver_opts: Optional[Dict[str, Any]] = None\n",
    "    # Heuristic knobs\n",
    "    greedy_min_fraction_of_day_to_move: Optional[float] = None  # if set, skip tiny moves\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffc1c8dc",
   "metadata": {},
   "source": [
    "### Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa37ed72",
   "metadata": {},
   "source": [
    "#### Temporal helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fbcae8c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_week_start_col(df_pd: pd.DataFrame, week_boundaries: str = \"Mon-Sun\") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Add a 'week_start' midnight column consistent with week boundaries.\n",
    "    Currently supports Monday-start weeks (\"Mon-Sun\") and ISO (also Monday).\n",
    "    \"\"\"\n",
    "    out = df_pd.copy()\n",
    "    day = pd.to_datetime(out[\"day\"])\n",
    "    if week_boundaries == \"ISO\":\n",
    "        week_start = (day - pd.to_timedelta(day.dt.dayofweek, unit=\"D\")).dt.normalize()\n",
    "    else:\n",
    "        # Mon-Sun default\n",
    "        week_start = (day - pd.to_timedelta(day.dt.dayofweek, unit=\"D\")).dt.normalize()\n",
    "    out[\"week_start\"] = week_start\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b11a4963",
   "metadata": {},
   "outputs": [],
   "source": [
    "@lru_cache(maxsize=256)\n",
    "def cached_pairs(\n",
    "    T: int,\n",
    "    W_slots: int\n",
    ") -> Tuple[Tuple[Tuple[int,int], ...], Tuple[np.ndarray, ...], Tuple[np.ndarray, ...]]:\n",
    "    \"\"\"\n",
    "    Generate allowed (t, s) pairs for a given time horizon T and window W_slots.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    T : int\n",
    "        Time horizon (number of slots).\n",
    "    W_slots : int\n",
    "        Maximum allowed distance between t and s.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pairs : Tuple[(int,int), ...]\n",
    "        Allowed (t, s) pairs with |t - s| ≤ W_slots.\n",
    "    by_src : Tuple[np.ndarray, ...]\n",
    "        by_src[t] -> indices i in 'pairs' where pairs[i][0] == t\n",
    "    by_dst : Tuple[np.ndarray, ...]\n",
    "        by_dst[s] -> indices i in 'pairs' where pairs[i][1] == s\n",
    "    \"\"\"\n",
    "    pairs: List[Tuple[int,int]] = []\n",
    "    for t in range(T):\n",
    "        s0, s1 = max(0, t - W_slots), min(T, t + W_slots + 1)\n",
    "        for s in range(s0, s1):\n",
    "            pairs.append((t, s))\n",
    "    pairs = tuple(pairs)\n",
    "    by_src = [[] for _ in range(T)]\n",
    "    by_dst = [[] for _ in range(T)]\n",
    "    for i, (t, s) in enumerate(pairs):\n",
    "        by_src[t].append(i)\n",
    "        by_dst[s].append(i)\n",
    "    by_src = tuple(np.asarray(ix, dtype=int) for ix in by_src)\n",
    "    by_dst = tuple(np.asarray(ix, dtype=int) for ix in by_dst)\n",
    "    return pairs, by_src, by_dst\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "939c2e55",
   "metadata": {},
   "outputs": [],
   "source": [
    "def day_and_slot_cols(\n",
    "        df: pl.DataFrame,\n",
    "        slot_len_min: int = 30\n",
    ") -> pl.DataFrame:\n",
    "    \"\"\"\n",
    "    Adds \"day\" (midnight timestamp) and half-hour \"slot\" [0..47] columns to the DataFrame.\n",
    "\n",
    "    Parameters:\n",
    "    ----------\n",
    "    df : pl.DataFrame\n",
    "        Input DataFrame containing a \"date\" column.\n",
    "    slot_len_min: int\n",
    "        Length of the time slots in minutes (default is 30).\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    pl.DataFrame\n",
    "        DataFrame with added \"day\" and \"slot\" columns.\n",
    "    \"\"\"\n",
    "    # Adds \"day\" (midnight ts) and half-hour \"slot\" [0..47] columns\n",
    "    half_hour = slot_len_min == 30\n",
    "    out = (\n",
    "        df\n",
    "        .with_columns([\n",
    "            pl.col(\"date\").dt.truncate(\"1d\").alias(\"day\"),\n",
    "            (pl.col(\"date\").dt.hour() * (60//slot_len_min) + (pl.col(\"date\").dt.minute() // slot_len_min)).alias(\"slot\")\n",
    "        ])\n",
    "    )\n",
    "    if half_hour:\n",
    "        # ensure slot in [0..47]\n",
    "        out = out.with_columns(pl.col(\"slot\").cast(pl.Int32))\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2a3e81c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def hours_to_slots(\n",
    "        hours: float,\n",
    "        slot_len_min: int = 30\n",
    ") -> int:\n",
    "    \"\"\"\n",
    "    Convert hours to the number of slots based on the slot length in minutes.\n",
    "\n",
    "    Parameters:\n",
    "    ----------\n",
    "    hours : float\n",
    "        The number of hours to convert.\n",
    "    slot_len_min : int\n",
    "        The length of each slot in minutes (default is 30).\n",
    "\n",
    "    Returns:\n",
    "    -------\n",
    "    int\n",
    "        The number of slots corresponding to the given hours.\n",
    "    \"\"\"\n",
    "    return int((hours * 60) // slot_len_min)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0b8cdc13",
   "metadata": {},
   "outputs": [],
   "source": [
    "def week_start(ts: pd.Timestamp, week_boundaries: Literal[\"Mon-Sun\",\"ISO\"]=\"Mon-Sun\") -> pd.Timestamp:\n",
    "    \"\"\"\n",
    "    Compute week start (midnight) for a timestamp given the boundary convention.\n",
    "    \"\"\"\n",
    "    ts = pd.to_datetime(ts).normalize()\n",
    "    dow = ts.weekday()  # Mon=0..Sun=6\n",
    "    if week_boundaries == \"ISO\":\n",
    "        # Monday is week start in ISO as well; keep same\n",
    "        return ts - pd.Timedelta(days=dow)\n",
    "    else:  # \"Mon-Sun\"\n",
    "        return ts - pd.Timedelta(days=dow)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab88875c",
   "metadata": {},
   "source": [
    "#### Baselines and Caps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0ef76b37",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cityday_baseline_by_slot(df_city_day: pd.DataFrame, slot_len_min: int = 30,  dtype=np.float32) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Compute city-day baseline per slot as the sum of original usage across customers.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df_city_day : pandas.DataFrame\n",
    "        Rows for a single (city, day). Requires 'slot' and 'value'.\n",
    "    slot_len_min : int\n",
    "        Slot size in minutes. Defaults to 30.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    np.ndarray\n",
    "        Baseline usage (kWh) per slot.\n",
    "    \"\"\"\n",
    "    if 60 % slot_len_min != 0:\n",
    "        raise ValueError(\"slot_len_min must divide 60.\")\n",
    "    T = 24 * (60 // slot_len_min)\n",
    "    base = np.zeros(T, dtype=dtype)\n",
    "    grp = df_city_day.groupby(\"slot\", as_index=False)[\"value\"].sum()\n",
    "    sl = grp[\"slot\"].to_numpy(dtype=int)\n",
    "    base[sl] = grp[\"value\"].to_numpy(dtype=dtype)\n",
    "    return base"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0bcac28",
   "metadata": {},
   "source": [
    "#### Customer Percentiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "cb52ecdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_city_day_percentiles(df: pl.DataFrame) -> pl.DataFrame:\n",
    "    # 1) daily kWh per (city, day, ca_id)\n",
    "    daily = (\n",
    "        df\n",
    "        .group_by([\"city\", \"day\", \"ca_id\"], maintain_order=False)\n",
    "        .agg(pl.col(\"value\").sum().alias(\"day_kwh\"))\n",
    "    )\n",
    "\n",
    "    # 2) percentile within (city, day); highest day_kwh gets pct close to 1.0\n",
    "    # Polars rank() starts at 1. We want a fractional percentile in [0,1].\n",
    "    # Use \"average\" to mirror pandas' method=\"average\".\n",
    "    daily = daily.with_columns([\n",
    "        (\n",
    "            pl.col(\"day_kwh\")\n",
    "            .rank(method=\"average\", descending=True)\n",
    "            .over([\"city\", \"day\"])\n",
    "            / pl.len().over([\"city\", \"day\"])\n",
    "        ).alias(\"pct\")\n",
    "    ])\n",
    "\n",
    "    return daily"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "456e1045",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_cityday_ca_order_map_pl(pl_df: pl.DataFrame) -> Dict[Tuple[str, pd.Timestamp], List[str]]:\n",
    "    # total kWh per (city, day, ca_id)\n",
    "    daily = (\n",
    "        pl_df\n",
    "        .group_by([\"city\", \"day\", \"ca_id\"], maintain_order=False)\n",
    "        .agg(pl.col(\"value\").sum().alias(\"day_kwh\"))\n",
    "        .sort(by=[\"city\", \"day\", \"day_kwh\"], descending=[False, False, True])\n",
    "    )\n",
    "    # Build grouped lists of ca_id ordered by day_kwh desc\n",
    "    # (Polars: group again and collect ca_id lists in sorted order above)\n",
    "    lists = (\n",
    "        daily\n",
    "        .group_by([\"city\", \"day\"], maintain_order=False)\n",
    "        .agg(pl.col(\"ca_id\").alias(\"ca_order\"))\n",
    "    )\n",
    "\n",
    "    # Materialize to Python dict\n",
    "    out: Dict[Tuple[str, pd.Timestamp], List[str]] = {}\n",
    "    for row in lists.iter_rows(named=True):\n",
    "        city = row[\"city\"]\n",
    "        day  = pd.to_datetime(row[\"day\"])  # ensure pandas Timestamp key\n",
    "        out[(city, day)] = row[\"ca_order\"]\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6f83c93a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rank_customers_by_daily_kwh(df_city_day: pd.DataFrame) -> Tuple[List[str], Dict[str, float], Dict[str, float]]:\n",
    "    \"\"\"\n",
    "    Rank customers by daily kWh within a city-day and compute percentiles.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    order : List[str]\n",
    "        ca_id ordered by descending total daily kWh.\n",
    "    pct : Dict[str, float]\n",
    "        ca_id -> percentile rank (0..100).\n",
    "    totals : Dict[str, float]\n",
    "        ca_id -> total kWh that day.\n",
    "    \"\"\"\n",
    "    totals = df_city_day.groupby(\"ca_id\", as_index=False)[\"value\"].sum()\n",
    "    totals = totals.sort_values(\"value\", ascending=False)\n",
    "    # Percentiles within this city-day\n",
    "    totals[\"pct\"] = 100.0 * totals[\"value\"].rank(pct=True, method=\"average\")\n",
    "    order = totals[\"ca_id\"].tolist()\n",
    "    pct = dict(zip(totals[\"ca_id\"], totals[\"pct\"]))\n",
    "    tot = dict(zip(totals[\"ca_id\"], totals[\"value\"]))\n",
    "    return order, pct, tot\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0ec09cc",
   "metadata": {},
   "source": [
    "#### Peak Helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "8b954c4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def city_peak_targets_for_day(\n",
    "    city: str,\n",
    "    day_ts,\n",
    "    peak_cfg: PeakHoursReductionLimitConfig,\n",
    "    slot_len_min: int = 30,\n",
    ") -> Tuple[Optional[np.ndarray], Optional[List[List[int]]], Optional[float], str]:\n",
    "    \"\"\"\n",
    "    Expand user-provided full hours (e.g. 8, 9, 10) into slot indices for a given day.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    city : str\n",
    "        City name. Must exist as a key in `peak_cfg.peak_hours_dict`.\n",
    "    day_ts : datetime\n",
    "        Midnight timestamp for the day being solved (group key).\n",
    "    peak_cfg : PeakHoursReductionLimitConfig\n",
    "        Configuration object with Z% and hourly lists per (city, weekday).\n",
    "    slot_len_min : int, optional\n",
    "        Slot size in minutes. Defaults to 30 (→ 48 slots).\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    mask : np.ndarray or None\n",
    "        Boolean mask of length T (True where destination slots are peak).\n",
    "    groups : List[List[int]] or None\n",
    "        Each inner list is the indices of the slots forming one *hour*. Only used\n",
    "        when applying a single cap per hour (i.e., limit_scope == \"hour\").\n",
    "    Z : float or None\n",
    "        Fractional cap (e.g., 0.30 for 30%).\n",
    "    limit_scope : {\"slot\", \"hour\"}\n",
    "        Scope of the cap; mirrors `peak_cfg.limit_scope`.\n",
    "\n",
    "    Notes\n",
    "    -----\n",
    "    - Weekday keys must match `strftime(\"%a\")` (\"Mon\"..\"Sun\").\n",
    "    - If city or weekday has no configured hours, returns (None, None, None, \"slot\").\n",
    "    \"\"\"\n",
    "    if not peak_cfg or not peak_cfg.peak_hours_dict:\n",
    "        return None, None, None, \"slot\"\n",
    "\n",
    "    wk = day_ts.strftime(\"%a\")  # \"Mon\"..\"Sun\"\n",
    "    hours = (peak_cfg.peak_hours_dict.get(city, {}) or {}).get(wk, [])\n",
    "    if not hours:\n",
    "        return None, None, None, \"slot\"\n",
    "\n",
    "    if 60 % slot_len_min != 0:\n",
    "        raise ValueError(f\"slot_len_min must divide 60; got {slot_len_min}.\")\n",
    "\n",
    "    per_hour = 60 // slot_len_min\n",
    "    T = 24 * per_hour\n",
    "\n",
    "    mask = np.zeros(T, dtype=bool)\n",
    "    groups: List[List[int]] = []\n",
    "    for h in hours:\n",
    "        if 0 <= h <= 23:\n",
    "            start = h * per_hour\n",
    "            end   = start + per_hour\n",
    "            groups.append(list(range(start, end)))\n",
    "            mask[start:end] = True\n",
    "\n",
    "    Z = peak_cfg.peak_hours_reduction_percent_limit / 100.0\n",
    "    return mask, groups, Z, peak_cfg.limit_scope\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84caa6a3",
   "metadata": {},
   "source": [
    "#### General Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "dfd5ee91",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_arrays_for_group_pd(\n",
    "    sub_df: pd.DataFrame,\n",
    "    slot_len_min: int = 30,\n",
    ") -> Optional[Dict[str, np.ndarray]]:\n",
    "    \"\"\"\n",
    "    Build dense, slot-aligned arrays for a single (ca_id, day, city) pandas group.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    sub_df : pandas.DataFrame\n",
    "        Expected columns:\n",
    "        - 'slot' (int), 'value' (float, kWh)\n",
    "        - 'marginal_emissions_factor_grams_co2_per_kWh' (float, gCO2/kWh)\n",
    "        - 'average_emissions_factor_grams_co2_per_kWh' (float, gCO2/kWh)\n",
    "        - 'floor_kwh' (optional, float)\n",
    "    slot_len_min : int, optional\n",
    "        Slot size in minutes. Defaults to 30 (→ 48 slots/day).\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    dict or None\n",
    "        dict with keys: 'usage', 'mef', 'aef' and optional 'floor'.\n",
    "        Returns None when sub_df is empty or required factors missing in used slots.\n",
    "    \"\"\"\n",
    "    if sub_df.empty:\n",
    "        return None\n",
    "    if 60 % slot_len_min != 0:\n",
    "        raise ValueError(f\"slot_len_min must evenly divide 60; got {slot_len_min}.\")\n",
    "    per_hour = 60 // slot_len_min\n",
    "    T = 24 * per_hour\n",
    "\n",
    "    usage = np.zeros(T, dtype=np.float32)\n",
    "    mef   = np.full(T, np.nan, dtype=np.float32)\n",
    "    aef   = np.full(T, np.nan, dtype=np.float32)\n",
    "    floor: Optional[np.ndarray] = None\n",
    "\n",
    "    sl = sub_df[\"slot\"].to_numpy(dtype=int)\n",
    "    if (sl < 0).any() or (sl >= T).any():\n",
    "        raise ValueError(f\"Found slot outside [0,{T-1}] for slot_len_min={slot_len_min}.\")\n",
    "\n",
    "    usage[sl] = sub_df[\"value\"].to_numpy(dtype=np.float32)\n",
    "    mef[sl]   = sub_df[\"marginal_emissions_factor_grams_co2_per_kWh\"].to_numpy(dtype=np.float32)\n",
    "    aef[sl]   = sub_df[\"average_emissions_factor_grams_co2_per_kWh\"].to_numpy(dtype=np.float32)\n",
    "\n",
    "    if \"floor_kwh\" in sub_df.columns:\n",
    "        floor = np.zeros(T, dtype=np.float32)\n",
    "        floor[sl] = sub_df[\"floor_kwh\"].to_numpy(dtype=np.float32)\n",
    "\n",
    "    used = usage > 0\n",
    "    if used.any() and (np.isnan(mef[used]).any() or np.isnan(aef[used]).any()):\n",
    "        return None\n",
    "\n",
    "    out = {\"usage\": usage, \"mef\": mef, \"aef\": aef}\n",
    "    if floor is not None:\n",
    "        out[\"floor\"] = floor\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "27977e27",
   "metadata": {},
   "outputs": [],
   "source": [
    "def progress_printer(total: int) -> Callable[[int], None]:\n",
    "    \"\"\"\n",
    "    Simple progress printer.\n",
    "    \"\"\"\n",
    "\n",
    "    # simple progress callback factory\n",
    "    def cb(i: int):\n",
    "        if total <= 0:\n",
    "            return\n",
    "        # print every ~5%\n",
    "        step = max(1, total // 20)\n",
    "        if i % step == 0 or i == total:\n",
    "            print(f\"[progress] {i}/{total} groups processed ({(i/total)*100:.1f}%)\")\n",
    "    return cb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c1dc3ee",
   "metadata": {},
   "source": [
    "#### Peak Masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ab542309",
   "metadata": {},
   "outputs": [],
   "source": [
    "def restrict_sources_mask_for_limit(usage: np.ndarray, mef: np.ndarray, K: int) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Choose up to K source slots allowed to move (others must stay).\n",
    "    Heuristic: highest potential benefit (mef[t] - best_dest_mef) * usage[t].\n",
    "    \"\"\"\n",
    "    T = len(usage)\n",
    "    if K is None or K >= T:\n",
    "        return np.ones(T, dtype=bool)\n",
    "\n",
    "    # take only slots with positive usage\n",
    "    pos = usage > 1e-12\n",
    "    idx = np.flatnonzero(pos)\n",
    "    if idx.size <= K:\n",
    "        mask = np.zeros(T, dtype=bool)\n",
    "        mask[idx] = True\n",
    "        return mask\n",
    "\n",
    "    # top-K by mef among positive-usage\n",
    "    k_idx_local = np.argpartition(-mef[idx], K-1)[:K]        # O(T)\n",
    "    top_idx = idx[k_idx_local]\n",
    "\n",
    "    mask = np.zeros(T, dtype=bool)\n",
    "    mask[top_idx] = True\n",
    "    return mask"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3488542",
   "metadata": {},
   "source": [
    "##### Emissions Computations and Reporting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "2c54017b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_emissions_totals(\n",
    "        usage: np.ndarray,\n",
    "        aef: np.ndarray,\n",
    "        mef: np.ndarray,\n",
    ") -> Dict[str, float]:\n",
    "    \"\"\"\n",
    "    Compute total emissions in grams and tonnes of CO2 based on usage and emission factors.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    usage : np.ndarray\n",
    "        The energy usage for each time slot.\n",
    "    aef : np.ndarray\n",
    "        The average emission factor for each time slot.\n",
    "    mef : np.ndarray\n",
    "        The marginal emission factor for each time slot.\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    Dict[str, float]\n",
    "        A dictionary containing the total emissions in grams and tonnes of CO2 for both average and marginal weighting.\n",
    "\n",
    "    \"\"\"\n",
    "    # returns totals in grams and tCO2 for both average and marginal weighting\n",
    "    g_avg = float(np.dot(usage, aef))   # total grams avg\n",
    "    g_mrg = float(np.dot(usage, mef))   # total grams marginal\n",
    "    return {\n",
    "        \"E_avg_g\": g_avg,\n",
    "        \"E_avg_t\": tco2_from_grams(g_avg),\n",
    "        \"E_marg_g\": g_mrg,\n",
    "        \"E_marg_t\": tco2_from_grams(g_mrg),\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "2268914e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def flows_table(\n",
    "        ca_id: str,\n",
    "        city: str,\n",
    "        day_ts,\n",
    "        flows: List[Tuple[int,int,float]],\n",
    "        mef: np.ndarray,\n",
    "        aef: np.ndarray,\n",
    "        slot_len_min: int = 30,\n",
    ") -> List[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Generate a table of flows with associated emissions data.\n",
    "\n",
    "    Parameters:\n",
    "    ----------\n",
    "    ca_id : str\n",
    "        The ID of the charging station.\n",
    "    city : str\n",
    "        The city where the charging station is located.\n",
    "    day_ts : datetime\n",
    "        The timestamp for the day of the flows.\n",
    "    flows : List[Tuple[int,int,float]]\n",
    "        A list of tuples representing the flows, where each tuple contains\n",
    "        (start_time, end_time, kwh).\n",
    "    mef : np.ndarray\n",
    "        The marginal emission factor for each time slot.\n",
    "    aef : np.ndarray\n",
    "        The average emission factor for each time slot.\n",
    "    slot_len_min : int, optional\n",
    "        The length of each time slot in minutes (default is 30).\n",
    "\n",
    "    Returns:\n",
    "    -------\n",
    "    List[Dict[str, Any]]\n",
    "        A list of dictionaries containing the flow data with emissions information.\n",
    "    \"\"\"\n",
    "    rows: List[Dict[str, Any]] = []\n",
    "    for (t, s, kwh) in flows:\n",
    "        t_minutes = t * slot_len_min\n",
    "        s_minutes = s * slot_len_min\n",
    "        t_ts = day_ts + timedelta(minutes=int(t_minutes))\n",
    "        s_ts = day_ts + timedelta(minutes=int(s_minutes))\n",
    "        dirn = \"forward\" if s > t else (\"backward\" if s < t else \"stay\")\n",
    "\n",
    "        # Per-move emissions deltas (grams) using marginal & average\n",
    "        g_marg_before = grams_co2_from_kwh_grams_per_kwh(kwh, mef[t])\n",
    "        g_marg_after  = grams_co2_from_kwh_grams_per_kwh(kwh, mef[s])\n",
    "        g_marg_delta  = g_marg_before - g_marg_after\n",
    "\n",
    "        g_avg_before = grams_co2_from_kwh_grams_per_kwh(kwh, aef[t])\n",
    "        g_avg_after  = grams_co2_from_kwh_grams_per_kwh(kwh, aef[s])\n",
    "        g_avg_delta  = g_avg_before - g_avg_after\n",
    "\n",
    "        rows.append({\n",
    "            \"ca_id\": ca_id,\n",
    "            \"city\": city,\n",
    "            \"day\": day_ts,\n",
    "            \"original_time\": t_ts,\n",
    "            \"proposed_shift_time\": s_ts,\n",
    "            \"delta_minutes\": int((s - t) * slot_len_min),\n",
    "            \"shift_direction\": dirn,\n",
    "            \"delta_kwh\": float(kwh),\n",
    "            \"marginal_emissions_before_shift_grams_co2\": g_marg_before,\n",
    "            \"marginal_emissions_after_shift_grams_co2\": g_marg_after,\n",
    "            \"marginal_emissions_delta_grams_co2\": g_marg_delta,\n",
    "            \"average_emissions_before_shift_grams_co2\": g_avg_before,\n",
    "            \"average_emissions_after_shift_grams_co2\": g_avg_after,\n",
    "            \"average_emissions_delta_grams_co2\": g_avg_delta,\n",
    "        })\n",
    "    return rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "e53c4605",
   "metadata": {},
   "outputs": [],
   "source": [
    "def grams_co2_from_kwh_grams_per_kwh(\n",
    "        kwh: float,\n",
    "        grams_co2_per_kwh: float\n",
    ") -> float:\n",
    "    \"\"\"\n",
    "    Convert energy in kWh to grams co2 based on a specific emission factor.\n",
    "\n",
    "    Parameters:\n",
    "    ----------\n",
    "    kwh : float\n",
    "        The energy in kilowatt-hours.\n",
    "    grams_co2_per_kwh : float\n",
    "        The emission factor in grams co2 per kilowatt-hour.\n",
    "\n",
    "    Returns:\n",
    "    -------\n",
    "    float\n",
    "        The equivalent emissions in grams of co2.\n",
    "    \"\"\"\n",
    "    return float(kwh * grams_co2_per_kwh)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "9d72f0a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tco2_from_grams(g: float) -> float:\n",
    "    \"\"\"\n",
    "    Convert grams of CO2 to tonnes of CO2.\n",
    "    \"\"\"\n",
    "    return g / 1_000_000.0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "ec6840dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def weighted_median_shift_minutes(flows: List[Tuple[int,int,float]], slot_len_min: int = 30) -> float:\n",
    "    \"\"\"\n",
    "    Energy-weighted median of |Δslots| converted to minutes.\n",
    "    \"\"\"\n",
    "    if not flows:\n",
    "        return 0.0\n",
    "    steps = np.array([abs(s - t) for (t, s, _) in flows], dtype=float)\n",
    "    w     = np.array([kwh for (_, _, kwh) in flows], dtype=float)\n",
    "    order = np.argsort(steps)\n",
    "    steps, w = steps[order], w[order]\n",
    "    cum = np.cumsum(w) / (w.sum() + 1e-12)\n",
    "    idx = np.searchsorted(cum, 0.5, side=\"left\")\n",
    "    return float(steps[min(idx, len(steps)-1)] * slot_len_min)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7ce669c",
   "metadata": {},
   "source": [
    "#### Unknown?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "adba4999",
   "metadata": {},
   "outputs": [],
   "source": [
    "def attach_household_floor(\n",
    "    df: pl.DataFrame,\n",
    "    cfg: HouseholdMinimumConsumptionLimitConfig,\n",
    ") -> pl.DataFrame:\n",
    "    \"\"\"\n",
    "    Adds a column 'floor_kwh' per row using:\n",
    "      floor = max(min_baseline, R% * robust_max_percentile), with epsilon floor.\n",
    "    Stratification: (hour-of-day, day-of-week); period filter applied across df.\n",
    "    \"\"\"\n",
    "    # Filter by period if you want (here we keep full df; you can parametrize)\n",
    "    base = (\n",
    "        df\n",
    "        .with_columns([\n",
    "            pl.col(\"date\").dt.hour().alias(\"hod\"),\n",
    "            pl.col(\"date\").dt.strftime(\"%A\").alias(\"dow\"),\n",
    "        ])\n",
    "        .group_by([\"ca_id\",\"hod\",\"dow\"])\n",
    "    )\n",
    "\n",
    "    # Baseline\n",
    "    if cfg.household_minimum_baseline_type == \"average\":\n",
    "        baseline = base.agg(pl.mean(\"value\").alias(\"baseline_kwh\"))\n",
    "    else:\n",
    "        baseline = base.agg(pl.min(\"value\").alias(\"baseline_kwh\"))\n",
    "\n",
    "    # Robust max (percentile on 'value')\n",
    "    p = cfg.household_minimum_robust_max_percentile\n",
    "    robust = (\n",
    "        df\n",
    "        .with_columns([\n",
    "            pl.col(\"date\").dt.hour().alias(\"hod\"),\n",
    "            pl.col(\"date\").dt.strftime(\"%A\").alias(\"dow\"),\n",
    "        ])\n",
    "        .group_by([\"ca_id\",\"hod\",\"dow\"])\n",
    "        .agg(pl.col(\"value\").quantile(p/100.0, interpolation=\"nearest\").alias(\"robust_max_kwh\"))\n",
    "    )\n",
    "\n",
    "    stats = baseline.join(robust, on=[\"ca_id\",\"hod\",\"dow\"], how=\"full\")\n",
    "    stats = stats.with_columns([\n",
    "        pl.col(\"baseline_kwh\").fill_null(0.0),\n",
    "        pl.col(\"robust_max_kwh\").fill_null(0.0),\n",
    "    ])\n",
    "\n",
    "    stats = stats.with_columns([\n",
    "        pl.max_horizontal([\n",
    "            pl.col(\"baseline_kwh\"),\n",
    "            (pl.col(\"robust_max_kwh\") * (cfg.household_minimum_R_percent / 100.0))\n",
    "        ]).alias(\"floor_kwh_raw\")\n",
    "    ]).with_columns([\n",
    "        pl.max_horizontal([pl.col(\"floor_kwh_raw\"), pl.lit(cfg.household_minimum_epsilon_floor_kWh)]).alias(\"floor_kwh\")\n",
    "    ]).select([\"ca_id\",\"hod\",\"dow\",\"floor_kwh\"])\n",
    "\n",
    "    # Attach back to original df\n",
    "    df_with_floor = (\n",
    "        df\n",
    "        .with_columns([\n",
    "            pl.col(\"date\").dt.hour().alias(\"hod\"),\n",
    "            pl.col(\"date\").dt.strftime(\"%A\").alias(\"dow\"),\n",
    "        ])\n",
    "        .join(stats, on=[\"ca_id\",\"hod\",\"dow\"], how=\"left\")\n",
    "        .drop([\"hod\",\"dow\"])\n",
    "    )\n",
    "    return df_with_floor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6006b4d",
   "metadata": {},
   "source": [
    "#### Solvers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "e78e2b55",
   "metadata": {},
   "outputs": [],
   "source": [
    "def greedy_k_moves(\n",
    "    usage: np.ndarray,\n",
    "    mef: np.ndarray,\n",
    "    aef: np.ndarray,\n",
    "    *,\n",
    "    W_slots: int,\n",
    "    slot_len_min: int,\n",
    "    floor_vec: Optional[np.ndarray],\n",
    "    peak_mask: Optional[np.ndarray],\n",
    "    peak_groups: Optional[List[List[int]]],\n",
    "    Z: Optional[float],\n",
    "    cap_mode: Literal[\"slot\",\"hour\"] = \"slot\",\n",
    "    dest_rem: np.ndarray,\n",
    "    reg_rem: float,\n",
    "    max_moves: int = 1,\n",
    "    enforce_distinct_sources: bool = True,   # NEW: enforce unique source slots\n",
    ") -> Tuple[np.ndarray, List[Tuple[int,int,float]], float, float]:\n",
    "    \"\"\"\n",
    "    Greedy solver that performs up to `max_moves` beneficial moves (t->s).\n",
    "    If `enforce_distinct_sources` is True, each move must use a *new* source slot t\n",
    "    (i.e., at most `max_moves` distinct sources moved for the day).\n",
    "    \"\"\"\n",
    "    T = len(usage)\n",
    "    usage_opt = usage.copy()\n",
    "    flows: List[Tuple[int,int,float]] = []\n",
    "    used_dest_total = 0.0\n",
    "    used_reg_total  = 0.0\n",
    "\n",
    "    # Track which sources have already been used (distinct-sources cap)\n",
    "    used_sources: Set[int] = set()\n",
    "\n",
    "    # Peak comfort remaining reducible energy (per slot or per hour) for THIS customer-day\n",
    "    if Z is not None and peak_mask is not None and peak_mask.any():\n",
    "        if cap_mode == \"slot\":\n",
    "            peak_rem_slot = np.zeros(T, dtype=np.float32)\n",
    "            peak_idx = np.where(peak_mask)[0]\n",
    "            peak_rem_slot[peak_mask] = np.float32(Z) * usage_opt[peak_mask].astype(np.float32, copy=False)\n",
    "            peak_hour_rem: Optional[List[float]] = None\n",
    "        else:\n",
    "            peak_hour_rem = []\n",
    "            for grp in (peak_groups or []):\n",
    "                peak_hour_rem.append(Z * float(usage_opt[grp].sum()))\n",
    "            peak_rem_slot = None\n",
    "    else:\n",
    "        peak_rem_slot = None\n",
    "        peak_hour_rem = None\n",
    "\n",
    "    for _ in range(max_moves):\n",
    "        best_gain = 0.0\n",
    "        best: Optional[Tuple[int,int,float]] = None\n",
    "\n",
    "        for t in range(T):\n",
    "            # Distinct sources: skip if t already used in a previous move\n",
    "            if enforce_distinct_sources and (t in used_sources):\n",
    "                continue\n",
    "\n",
    "            if usage_opt[t] <= 1e-12:\n",
    "                continue\n",
    "\n",
    "            floor_t = float(floor_vec[t]) if (floor_vec is not None) else 0.0\n",
    "            avail_from_t = max(0.0, usage_opt[t] - floor_t)\n",
    "            if avail_from_t <= 1e-12:\n",
    "                continue\n",
    "\n",
    "            # Peak remaining allowance if moving OUT of peak\n",
    "            peak_lim_t = float(\"inf\")\n",
    "            if peak_rem_slot is not None and (peak_rem_slot[t] > 0.0):\n",
    "                peak_lim_t = peak_rem_slot[t]\n",
    "            elif peak_hour_rem is not None and peak_groups is not None:\n",
    "                # find hour group index for t (only if t is inside any peak group)\n",
    "                for k, grp in enumerate(peak_groups):\n",
    "                    if t in grp:\n",
    "                        if peak_hour_rem[k] > 0.0:\n",
    "                            peak_lim_t = peak_hour_rem[k]\n",
    "                        else:\n",
    "                            peak_lim_t = 0.0\n",
    "                        break\n",
    "\n",
    "            s0, s1 = max(0, t - W_slots), min(T, t + W_slots + 1)\n",
    "            for s in range(s0, s1):\n",
    "                if s == t:\n",
    "                    continue\n",
    "                # Only consider moves that reduce emissions\n",
    "                if mef[s] >= mef[t] - 1e-12:\n",
    "                    continue\n",
    "\n",
    "                # Destination anti-spike residual\n",
    "                cap_dest = float(dest_rem[s]) if dest_rem is not None else float(\"inf\")\n",
    "                # Regional budget\n",
    "                cap_reg = float(reg_rem) if reg_rem is not None else float(\"inf\")\n",
    "\n",
    "                # If moving OUT of peak, also cap by remaining peak allowance from t\n",
    "                if (peak_mask is not None) and peak_mask.any() and peak_mask[t]:\n",
    "                    q_max = min(avail_from_t, peak_lim_t, cap_dest, cap_reg)\n",
    "                else:\n",
    "                    q_max = min(avail_from_t, cap_dest, cap_reg)\n",
    "\n",
    "                if q_max <= 1e-12:\n",
    "                    continue\n",
    "\n",
    "                # emissions gain = (mef[t]-mef[s]) * q\n",
    "                gain = (mef[t] - mef[s]) * q_max\n",
    "                if gain > best_gain + 1e-12:\n",
    "                    best_gain = gain\n",
    "                    best = (t, s, q_max)\n",
    "\n",
    "        if best is None:\n",
    "            break  # no beneficial distinct-source move left\n",
    "\n",
    "        # Apply the best move\n",
    "        t, s, q = best\n",
    "        usage_opt[t] -= q\n",
    "        usage_opt[s] += q\n",
    "        flows.append((t, s, float(q)))\n",
    "\n",
    "        # Mark source as used if enforcing distinct sources\n",
    "        if enforce_distinct_sources:\n",
    "            used_sources.add(t)\n",
    "\n",
    "        # Update shared caps\n",
    "        if dest_rem is not None:\n",
    "            dest_rem[s] = max(0.0, dest_rem[s] - q)\n",
    "        if reg_rem is not None:\n",
    "            reg_rem = max(0.0, reg_rem - q)\n",
    "\n",
    "        used_dest_total += q\n",
    "        used_reg_total  += q\n",
    "\n",
    "        # Update peak allowance if we moved OUT of peak\n",
    "        if Z is not None and peak_mask is not None and peak_mask.any() and peak_mask[t]:\n",
    "            if peak_rem_slot is not None:\n",
    "                peak_rem_slot[t] = max(0.0, peak_rem_slot[t] - q)\n",
    "            elif peak_hour_rem is not None and peak_groups is not None:\n",
    "                for k, grp in enumerate(peak_groups):\n",
    "                    if t in grp:\n",
    "                        peak_hour_rem[k] = max(0.0, peak_hour_rem[k] - q)\n",
    "                        break\n",
    "\n",
    "    return usage_opt, flows, used_dest_total, used_reg_total\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "9231783f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def solve_milp_k(\n",
    "    mef: np.ndarray,\n",
    "    usage: np.ndarray,\n",
    "    W_slots: int,\n",
    "    cfg: SolverConfig,\n",
    "    *,\n",
    "    max_moves: int,\n",
    "    peak_mask: Optional[np.ndarray] = None,\n",
    "    peak_groups: Optional[List[List[int]]] = None,\n",
    "    Z: Optional[float] = None,\n",
    "    cap_mode: Literal[\"slot\",\"hour\"] = \"hour\",\n",
    "    floor_vec: Optional[np.ndarray] = None,\n",
    "    dest_upper_bounds: Optional[np.ndarray] = None,\n",
    "    moved_kwh_cap: Optional[float] = None,\n",
    ") -> Tuple[np.ndarray, List[Tuple[int,int,float]]]:\n",
    "    assert pyo is not None\n",
    "    T = len(mef)\n",
    "    pairs, by_src, by_dst = cached_pairs(T, W_slots)\n",
    "\n",
    "    m = pyo.ConcreteModel()\n",
    "    m.P = pyo.RangeSet(0, len(pairs)-1)\n",
    "    m.y = pyo.Var(m.P, domain=pyo.NonNegativeReals)\n",
    "\n",
    "    # z_t = 1 if any move out of t to s != t\n",
    "    m.TS = pyo.RangeSet(0, T-1)\n",
    "    m.z  = pyo.Var(m.TS, domain=pyo.Binary)\n",
    "\n",
    "    # Objective: minimize post emissions\n",
    "    m.obj = pyo.Objective(expr=sum(m.y[i] * mef[pairs[i][1]] for i in m.P), sense=pyo.minimize)\n",
    "\n",
    "    # Supply conservation: for each t, sum_s y_{t->s} = usage[t]\n",
    "    def supply_rule(m, t):\n",
    "        idx = by_src[t].tolist()\n",
    "        return sum(m.y[i] for i in idx) == float(usage[t])\n",
    "    m.supply = pyo.Constraint(m.TS, rule=supply_rule)\n",
    "\n",
    "    # Link y to z: any flow to s!=t implies z_t = 1\n",
    "    m.link = pyo.ConstraintList()\n",
    "    # Big-M per pair\n",
    "    for i, (t, s) in enumerate(pairs):\n",
    "        if s == t:\n",
    "            continue\n",
    "        # M bound like your greedy: source above floor, dest headroom, peak allowance, regional cap\n",
    "        src_floor = float(floor_vec[t]) if (floor_vec is not None) else 0.0\n",
    "        ub_src = max(0.0, float(usage[t] - src_floor))\n",
    "        if dest_upper_bounds is not None and np.isfinite(dest_upper_bounds[s]):\n",
    "            ub_dest = max(0.0, float(dest_upper_bounds[s]))\n",
    "        else:\n",
    "            ub_dest = float(\"inf\")\n",
    "        ub_peak = float(\"inf\")\n",
    "        if Z is not None and peak_mask is not None and peak_mask.any() and peak_mask[t]:\n",
    "            # cap per slot or per hour of the source side – conservative slot-level bound\n",
    "            if cap_mode == \"slot\":\n",
    "                ub_peak = Z * float(usage[t])\n",
    "            else:\n",
    "                # if hour groups provided, use the hour bound for t\n",
    "                if peak_groups:\n",
    "                    for grp in peak_groups:\n",
    "                        if t in grp:\n",
    "                            ub_peak = Z * float(usage[grp].sum())\n",
    "                            break\n",
    "        ub_reg = float(moved_kwh_cap) if (moved_kwh_cap is not None) else float(\"inf\")\n",
    "        M_ts = min(ub_src, ub_dest, ub_peak, ub_reg)\n",
    "        if not np.isfinite(M_ts) or M_ts < 0:\n",
    "            M_ts = 0.0\n",
    "        m.link.add(m.y[i] <= M_ts * m.z[t])\n",
    "\n",
    "    # K moves total\n",
    "    m.kcap = pyo.Constraint(expr=sum(m.z[t] for t in m.TS) <= int(max_moves))\n",
    "\n",
    "    # # Floors at destinations\n",
    "    # if floor_vec is not None:\n",
    "    #     for s in range(T):\n",
    "    #         if floor_vec[s] > 0:\n",
    "    #             idx = by_dst[s].tolist()\n",
    "    #             if idx:\n",
    "    #                 m.add_component(f\"floor_{s}\", pyo.Constraint(expr=sum(m.y[i] for i in idx) >= float(floor_vec[s])))\n",
    "\n",
    "    # # Peak comfort at destinations (keep your existing per-slot/per-hour minima)\n",
    "    # if peak_mask is not None and Z is not None:\n",
    "    #     if cap_mode == \"slot\":\n",
    "    #         for s in np.where(peak_mask)[0]:\n",
    "    #             base_s = float(usage[s])\n",
    "    #             if base_s > 1e-12:\n",
    "    #                 idx = by_dst[s].tolist()\n",
    "    #                 if idx:\n",
    "    #                     m.add_component(f\"peak_slot_min_{s}\", pyo.Constraint(expr=sum(m.y[i] for i in idx) >= (1.0 - Z) * base_s))\n",
    "    #     else:\n",
    "    #         if peak_groups:\n",
    "    #             for k, grp in enumerate(peak_groups):\n",
    "    #                 base_h = float(usage[grp].sum())\n",
    "    #                 if base_h > 1e-12:\n",
    "    #                     idxs = []\n",
    "    #                     for s in grp:\n",
    "    #                         idxs.extend(by_dst[s].tolist())\n",
    "    #                     if idxs:\n",
    "    #                         m.add_component(f\"peak_hour_min_{k}\", pyo.Constraint(expr=sum(m.y[i] for i in idxs) >= (1.0 - Z) * base_h))\n",
    "\n",
    "    # Anti-spike dest caps (remaining headroom for this customer)\n",
    "    if dest_upper_bounds is not None:\n",
    "        for s in range(T):\n",
    "            cap = dest_upper_bounds[s]\n",
    "            if np.isfinite(cap):\n",
    "                idx = by_dst[s].tolist()\n",
    "                if idx:\n",
    "                    m.add_component(f\"dest_cap_{s}\",\n",
    "                                    pyo.Constraint(expr=sum(m.y[i] for i in idx) <= float(usage[s]) + float(cap))\n",
    "                                    )\n",
    "\n",
    "    # Regional moved-kWh cap\n",
    "    if moved_kwh_cap is not None:\n",
    "        stay_idxs = [i for i, (t, s) in enumerate(pairs) if t == s]\n",
    "        stay_expr = sum(m.y[i] for i in stay_idxs)\n",
    "        m.add_component(\"moved_cap\", pyo.Constraint(expr=(float(usage.sum()) - stay_expr) <= float(moved_kwh_cap)))\n",
    "\n",
    "    # NEW: robust solver creation with fallbacks\n",
    "    solver_name = cfg.milp_solver or \"cbc\"\n",
    "    solver = pyo.SolverFactory(solver_name)\n",
    "\n",
    "    # If preferred solver isn't available, try a few common fallbacks\n",
    "    if (solver is None) or (not solver.available(False)):\n",
    "        for cand in (\"highs\", \"glpk\"):\n",
    "            s = pyo.SolverFactory(cand)\n",
    "            if (s is not None) and s.available(False):\n",
    "                solver, solver_name = s, cand\n",
    "                break\n",
    "        else:\n",
    "            raise RuntimeError(\"No MILP solver available (tried cbc, highs, glpk). \"\n",
    "                            \"Install one of them or set SolverConfig.milp_solver accordingly.\")\n",
    "\n",
    "    # Optional: pass any user-specified options (e.g., {\"threads\": 1, \"time_limit\": 60})\n",
    "    if cfg.milp_solver_opts:\n",
    "        for k, v in cfg.milp_solver_opts.items():\n",
    "            solver.options[k] = v\n",
    "\n",
    "    # Solve\n",
    "    solver.solve(m, tee=False)\n",
    "\n",
    "\n",
    "    y = np.array([pyo.value(m.y[i]) for i in m.P], dtype=float)\n",
    "    usage_opt = np.zeros(T, dtype=float)\n",
    "    flows: List[Tuple[int,int,float]] = []\n",
    "    for i, val in enumerate(y):\n",
    "        if val is None or val <= 1e-12:\n",
    "            continue\n",
    "        t, s = pairs[i]\n",
    "        usage_opt[s] += val\n",
    "        flows.append((t, s, val))\n",
    "    return usage_opt, flows\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8545178b",
   "metadata": {},
   "source": [
    "#### Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "c16fcd69",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _init_worker_singlethread():\n",
    "    # Keep each worker process single-threaded for math libs\n",
    "    os.environ.setdefault(\"OMP_NUM_THREADS\", \"1\")\n",
    "    os.environ.setdefault(\"OPENBLAS_NUM_THREADS\", \"1\")\n",
    "    os.environ.setdefault(\"MKL_NUM_THREADS\", \"1\")\n",
    "    os.environ.setdefault(\"VECLIB_MAXIMUM_THREADS\", \"1\")\n",
    "    try:\n",
    "        # This caps numpy/scipy threadpools too (if available)\n",
    "        from threadpoolctl import threadpool_limits\n",
    "        threadpool_limits(1)\n",
    "    except Exception:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "41826c28",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _iter_results_local(job_args, workers: int):\n",
    "    import multiprocessing as mp\n",
    "    # Use 'fork' on macOS/Linux; safer in notebooks\n",
    "    ctx = mp.get_context(\"fork\")\n",
    "    with ctx.Pool(processes=workers, initializer=_init_worker_singlethread) as pool:\n",
    "        # tune chunksize if needed\n",
    "        for res in pool.imap(_solve_cityweek_worker, job_args, chunksize=1):\n",
    "            yield res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "ffcf483f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _iter_results_mpi(job_args, workers: int):\n",
    "    try:\n",
    "        from mpi4py.futures import MPIPoolExecutor\n",
    "    except Exception as e:\n",
    "        raise RuntimeError(\n",
    "            \"MPI requested but mpi4py is not available. \"\n",
    "            \"Install mpi4py and run with mpirun/mpiexec.\"\n",
    "        ) from e\n",
    "    with MPIPoolExecutor(max_workers=workers) as ex:\n",
    "        for res in ex.map(_solve_cityweek_worker, job_args, chunksize=1):\n",
    "            yield res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "fc38546f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- TOP-LEVEL WORKER (must live at module scope) ---\n",
    "def _solve_cityweek_worker(args):\n",
    "    (\n",
    "        city, week_ts, df_cityweek,\n",
    "        policy, solver,\n",
    "        slot_len, W_slots,\n",
    "        cityday_ca_order,               # dict[(city, day)->list of ca_id] or None\n",
    "        emit_optimised_rows\n",
    "    ) = args\n",
    "\n",
    "    m_rows, mv_rows, o_rows = [], [], []\n",
    "\n",
    "    # Weekly move counters per customer\n",
    "    weekly_quota = dict.fromkeys(\n",
    "        df_cityweek[\"ca_id\"].unique().tolist(),\n",
    "        policy.behavioral.customer_power_moves_per_week\n",
    "    )\n",
    "\n",
    "    for day_ts, df_city_day in df_cityweek.groupby(\"day\", sort=True, group_keys=False):\n",
    "        print(\"DEBUG: entered day loop\")\n",
    "        # float32 city-day arrays\n",
    "        base_city = cityday_baseline_by_slot(df_city_day, slot_len_min=slot_len, dtype=np.float32)\n",
    "        alpha = (policy.spike_cap.alpha_peak_cap_percent / 100.0) if policy.spike_cap else 0.25\n",
    "        post_city_cap  = (1.0 + np.float32(alpha)) * base_city.astype(np.float32, copy=False)\n",
    "        post_city_used = np.zeros_like(post_city_cap, dtype=np.float32)\n",
    "\n",
    "        print(\"DEBUG: city cap calculated\")\n",
    "        # regional moved budget (float32)\n",
    "        if policy.regional_cap:\n",
    "            P_pct = np.float32(policy.regional_cap.regional_load_shift_percent_limit / 100.0)\n",
    "            city_daily_avg = np.float32(\n",
    "                policy.regional_cap.regional_total_daily_average_load_kWh.get(city, 0.0)\n",
    "            )\n",
    "            moved_budget_remaining = np.float32(P_pct * city_daily_avg)\n",
    "        else:\n",
    "            moved_budget_remaining = np.float32(0.0)\n",
    "        print(\"DEBUG: moved budget calculated\")\n",
    "\n",
    "        # precomputed order\n",
    "        if cityday_ca_order is not None:\n",
    "            order = cityday_ca_order.get((city, day_ts))\n",
    "            if order is None:\n",
    "                order = df_city_day[\"ca_id\"].drop_duplicates().tolist()\n",
    "        else:\n",
    "            order = (\n",
    "                df_city_day.groupby(\"ca_id\", as_index=False)[\"value\"].sum()\n",
    "                           .sort_values(\"value\", ascending=False)[\"ca_id\"]\n",
    "                           .tolist()\n",
    "            )\n",
    "        print(\"DEBUG: precomputed order determined\")\n",
    "\n",
    "        # PRE-SPLIT once per day: avoid repeated boolean filters on Arrow strings\n",
    "        by_ca = {cid: g for cid, g in df_city_day.groupby(\"ca_id\", sort=False)}\n",
    "\n",
    "        for ca_id in order:\n",
    "            # print(f\"DEBUG Entering customer loop - processing {ca_id}\")\n",
    "            if weekly_quota.get(ca_id, 0) <= 0:\n",
    "                continue\n",
    "\n",
    "            # O(1) lookup instead of df_city_day[df_city_day[\"ca_id\"] == ca_id]\n",
    "            sub = by_ca.get(ca_id)\n",
    "            if sub is None or sub.empty:\n",
    "                continue\n",
    "\n",
    "            arrs = build_arrays_for_group_pd(sub, slot_len_min=slot_len)\n",
    "            if arrs is None:\n",
    "                continue\n",
    "\n",
    "            usage = arrs[\"usage\"].astype(np.float32, copy=False)\n",
    "            mef   = arrs[\"mef\"].astype(np.float32, copy=False)\n",
    "            aef   = arrs[\"aef\"].astype(np.float32, copy=False)\n",
    "            floor_vec = arrs.get(\"floor\")\n",
    "            if floor_vec is not None:\n",
    "                floor_vec = floor_vec.astype(np.float32, copy=False)\n",
    "\n",
    "            peak_cfg = policy.behavioral.peak_hours_reduction_limit_config\n",
    "            peak_mask, peak_groups, Z, cap_mode = city_peak_targets_for_day(\n",
    "                city, day_ts, peak_cfg, slot_len_min=slot_len\n",
    "            )\n",
    "            if Z is not None:\n",
    "                Z = np.float32(Z)\n",
    "\n",
    "            K_sources = int(max(0, min(policy.behavioral.customer_power_moves_per_day,\n",
    "                                       weekly_quota[ca_id])))\n",
    "\n",
    "            resid_cap = np.maximum(np.float32(0.0), post_city_cap - post_city_used).astype(np.float32, copy=False)\n",
    "            moved_kwh_cap = np.float32(max(0.0, float(moved_budget_remaining)))\n",
    "\n",
    "            # print(\"DEBUG: prepped usage, mef, aef, peak_mask, resid_cap, moved_kwh_cap\")\n",
    "            # print(\"DEBUG: entering solver\")\n",
    "            # solve\n",
    "            if solver.solver_family == \"milp\":\n",
    "                usage_opt, flows = solve_milp_k(\n",
    "                    mef=mef, usage=usage, W_slots=W_slots, cfg=solver, max_moves=K_sources,\n",
    "                    peak_mask=peak_mask, peak_groups=peak_groups, Z=Z, cap_mode=cap_mode,\n",
    "                    floor_vec=floor_vec, dest_upper_bounds=resid_cap, moved_kwh_cap=float(moved_kwh_cap),\n",
    "                )\n",
    "            else:\n",
    "                usage_opt, flows, used_dest, used_reg = greedy_k_moves(\n",
    "                    usage=usage, mef=mef, aef=aef,\n",
    "                    W_slots=W_slots, slot_len_min=slot_len, floor_vec=floor_vec,\n",
    "                    peak_mask=peak_mask, peak_groups=peak_groups, Z=Z, cap_mode=cap_mode,\n",
    "                    dest_rem=resid_cap.copy(), reg_rem=float(moved_kwh_cap), max_moves=K_sources,\n",
    "                )\n",
    "\n",
    "            # print(\"DEBUG: solver finished\")\n",
    "            usage_opt = usage_opt.astype(np.float32, copy=False)\n",
    "\n",
    "            # update tallies\n",
    "            post_city_used += usage_opt\n",
    "            moved_kwh = float(np.maximum(np.float32(0.0), usage - usage_opt).sum(dtype=np.float32))\n",
    "            moved_budget_remaining = np.maximum(np.float32(0.0), moved_budget_remaining - np.float32(moved_kwh))\n",
    "            weekly_moves_used = len({t for (t, s, _) in flows if s != t})\n",
    "            weekly_quota[ca_id] = max(0, weekly_quota[ca_id] - weekly_moves_used)\n",
    "\n",
    "            # print(\"DEBUG: updated weekly quota\")\n",
    "            # metrics row (emit as float64 for reporting)\n",
    "            base = compute_emissions_totals(usage.astype(float), aef.astype(float), mef.astype(float))\n",
    "            post = compute_emissions_totals(usage_opt.astype(float), aef.astype(float), mef.astype(float))\n",
    "            median_shift = weighted_median_shift_minutes(flows, slot_len)\n",
    "            m_rows.append({\n",
    "                \"ca_id\": ca_id, \"city\": city, \"day\": day_ts,\n",
    "                \"baseline_E_avg_g\": base[\"E_avg_g\"], \"post_E_avg_g\": post[\"E_avg_g\"],\n",
    "                \"delta_E_avg_g\": base[\"E_avg_g\"] - post[\"E_avg_g\"],\n",
    "                \"baseline_E_marg_g\": base[\"E_marg_g\"], \"post_E_marg_g\": post[\"E_marg_g\"],\n",
    "                \"delta_E_marg_g\": base[\"E_marg_g\"] - post[\"E_marg_g\"],\n",
    "                \"baseline_kwh\": float(np.sum(usage, dtype=np.float32)),\n",
    "                \"post_kwh\": float(np.sum(usage_opt, dtype=np.float32)),\n",
    "                \"moved_kwh\": moved_kwh,\n",
    "                \"avg_shift_minutes_energy_weighted\": float(\n",
    "                    (sum(abs(s - t) * val for (t, s, val) in flows) / max(moved_kwh, 1e-9)) * slot_len\n",
    "                ) if flows else 0.0,\n",
    "                \"median_shift_minutes_energy_weighted\": median_shift,\n",
    "                \"weekly_moves_used\": weekly_moves_used,\n",
    "                \"weekly_moves_remaining\": weekly_quota[ca_id],\n",
    "            })\n",
    "            # print(\"DEBUG: metrics row added\")\n",
    "            # moves\n",
    "            mv_rows.extend(\n",
    "                flows_table(ca_id, city, day_ts, flows, mef.astype(float), aef.astype(float), slot_len_min=slot_len)\n",
    "            )\n",
    "\n",
    "            # optional per-slot outputs\n",
    "            if emit_optimised_rows:\n",
    "                for s in range(len(usage_opt)):\n",
    "                    ts = day_ts + timedelta(minutes=int(s * slot_len))\n",
    "                    o_rows.append({\n",
    "                        \"ca_id\": ca_id, \"city\": city, \"day\": day_ts,\n",
    "                        \"slot\": s, \"date\": ts, \"optimised_value\": float(usage_opt[s]),\n",
    "                    })\n",
    "        # print(\"DEBUG: customer loop\")\n",
    "    # print(\"DEBUG: exiting day loop\")\n",
    "    return m_rows, mv_rows, o_rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "5f9b174b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_pipeline_pandas_cityweek_budget(\n",
    "    df_pd: pd.DataFrame,\n",
    "    policy: ShiftPolicy,\n",
    "    solver: SolverConfig,\n",
    "    *,\n",
    "    shuffle_high_usage_order: bool = False,\n",
    "    emit_optimised_rows: bool = True,\n",
    "    workers: int = 1,\n",
    "    show_progress: bool = True,\n",
    "    cityday_ca_order: Optional[Dict[Tuple[str, pd.Timestamp], List[str]]] = None,\n",
    "    backend: Literal[\"local\",\"mpi\"] = \"local\",   # <— NEW\n",
    ") -> Tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame]:\n",
    "\n",
    "    # derive week_start once\n",
    "    df = add_week_start_col(df_pd, policy.behavioral.week_boundaries)\n",
    "\n",
    "    df = df.sort_values([\"city\", \"week_start\", \"day\", \"ca_id\", \"slot\"])\n",
    "    groups = list(df.groupby([\"city\",\"week_start\"], sort=True, group_keys=False))\n",
    "    total = len(groups)\n",
    "\n",
    "    # hoist constants\n",
    "    slot_len = policy.behavioral.slot_length_minutes\n",
    "    W_slots = hours_to_slots(policy.behavioral.shift_hours_window, slot_len)\n",
    "\n",
    "    metrics_rows: List[Dict[str, Any]] = []\n",
    "    move_rows: List[Dict[str, Any]] = []\n",
    "    opt_rows:  List[Dict[str, Any]] = []\n",
    "\n",
    "    # Build job args (must be picklable)\n",
    "    job_args = [\n",
    "        (city, wk, df_cw, policy, solver, slot_len, W_slots, cityday_ca_order, emit_optimised_rows)\n",
    "        for (city, wk), df_cw in groups\n",
    "    ]\n",
    "\n",
    "    workers = min(workers or 1, total)\n",
    "\n",
    "    if workers > 1:\n",
    "        if backend == \"mpi\":\n",
    "            iterator = _iter_results_mpi(job_args, workers)\n",
    "        else:\n",
    "            iterator = _iter_results_local(job_args, workers)\n",
    "\n",
    "        for i, (m, mv, o) in enumerate(iterator, 1):\n",
    "            metrics_rows.extend(m); move_rows.extend(mv); opt_rows.extend(o)\n",
    "            if show_progress and (i % max(1, total//20) == 0 or i == total):\n",
    "                print(f\"[progress] {i}/{total} city-weeks processed ({i/total*100:.1f}%)\")\n",
    "    else:\n",
    "        for i, args in enumerate(job_args, 1):\n",
    "            m, mv, o = _solve_cityweek_worker(args)\n",
    "            metrics_rows.extend(m); move_rows.extend(mv); opt_rows.extend(o)\n",
    "            if show_progress and (i % max(1, total//20) == 0 or i == total):\n",
    "                print(f\"[progress] {i}/{total} city-weeks processed ({i/total*100:.1f}%)\")\n",
    "\n",
    "    return pd.DataFrame(metrics_rows), pd.DataFrame(move_rows), pd.DataFrame(opt_rows)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad3dfb3a",
   "metadata": {},
   "source": [
    "### Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fb3a7fb",
   "metadata": {},
   "source": [
    "Regional Total Daily Average Loads\n",
    "\n",
    "Sources:\n",
    "* Delhi : https://www.ceicdata.com/en/india/electricity-consumption-utilities/electricity-consumption-utilities-delhi\n",
    "* Maharashtra : https://www.ceicdata.com/en/india/electricity-consumption-utilities/electricity-consumption-utilities-maharashtra\n",
    "\n",
    "Values:\n",
    "* Delhi Annual Electricity Consumption (2023): 34,107.000 GWh\n",
    "* Maharashtra Annual Electricity Consumption (2023): 155,518.000 GWh\n",
    "\n",
    "Logic:\n",
    "* Delhi : 34,107.000 GWh Annual / 365 = 93.500 GWh Daily / 24  = 3.937 GWh Hourly\n",
    "* Maharashtra : 155,518.000 GWh Annual / 365 = 426.000 GWh Daily / 24 = 17.750 GWh Hourly\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "f2c4feb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "delhi_total_daily_average_load_gWh = 3.937  # GWh\n",
    "maharashtra_total_daily_average_load_gWh = 17.750  # GWh\n",
    "\n",
    "delhi_total_daily_average_load_kWh = delhi_total_daily_average_load_gWh * 1_000_000 # convert GWh to kWh\n",
    "maharashtra_total_daily_average_load_kWh = maharashtra_total_daily_average_load_gWh * 1_000_000 # convert GWh to kWh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "f10956b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "peak_hours_reduction_limit_config = PeakHoursReductionLimitConfig(\n",
    "    peak_hours_reduction_percent_limit=25.0,\n",
    "    peak_hours_reduction_scope=\"per_city\",\n",
    "    peak_hours_dict={\n",
    "        \"delhi\": {\n",
    "            \"Mon\": [8,9,10,11,12,20],\n",
    "            \"Tue\": [9,10,11,20,21,22],\n",
    "            \"Wed\": [9,10,11,20,21,22],\n",
    "            \"Thu\": [9,10,11,20,21,22],\n",
    "            \"Fri\": [8,9,10,11,20,21],\n",
    "            \"Sat\": [9,10,11,12,13,20],\n",
    "            \"Sun\": [10,11,12,13,14,20],\n",
    "        },\n",
    "        \"mumbai\": {\n",
    "            \"Mon\": [8,9,10,11,12,20],\n",
    "            \"Tue\": [9,10,11,20,21,22],\n",
    "            \"Wed\": [9,10,11,20,21,22],\n",
    "            \"Thu\": [9,10,11,20,21,22],\n",
    "            \"Fri\": [8,9,10,11,20,21],\n",
    "            \"Sat\": [9,10,11,12,13,20],\n",
    "            \"Sun\": [10,11,12,13,14,20],\n",
    "        },\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "8aaccbcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "policy = ShiftPolicy(\n",
    "    behavioral=CustomerAdoptionBehavioralConfig(\n",
    "                        customer_power_moves_per_day=1,\n",
    "                        customer_power_moves_per_week=3,\n",
    "                        timezone=\"Asia/Kolkata\",\n",
    "                        day_boundaries=\"00:00-24:00\",\n",
    "                        week_boundaries=\"Mon-Sun\",\n",
    "                        shift_hours_window=2.0,\n",
    "                        slot_length_minutes=30,\n",
    "                        peak_hours_reduction_limit_config=peak_hours_reduction_limit_config\n",
    "                    ),\n",
    "    regional_cap=RegionalLoadShiftingLimitConfig(\n",
    "                        regional_load_shift_percent_limit=10.0,\n",
    "                        regional_total_daily_average_load_kWh={\"delhi\": delhi_total_daily_average_load_kWh,\n",
    "                                                            \"mumbai\": maharashtra_total_daily_average_load_kWh}),\n",
    "    household_min=HouseholdMinimumConsumptionLimitConfig(\n",
    "                        household_minimum_baseline_period=\"year\",\n",
    "                        household_minimum_baseline_type=\"average\",\n",
    "                        household_minimum_robust_max_percentile=95,\n",
    "                        household_minimum_R_percent=10),\n",
    "    spike_cap=ShiftWithoutSpikeLimitConfig(alpha_peak_cap_percent=25),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "954a0104",
   "metadata": {},
   "outputs": [],
   "source": [
    "# marginal_emissions_pldf.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "9b19972c",
   "metadata": {},
   "outputs": [],
   "source": [
    "pl_df = day_and_slot_cols(marginal_emissions_pldf, slot_len_min=policy.behavioral.slot_length_minutes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "3d54e440",
   "metadata": {},
   "outputs": [],
   "source": [
    "if policy.household_min is not None:\n",
    "    pl_df = attach_household_floor(pl_df, policy.household_min)\n",
    "\n",
    "# usually takes about 40s to run on a two week dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "013fdd66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2) Keep only needed columns\n",
    "cols_needed = [\n",
    "    \"ca_id\",\"city\",\"date\",\"day\",\"slot\",\"value\",\n",
    "    \"marginal_emissions_factor_grams_co2_per_kWh\",\n",
    "    \"average_emissions_factor_grams_co2_per_kWh\",\n",
    "]\n",
    "if \"floor_kwh\" in pl_df.columns:\n",
    "    cols_needed.append(\"floor_kwh\")\n",
    "pl_df = pl_df.select(cols_needed).sort([\"ca_id\",\"day\",\"slot\"])\n",
    "\n",
    "# takes ~16 seconds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "fccdc91b",
   "metadata": {},
   "outputs": [],
   "source": [
    "cityday_ca_order = build_cityday_ca_order_map_pl(pl_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "b5095e5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "pct_tbl = compute_city_day_percentiles(pl_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "91333002",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to pandas for better paralellisation\n",
    "df_pd = pl_df.to_pandas(use_pyarrow_extension_array=False)  # keeps dictionary cols efficient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "13547914",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ca_id</th>\n",
       "      <th>city</th>\n",
       "      <th>date</th>\n",
       "      <th>day</th>\n",
       "      <th>slot</th>\n",
       "      <th>value</th>\n",
       "      <th>marginal_emissions_factor_grams_co2_per_kWh</th>\n",
       "      <th>average_emissions_factor_grams_co2_per_kWh</th>\n",
       "      <th>floor_kwh</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>60000005516</td>\n",
       "      <td>delhi</td>\n",
       "      <td>2022-05-04 00:00:00+05:30</td>\n",
       "      <td>2022-05-04 00:00:00+05:30</td>\n",
       "      <td>0</td>\n",
       "      <td>0.002</td>\n",
       "      <td>738.426404</td>\n",
       "      <td>763.740246</td>\n",
       "      <td>0.00220</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>60000005516</td>\n",
       "      <td>delhi</td>\n",
       "      <td>2022-05-04 00:30:00+05:30</td>\n",
       "      <td>2022-05-04 00:00:00+05:30</td>\n",
       "      <td>1</td>\n",
       "      <td>0.002</td>\n",
       "      <td>691.052088</td>\n",
       "      <td>768.351440</td>\n",
       "      <td>0.00220</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>60000005516</td>\n",
       "      <td>delhi</td>\n",
       "      <td>2022-05-04 01:00:00+05:30</td>\n",
       "      <td>2022-05-04 00:00:00+05:30</td>\n",
       "      <td>2</td>\n",
       "      <td>0.002</td>\n",
       "      <td>637.266463</td>\n",
       "      <td>774.318949</td>\n",
       "      <td>0.00225</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>60000005516</td>\n",
       "      <td>delhi</td>\n",
       "      <td>2022-05-04 01:30:00+05:30</td>\n",
       "      <td>2022-05-04 00:00:00+05:30</td>\n",
       "      <td>3</td>\n",
       "      <td>0.002</td>\n",
       "      <td>646.189219</td>\n",
       "      <td>783.035512</td>\n",
       "      <td>0.00225</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>60000005516</td>\n",
       "      <td>delhi</td>\n",
       "      <td>2022-05-04 02:00:00+05:30</td>\n",
       "      <td>2022-05-04 00:00:00+05:30</td>\n",
       "      <td>4</td>\n",
       "      <td>0.002</td>\n",
       "      <td>682.124021</td>\n",
       "      <td>789.819035</td>\n",
       "      <td>0.00225</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         ca_id   city                      date                       day  \\\n",
       "0  60000005516  delhi 2022-05-04 00:00:00+05:30 2022-05-04 00:00:00+05:30   \n",
       "1  60000005516  delhi 2022-05-04 00:30:00+05:30 2022-05-04 00:00:00+05:30   \n",
       "2  60000005516  delhi 2022-05-04 01:00:00+05:30 2022-05-04 00:00:00+05:30   \n",
       "3  60000005516  delhi 2022-05-04 01:30:00+05:30 2022-05-04 00:00:00+05:30   \n",
       "4  60000005516  delhi 2022-05-04 02:00:00+05:30 2022-05-04 00:00:00+05:30   \n",
       "\n",
       "   slot  value  marginal_emissions_factor_grams_co2_per_kWh  \\\n",
       "0     0  0.002                                   738.426404   \n",
       "1     1  0.002                                   691.052088   \n",
       "2     2  0.002                                   637.266463   \n",
       "3     3  0.002                                   646.189219   \n",
       "4     4  0.002                                   682.124021   \n",
       "\n",
       "   average_emissions_factor_grams_co2_per_kWh  floor_kwh  \n",
       "0                                  763.740246    0.00220  \n",
       "1                                  768.351440    0.00220  \n",
       "2                                  774.318949    0.00225  \n",
       "3                                  783.035512    0.00225  \n",
       "4                                  789.819035    0.00225  "
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_pd.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "aed3274e",
   "metadata": {},
   "outputs": [],
   "source": [
    "solver = SolverConfig(solver_family=\"greedy\",)\n",
    "parallel = ParallelConfig(enabled=False, method=\"local\", workers=1, show_progress=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16df5c9b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1'"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "os.environ.setdefault(\"VECLIB_MAXIMUM_THREADS\", \"1\")\n",
    "os.environ.setdefault(\"OMP_NUM_THREADS\", \"1\")\n",
    "os.environ.setdefault(\"OPENBLAS_NUM_THREADS\", \"1\")\n",
    "os.environ.setdefault(\"MKL_NUM_THREADS\", \"1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b64b63b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/lg/rt7jvg4x71vd57p0zm_718200000gn/T/ipykernel_70283/3844934046.py:18: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n",
      "  groups = list(df.groupby([\"city\",\"week_start\"], sort=True, group_keys=False))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEBUG: entered day loop\n",
      "DEBUG: city cap calculated\n",
      "DEBUG: moved budget calculated\n",
      "DEBUG: precomputed order determined\n",
      "DEBUG: entered day loop\n",
      "DEBUG: city cap calculated\n",
      "DEBUG: moved budget calculated\n",
      "DEBUG: precomputed order determined\n",
      "DEBUG: entered day loop\n",
      "DEBUG: city cap calculated\n",
      "DEBUG: moved budget calculated\n",
      "DEBUG: precomputed order determined\n",
      "DEBUG: entered day loop\n",
      "DEBUG: city cap calculated\n",
      "DEBUG: moved budget calculated\n",
      "DEBUG: precomputed order determined\n",
      "DEBUG: entered day loop\n",
      "DEBUG: city cap calculated\n",
      "DEBUG: moved budget calculated\n",
      "DEBUG: precomputed order determined\n",
      "[progress] 1/3 city-weeks processed (33.3%)\n",
      "DEBUG: entered day loop\n",
      "DEBUG: city cap calculated\n",
      "DEBUG: moved budget calculated\n",
      "DEBUG: precomputed order determined\n",
      "DEBUG: entered day loop\n",
      "DEBUG: city cap calculated\n",
      "DEBUG: moved budget calculated\n",
      "DEBUG: precomputed order determined\n",
      "DEBUG: entered day loop\n",
      "DEBUG: city cap calculated\n",
      "DEBUG: moved budget calculated\n",
      "DEBUG: precomputed order determined\n"
     ]
    }
   ],
   "source": [
    "# 5) Run the city-week, K-moves pipeline (MILP or Greedy)\n",
    "metrics_df, moves_df, opt_df = run_pipeline_pandas_cityweek_budget(\n",
    "    df_pd=df_pd,\n",
    "    policy=policy,\n",
    "    solver=solver,          # set solver.solver_family to \"milp\" or \"greedy\"\n",
    "    shuffle_high_usage_order=False,\n",
    "    emit_optimised_rows=True,\n",
    "    show_progress=parallel.show_progress,\n",
    "    workers=parallel.workers,  # number of parallel workers\n",
    "    cityday_ca_order=cityday_ca_order,   # pass the precomputed order\n",
    "    backend=parallel.method,  # pass the backend\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d13ea3cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_directory = os.path.join(optimisation_development_directory, \"results\")\n",
    "os.makedirs(results_directory, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "087a776b",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_df.to_parquet(os.path.join(results_directory, \"metrics_config_2_greedy.parquet\"))\n",
    "moves_df.to_parquet(os.path.join(results_directory, \"moves_config_2_greedy.parquet\"))\n",
    "opt_df.to_parquet(os.path.join(results_directory, \"optimised_config_2_greedy.parquet\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d0b6f7a",
   "metadata": {},
   "source": [
    "#### CONFIGURATION 1 - Lax with the limits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "7d9b40f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "peak_hours_reduction_limit_config_1 = PeakHoursReductionLimitConfig(\n",
    "        peak_hours_reduction_percent_limit=80,\n",
    "        peak_hours_reduction_scope=\"per_city\",\n",
    "        peak_hours_dict={\n",
    "            \"delhi\": {\n",
    "                \"Mon\": [8,9,10,11,12,20],\n",
    "                \"Tue\": [9,10,11,20,21,22],\n",
    "                \"Wed\": [9,10,11,20,21,22],\n",
    "                \"Thu\": [9,10,11,20,21,22],\n",
    "                \"Fri\": [8,9,10,11,20,21],\n",
    "                \"Sat\": [9,10,11,12,13,20],\n",
    "                \"Sun\": [10,11,12,13,14,20],\n",
    "            },\n",
    "            \"mumbai\": {\n",
    "                \"Mon\": [8,9,10,11,12,20],\n",
    "                \"Tue\": [9,10,11,20,21,22],\n",
    "                \"Wed\": [9,10,11,20,21,22],\n",
    "                \"Thu\": [9,10,11,20,21,22],\n",
    "                \"Fri\": [8,9,10,11,20,21],\n",
    "                \"Sat\": [9,10,11,12,13,20],\n",
    "                \"Sun\": [10,11,12,13,14,20],\n",
    "            },\n",
    "        },\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "67b1cd89",
   "metadata": {},
   "outputs": [],
   "source": [
    "policy_1 = ShiftPolicy(\n",
    "        behavioral=CustomerAdoptionBehavioralConfig(\n",
    "                            customer_power_moves_per_day=2,\n",
    "                            customer_power_moves_per_week=7,\n",
    "                            timezone=\"Asia/Kolkata\",\n",
    "                            day_boundaries=\"00:00-24:00\",\n",
    "                            week_boundaries=\"Mon-Sun\",\n",
    "                            shift_hours_window=3.0,\n",
    "                            slot_length_minutes=30,\n",
    "                            peak_hours_reduction_limit_config=peak_hours_reduction_limit_config_1\n",
    "                        ),\n",
    "        regional_cap=RegionalLoadShiftingLimitConfig(\n",
    "                            regional_load_shift_percent_limit=10.0,\n",
    "                            regional_total_daily_average_load_kWh={\"delhi\": delhi_total_daily_average_load_kWh,\n",
    "                                                                \"mumbai\": maharashtra_total_daily_average_load_kWh}),\n",
    "        household_min=HouseholdMinimumConsumptionLimitConfig(\n",
    "                            household_minimum_baseline_period=\"year\",\n",
    "                            household_minimum_baseline_type=\"average\",\n",
    "                            household_minimum_robust_max_percentile=95,\n",
    "                            household_minimum_R_percent=10),\n",
    "        spike_cap=ShiftWithoutSpikeLimitConfig(alpha_peak_cap_percent=25),\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ef7cc08",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "parallel_1 = ParallelConfig(enabled=False, method=\"local\", workers=1, show_progress=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03f62f0e",
   "metadata": {},
   "source": [
    "##### CONFIGURATION 1 - GREEDY SOLVER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "198d96d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/lg/rt7jvg4x71vd57p0zm_718200000gn/T/ipykernel_79527/3844934046.py:18: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n",
      "  groups = list(df.groupby([\"city\",\"week_start\"], sort=True, group_keys=False))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEBUG: entered day loop\n",
      "DEBUG: city cap calculated\n",
      "DEBUG: moved budget calculated\n",
      "DEBUG: precomputed order determined\n"
     ]
    }
   ],
   "source": [
    "solver_1_greedy = SolverConfig(solver_family=\"greedy\")\n",
    "\n",
    "marginal_emissions_pldf_run_1_greedy = day_and_slot_cols(marginal_emissions_pldf, slot_len_min=policy_1.behavioral.slot_length_minutes)\n",
    "\n",
    "if policy_1.household_min is not None:\n",
    "    marginal_emissions_pldf_run_1_greedy = attach_household_floor(df=marginal_emissions_pldf_run_1_greedy, cfg=policy_1.household_min)\n",
    "\n",
    "cols_needed = [\n",
    "        \"ca_id\",\"city\",\"date\",\"day\",\"slot\",\"value\",\n",
    "        \"marginal_emissions_factor_grams_co2_per_kWh\",\n",
    "        \"average_emissions_factor_grams_co2_per_kWh\",\n",
    "]\n",
    "if \"floor_kwh\" in marginal_emissions_pldf_run_1_greedy.columns:\n",
    "        cols_needed.append(\"floor_kwh\")\n",
    "\n",
    "marginal_emissions_pldf_run_1_greedy = marginal_emissions_pldf_run_1_greedy.select(cols_needed).sort([\"ca_id\",\"day\",\"slot\"])\n",
    "\n",
    "cityday_ca_order_1_greedy = build_cityday_ca_order_map_pl(marginal_emissions_pldf_run_1_greedy)\n",
    "\n",
    "pct_tbl_1_greedy = compute_city_day_percentiles(marginal_emissions_pldf_run_1_greedy)\n",
    "\n",
    "marginal_emissions_pldf_run_1_greedy = marginal_emissions_pldf_run_1_greedy.to_pandas()  # keeps dictionary cols efficient\n",
    "\n",
    "metrics_df_1_greedy, moves_df_1_greedy, opt_df_1_greedy = run_pipeline_pandas_cityweek_budget(\n",
    "        df_pd=marginal_emissions_pldf_run_1_greedy,\n",
    "        policy=policy_1,\n",
    "        solver=solver_1_greedy,          # set solver.solver_family to \"milp\" or \"greedy\"\n",
    "        shuffle_high_usage_order=False,\n",
    "        emit_optimised_rows=True,\n",
    "        show_progress=True,\n",
    "        workers=parallel_1.workers,  # number of parallel workers\n",
    "        cityday_ca_order=cityday_ca_order_1_greedy,   # pass the precomputed order\n",
    "        backend=parallel_1.method,  # pass the backend\n",
    ")\n",
    "\n",
    "metrics_df_1_greedy.to_parquet(os.path.join(results_directory, \"metrics_config_1_greedy.parquet\"))\n",
    "moves_df_1_greedy.to_parquet(os.path.join(results_directory, \"moves_config_1_greedy.parquet\"))\n",
    "opt_df_1_greedy.to_parquet(os.path.join(results_directory, \"optimised_config_1_greedy.parquet\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1aee4ece",
   "metadata": {},
   "source": [
    "##### CONFIGURATION 1 - MILP SOLVER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "842bd1e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "solver_1_milp = SolverConfig(solver_family=\"milp\")\n",
    "\n",
    "# IMPLEMENTATION 1 - MILP\n",
    "\n",
    "marginal_emissions_pldf_run_1_milp = day_and_slot_cols(marginal_emissions_pldf, slot_len_min=policy_1.behavioral.slot_length_minutes)\n",
    "\n",
    "if policy_1.household_min is not None:\n",
    "        marginal_emissions_pldf_run_1_milp = attach_household_floor(df=marginal_emissions_pldf_run_1_milp, cfg=policy_1.household_min)\n",
    "\n",
    "cols_needed = [\n",
    "        \"ca_id\",\"city\",\"date\",\"day\",\"slot\",\"value\",\n",
    "        \"marginal_emissions_factor_grams_co2_per_kWh\",\n",
    "        \"average_emissions_factor_grams_co2_per_kWh\",\n",
    "]\n",
    "if \"floor_kwh\" in marginal_emissions_pldf_run_1_milp.columns:\n",
    "        cols_needed.append(\"floor_kwh\")\n",
    "\n",
    "marginal_emissions_pldf_run_1_milp = marginal_emissions_pldf_run_1_milp.select(cols_needed).sort([\"ca_id\",\"day\",\"slot\"])\n",
    "\n",
    "cityday_ca_order_1_milp = build_cityday_ca_order_map_pl(marginal_emissions_pldf_run_1_milp)\n",
    "\n",
    "pct_tbl_1_milp = compute_city_day_percentiles(marginal_emissions_pldf_run_1_milp)\n",
    "\n",
    "marginal_emissions_pldf_run_1_milp = marginal_emissions_pldf_run_1_milp.to_pandas()  # keeps dictionary cols efficient\n",
    "\n",
    "metrics_df_1_milp, moves_df_1_milp, opt_df_1_milp = run_pipeline_pandas_cityweek_budget(\n",
    "        df_pd=marginal_emissions_pldf_run_1_milp,\n",
    "        policy=policy_1,\n",
    "        solver=solver_1_milp,          # set solver.solver_family to \"milp\" or \"greedy\"\n",
    "        shuffle_high_usage_order=False,\n",
    "        emit_optimised_rows=True,\n",
    "        show_progress=True,\n",
    "        workers=parallel_1.workers,  # number of parallel workers\n",
    "        cityday_ca_order=cityday_ca_order_1_milp,   # pass the precomputed order\n",
    "        backend=parallel_1.method,  # pass the backend\n",
    ")\n",
    "\n",
    "metrics_df_1_milp.to_parquet(os.path.join(results_directory, \"metrics_config_1_milp.parquet\"))\n",
    "moves_df_1_milp.to_parquet(os.path.join(results_directory, \"moves_config_1_milp.parquet\"))\n",
    "opt_df_1_milp.to_parquet(os.path.join(results_directory, \"optimised_config_1_milp.parquet\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "218965a3",
   "metadata": {},
   "source": [
    "#### CONFIGURATION 2 - RESTRICTED"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb5bc631",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CONFIGURATION 2:\n",
    "peak_hours_reduction_limit_config_2 = PeakHoursReductionLimitConfig(\n",
    "        peak_hours_reduction_percent_limit=25.0,\n",
    "        peak_hours_reduction_scope=\"per_city\",\n",
    "        peak_hours_dict={\n",
    "            \"delhi\": {\n",
    "                \"Mon\": [8,9,10,11,12,20],\n",
    "                \"Tue\": [9,10,11,20,21,22],\n",
    "                \"Wed\": [9,10,11,20,21,22],\n",
    "                \"Thu\": [9,10,11,20,21,22],\n",
    "                \"Fri\": [8,9,10,11,20,21],\n",
    "                \"Sat\": [9,10,11,12,13,20],\n",
    "                \"Sun\": [10,11,12,13,14,20],\n",
    "            },\n",
    "            \"mumbai\": {\n",
    "                \"Mon\": [8,9,10,11,12,20],\n",
    "                \"Tue\": [9,10,11,20,21,22],\n",
    "                \"Wed\": [9,10,11,20,21,22],\n",
    "                \"Thu\": [9,10,11,20,21,22],\n",
    "                \"Fri\": [8,9,10,11,20,21],\n",
    "                \"Sat\": [9,10,11,12,13,20],\n",
    "                \"Sun\": [10,11,12,13,14,20],\n",
    "            },\n",
    "        },\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebfca88f",
   "metadata": {},
   "outputs": [],
   "source": [
    "policy_2 = ShiftPolicy(\n",
    "        behavioral=CustomerAdoptionBehavioralConfig(\n",
    "                            customer_power_moves_per_day=1,\n",
    "                            customer_power_moves_per_week=3,\n",
    "                            timezone=\"Asia/Kolkata\",\n",
    "                            day_boundaries=\"00:00-24:00\",\n",
    "                            week_boundaries=\"Mon-Sun\",\n",
    "                            shift_hours_window=2.0,\n",
    "                            slot_length_minutes=30,\n",
    "                            peak_hours_reduction_limit_config=peak_hours_reduction_limit_config_2\n",
    "                        ),\n",
    "        regional_cap=RegionalLoadShiftingLimitConfig(\n",
    "                            regional_load_shift_percent_limit=10.0,\n",
    "                            regional_total_daily_average_load_kWh={\"delhi\": delhi_total_daily_average_load_kWh,\n",
    "                                                                \"mumbai\": maharashtra_total_daily_average_load_kWh}),\n",
    "        household_min=HouseholdMinimumConsumptionLimitConfig(\n",
    "                            household_minimum_baseline_period=\"year\",\n",
    "                            household_minimum_baseline_type=\"average\",\n",
    "                            household_minimum_robust_max_percentile=95,\n",
    "                            household_minimum_R_percent=10),\n",
    "        spike_cap=ShiftWithoutSpikeLimitConfig(alpha_peak_cap_percent=25),\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dd6a62a",
   "metadata": {},
   "outputs": [],
   "source": [
    "parallel_2 = ParallelConfig(enabled=False, method=\"local\", workers=1, show_progress=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "352b885e",
   "metadata": {},
   "source": [
    "##### CONFIGURATION 2 - GREEDY SOLVER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82a191ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    " # CONFIGURATION 2 - GREEDY\n",
    "\n",
    "solver_2_greedy = SolverConfig(solver_family=\"greedy\")\n",
    "\n",
    "# IMPLEMENTATION 2 - GREEDY\n",
    "\n",
    "marginal_emissions_pldf_run_2_greedy = day_and_slot_cols(marginal_emissions_pldf, slot_len_min=policy_2.behavioral.slot_length_minutes)\n",
    "\n",
    "if policy_2.household_min is not None:\n",
    "        marginal_emissions_pldf_run_2_greedy = attach_household_floor(df=marginal_emissions_pldf_run_2_greedy, cfg=policy_2.household_min)\n",
    "\n",
    "cols_needed = [\n",
    "        \"ca_id\",\"city\",\"date\",\"day\",\"slot\",\"value\",\n",
    "        \"marginal_emissions_factor_grams_co2_per_kWh\",\n",
    "        \"average_emissions_factor_grams_co2_per_kWh\",\n",
    "]\n",
    "if \"floor_kwh\" in marginal_emissions_pldf_run_2_greedy.columns:\n",
    "        cols_needed.append(\"floor_kwh\")\n",
    "\n",
    "marginal_emissions_pldf_run_2_greedy = marginal_emissions_pldf_run_2_greedy.select(cols_needed).sort([\"ca_id\",\"day\",\"slot\"])\n",
    "\n",
    "cityday_ca_order_2_greedy = build_cityday_ca_order_map_pl(marginal_emissions_pldf_run_2_greedy)\n",
    "\n",
    "pct_tbl_2_greedy = compute_city_day_percentiles(marginal_emissions_pldf_run_2_greedy)\n",
    "\n",
    "marginal_emissions_pldf_run_2_greedy = marginal_emissions_pldf_run_2_greedy.to_pandas()  # keeps dictionary cols efficient\n",
    "\n",
    "metrics_df_2_greedy, moves_df_2_greedy, opt_df_2_greedy = run_pipeline_pandas_cityweek_budget(\n",
    "        df_pd=marginal_emissions_pldf_run_2_greedy,\n",
    "        policy=policy_2,\n",
    "        solver=solver_2_greedy,          # set solver.solver_family to \"milp\" or \"greedy\"\n",
    "        shuffle_high_usage_order=False,\n",
    "        emit_optimised_rows=True,\n",
    "        show_progress=True,\n",
    "        workers=parallel_2.workers,  # number of parallel workers\n",
    "        cityday_ca_order=cityday_ca_order_2_greedy,   # pass the precomputed order\n",
    "        backend=parallel_2.method,  # pass the backend\n",
    ")\n",
    "\n",
    "metrics_df_2_greedy.to_parquet(os.path.join(results_directory, \"metrics_config_2_greedy.parquet\"))\n",
    "moves_df_2_greedy.to_parquet(os.path.join(results_directory, \"moves_config_2_greedy.parquet\"))\n",
    "opt_df_2_greedy.to_parquet(os.path.join(results_directory, \"optimised_config_2_greedy.parquet\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "185d3ab3",
   "metadata": {},
   "source": [
    "##### CONFIGURATION 2 - MILP SOLVER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bfdbbaa",
   "metadata": {},
   "outputs": [],
   "source": [
    " # CONFIGURATION 2 - MILP\n",
    "\n",
    "solver_2_milp = SolverConfig(solver_family=\"milp\")\n",
    "\n",
    " # IMPLEMENTATION 2 - MILP\n",
    "\n",
    "marginal_emissions_pldf_run_2_milp = day_and_slot_cols(marginal_emissions_pldf, slot_len_min=policy_2.behavioral.slot_length_minutes)\n",
    "if policy_2.household_min is not None:\n",
    "        marginal_emissions_pldf_run_2_milp = attach_household_floor(df=marginal_emissions_pldf_run_2_milp, cfg=policy_2.household_min)\n",
    "\n",
    "cols_needed = [\n",
    "        \"ca_id\",\"city\",\"date\",\"day\",\"slot\",\"value\",\n",
    "        \"marginal_emissions_factor_grams_co2_per_kWh\",\n",
    "        \"average_emissions_factor_grams_co2_per_kWh\",\n",
    "]\n",
    "if \"floor_kwh\" in marginal_emissions_pldf_run_2_milp.columns:\n",
    "        cols_needed.append(\"floor_kwh\")\n",
    "\n",
    "marginal_emissions_pldf_run_2_milp = marginal_emissions_pldf_run_2_milp.select(cols_needed).sort([\"ca_id\",\"day\",\"slot\"])\n",
    "\n",
    "cityday_ca_order_2_milp = build_cityday_ca_order_map_pl(marginal_emissions_pldf_run_2_milp)\n",
    "\n",
    "pct_tbl_2_milp = compute_city_day_percentiles(marginal_emissions_pldf_run_2_milp)\n",
    "\n",
    "marginal_emissions_pldf_run_2_milp = marginal_emissions_pldf_run_2_milp.to_pandas()  # keeps dictionary cols efficient\n",
    "\n",
    "metrics_df_2_milp, moves_df_2_milp, opt_df_2_milp = run_pipeline_pandas_cityweek_budget(\n",
    "        df_pd=marginal_emissions_pldf_run_2_milp,\n",
    "        policy=policy_2,\n",
    "        solver=solver_2_milp,          # set solver.solver_family to \"milp\" or \"greedy\"\n",
    "        shuffle_high_usage_order=False,\n",
    "        emit_optimised_rows=True,\n",
    "        show_progress=True,\n",
    "        workers=parallel_2.workers,  # number of parallel workers\n",
    "        cityday_ca_order=cityday_ca_order_2_milp,   # pass the precomputed order\n",
    "        backend=parallel_2.method,  # pass the backend\n",
    "    )\n",
    "\n",
    "metrics_df_2_milp.to_parquet(os.path.join(results_directory, \"metrics_config_2_milp.parquet\"))\n",
    "moves_df_2_milp.to_parquet(os.path.join(results_directory, \"moves_config_2_milp.parquet\"))\n",
    "opt_df_2_milp.to_parquet(os.path.join(results_directory, \"optimised_config_2_milp.parquet\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63104760",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2ecbe7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "pct_tbl_pd = pct_tbl.to_pandas()\n",
    "\n",
    "# Ensure datetime alignment (no tz and normalized to midnight)\n",
    "metrics_df[\"day\"]  = pd.to_datetime(metrics_df[\"day\"]).dt.normalize()\n",
    "pct_tbl_pd[\"day\"]  = pd.to_datetime(pct_tbl_pd[\"day\"]).dt.normalize()\n",
    "\n",
    "# Merge (left join keeps all metrics rows)\n",
    "metrics_df = metrics_df.merge(\n",
    "    pct_tbl_pd[[\"city\",\"day\",\"ca_id\",\"day_kwh\",\"pct\"]],\n",
    "    on=[\"city\",\"day\",\"ca_id\"],\n",
    "    how=\"left\",\n",
    "    validate=\"many_to_one\"   # optional safety: each metrics row matches ≤1 percentile row\n",
    ")\n",
    "\n",
    "# If you prefer 0..100 instead of 0..1:\n",
    "metrics_df[\"pct\"] = (metrics_df[\"pct\"] * 100.0).astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a22d3352",
   "metadata": {},
   "outputs": [],
   "source": [
    "moves_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29b92795",
   "metadata": {},
   "outputs": [],
   "source": [
    "opt_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28831265",
   "metadata": {},
   "source": [
    "### LEGACY"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0633596a",
   "metadata": {},
   "source": [
    "#### HELPERS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3daffdb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def per_city_slot_baseline(df_city_day: pl.DataFrame) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Return baseline per slot (city-level same-day baseline) for spike cap comparison.\n",
    "\n",
    "    Parameters:\n",
    "    ----------\n",
    "    df_city_day : pl.DataFrame\n",
    "        The DataFrame containing the relevant data for the city and day.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    np.ndarray\n",
    "        The baseline per slot (city-level same-day baseline) for spike cap comparison.\n",
    "    \"\"\"\n",
    "    slot_count = 48\n",
    "    base = np.zeros(slot_count, dtype=float)\n",
    "    if df_city_day.height == 0:\n",
    "        return base\n",
    "    # baseline = sum of original usage by slot for that city-day\n",
    "    grouped = df_city_day.group_by(\"slot\").agg(pl.sum(\"value\").alias(\"sum_kwh\"))\n",
    "    base[grouped[\"slot\"].to_numpy().astype(int)] = grouped[\"sum_kwh\"].to_numpy()\n",
    "    return base\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "106d53b5",
   "metadata": {},
   "source": [
    "#### Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9774e850",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_pipeline_pandas(df_pd: pd.DataFrame, policy: ShiftPolicy, solver: SolverConfig,\n",
    "                        workers: int = 4, show_progress: bool = True):\n",
    "    # Make sure day is datetime64[ns, tz] or tz-naive consistently; keep as-is if already tz-aware\n",
    "    # Group by ca_id, day, city\n",
    "    gobj = df_pd.groupby([\"ca_id\",\"day\",\"city\"], sort=True, group_keys=False)\n",
    "\n",
    "    # Build argument tuples for Pool\n",
    "    policy_d = {\n",
    "        \"slot_length_minutes\": policy.behavioral.slot_length_minutes,\n",
    "        \"shift_hours_window\": policy.behavioral.shift_hours_window,\n",
    "        \"peak_hours_reduction_limit_config\": policy.behavioral.peak_hours_reduction_limit_config,\n",
    "    }\n",
    "    solver_d = {\n",
    "        \"solver_family\": solver.solver_family,\n",
    "        \"lp_solver\": solver.lp_solver,\n",
    "        \"lp_solver_opts\": solver.lp_solver_opts,\n",
    "        \"milp_solver\": solver.milp_solver,\n",
    "        \"milp_solver_opts\": solver.milp_solver_opts,\n",
    "        \"greedy_min_fraction_of_day_to_move\": solver.greedy_min_fraction_of_day_to_move,\n",
    "    }\n",
    "\n",
    "    jobs = []\n",
    "    for (ca_id, day_ts, city), sub in gobj:\n",
    "        # we send a small dict (column -> list) to reduce pandas pickle overhead\n",
    "        sub_dict = sub[[\"slot\",\"value\",\n",
    "                        \"marginal_emissions_factor_grams_co2_per_kWh\",\n",
    "                        \"average_emissions_factor_grams_co2_per_kWh\"] +\n",
    "                       ([\"floor_kwh\"] if \"floor_kwh\" in sub.columns else [])].to_dict(\"list\")\n",
    "        jobs.append(((ca_id, day_ts, city), sub_dict, policy_d, solver_d))\n",
    "\n",
    "    metrics_rows, move_rows, opt_rows = [], [], []\n",
    "\n",
    "    if workers and workers > 1:\n",
    "        with mp.Pool(processes=workers) as pool:\n",
    "            it = pool.imap(worker_solve_group, jobs, chunksize=16)\n",
    "            for i, out in enumerate(it, 1):\n",
    "                if show_progress and (i % max(1, len(jobs)//20) == 0 or i == len(jobs)):\n",
    "                    print(f\"[progress] {i}/{len(jobs)} groups processed ({i/len(jobs)*100:.1f}%)\")\n",
    "                if out is None:\n",
    "                    continue\n",
    "                mrow, mrows, orows = out\n",
    "                metrics_rows.append(mrow); move_rows.extend(mrows); opt_rows.extend(orows)\n",
    "    else:\n",
    "        for i, args in enumerate(jobs, 1):\n",
    "            out = worker_solve_group(args)\n",
    "            if show_progress and (i % max(1, len(jobs)//20) == 0 or i == len(jobs)):\n",
    "                print(f\"[progress] {i}/{len(jobs)} groups processed ({i/len(jobs)*100:.1f}%)\")\n",
    "            if out is None:\n",
    "                continue\n",
    "            mrow, mrows, orows = out\n",
    "            metrics_rows.append(mrow); move_rows.extend(mrows); opt_rows.extend(orows)\n",
    "\n",
    "    metrics_df = pd.DataFrame(metrics_rows)\n",
    "    moves_df   = pd.DataFrame(move_rows)\n",
    "    opt_df     = pd.DataFrame(opt_rows)\n",
    "    return metrics_df, moves_df, opt_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "593f5c8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_pipeline(\n",
    "        df_raw: pl.DataFrame,\n",
    "        policy: ShiftPolicy,\n",
    "        solver: SolverConfig,\n",
    "        parallel: Optional[ParallelConfig] = None,\n",
    "        emit_optimised_rows: bool = False,\n",
    "        progress: bool = False,\n",
    ") -> Tuple[pl.DataFrame, pl.DataFrame, Optional[pl.DataFrame]]:\n",
    "    \"\"\"\n",
    "    Run the load shifting optimization pipeline.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    df_raw: pl.DataFrame\n",
    "        The raw input DataFrame containing the data to process.\n",
    "    policy: ShiftPolicy\n",
    "        The policy configuration to use for the shift optimization.\n",
    "    solver: SolverConfig\n",
    "        The solver configuration to use for the optimization.\n",
    "    parallel: Optional[ParallelConfig], optional\n",
    "        The parallelization configuration to use (default is None).\n",
    "    emit_optimised_rows: bool, optional\n",
    "        Whether to emit the optimized rows (default is False).\n",
    "    progress: bool, optional\n",
    "        Whether to show progress (default is False).\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    Tuple[pl.DataFrame, pl.DataFrame, Optional[pl.DataFrame]]\n",
    "        metrics_df: per customer-day totals & savings\n",
    "        moves_df: per move (t->s) detailed records for auditing\n",
    "        optimised_rows_df: 48-slot reconstructed series if requested\n",
    "    \"\"\"\n",
    "    if parallel is None:\n",
    "        parallel = ParallelConfig(enabled=False)\n",
    "\n",
    "    df = day_and_slot_cols(df_raw, slot_len_min=policy.behavioral.slot_length_minutes)\n",
    "    df = df.sort([\"ca_id\",\"day\",\"slot\"])\n",
    "\n",
    "    # group iterator\n",
    "    groups = list(df.group_by([\"ca_id\",\"day\",\"city\"], maintain_order=True))\n",
    "    total_groups = len(groups)\n",
    "    prog_cb = progress_printer(total_groups) if (progress or (parallel and parallel.show_progress)) else (lambda i: None)\n",
    "\n",
    "    # choose backend\n",
    "    W = hours_to_slots(policy.behavioral.shift_hours_window, policy.behavioral.slot_length_minutes)\n",
    "\n",
    "    results_metrics: List[Dict[str, Any]] = []\n",
    "    results_moves: List[Dict[str, Any]] = []\n",
    "    results_opt_rows: List[Dict[str, Any]] = [] if emit_optimised_rows else None\n",
    "\n",
    "    if parallel.enabled and parallel.method == \"local\" and parallel.workers and parallel.workers > 1:\n",
    "        with mp.Pool(processes=parallel.workers) as pool:\n",
    "            worker = partial(\n",
    "                solve_one,\n",
    "                solver=solver, W=W, policy=policy, emit_optimised_rows=emit_optimised_rows\n",
    "            )\n",
    "            for i, out in enumerate(pool.imap(worker, groups, chunksize=8), start=1):\n",
    "                prog_cb(i)\n",
    "                if out is None:\n",
    "                    continue\n",
    "                mrow, mrows, orows = out\n",
    "                results_metrics.append(mrow)\n",
    "                results_moves.extend(mrows)\n",
    "                if emit_optimised_rows and orows:\n",
    "                    results_opt_rows.extend(orows)\n",
    "    else:\n",
    "        for i, g in enumerate(groups, start=1):\n",
    "            out = solve_one(sub_tuple=g, solver=solver, W=W, policy=policy, emit_optimised_rows=emit_optimised_rows)\n",
    "            prog_cb(i)\n",
    "            if out is None:\n",
    "                continue\n",
    "            mrow, mrows, orows = out\n",
    "            results_metrics.append(mrow)\n",
    "            results_moves.extend(mrows)\n",
    "            if emit_optimised_rows and orows:\n",
    "                results_opt_rows.extend(orows)\n",
    "\n",
    "    metrics_df = pl.DataFrame(results_metrics) if results_metrics else pl.DataFrame()\n",
    "    moves_df   = pl.DataFrame(results_moves)   if results_moves   else pl.DataFrame()\n",
    "    opt_df     = pl.DataFrame(results_opt_rows) if (emit_optimised_rows and results_opt_rows) else None\n",
    "\n",
    "    return metrics_df, moves_df, opt_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb505ba8",
   "metadata": {},
   "source": [
    "#### Solvers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e4fe949",
   "metadata": {},
   "outputs": [],
   "source": [
    "def solve_city_day(\n",
    "    df_city_day: pd.DataFrame,\n",
    "    policy: ShiftPolicy,\n",
    "    solver: SolverConfig,\n",
    "    *,\n",
    "    shuffle_high_usage_order: bool = False,\n",
    "    emit_optimised_rows: bool = True,\n",
    ") -> Tuple[List[Dict[str, Any]], List[Dict[str, Any]], List[Dict[str, Any]]]:\n",
    "    \"\"\"\n",
    "    Solve one (city, day) by iterating customers in a chosen order while enforcing:\n",
    "      - Regional moved-kWh budget (P% × city_daily_avg)\n",
    "      - Per-slot anti-spike cap (≤ (1+α%) × baseline)\n",
    "      - Per-customer peak-hour comfort (slot/hour)\n",
    "      - Household per-slot floors (if present)\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df_city_day : pandas.DataFrame\n",
    "        Filtered rows for a single (city, day).\n",
    "    policy : ShiftPolicy\n",
    "        Full policy with behavioral, regional and anti-spike configs.\n",
    "    solver : SolverConfig\n",
    "        Solver configuration.\n",
    "    shuffle_high_usage_order : bool, optional\n",
    "        If True, randomly shuffles the high-usage order after ranking.\n",
    "    emit_optimised_rows : bool, optional\n",
    "        If True, emit per-slot post-usage rows.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    metrics_rows, move_rows, opt_rows : lists of dict\n",
    "        Aggregated outputs for this city-day across customers.\n",
    "    \"\"\"\n",
    "    assert {\"ca_id\",\"city\",\"day\",\"slot\",\"value\",\n",
    "            \"marginal_emissions_factor_grams_co2_per_kWh\",\n",
    "            \"average_emissions_factor_grams_co2_per_kWh\"}.issubset(df_city_day.columns)\n",
    "\n",
    "    city = df_city_day[\"city\"].iloc[0]\n",
    "    day_ts = df_city_day[\"day\"].iloc[0]\n",
    "    slot_len = policy.behavioral.slot_length_minutes\n",
    "    per_hour = 60 // slot_len\n",
    "    T = 24 * per_hour\n",
    "\n",
    "    # City-day baselines and caps for anti-spike\n",
    "    base_city = cityday_baseline_by_slot(df_city_day, slot_len_min=slot_len)\n",
    "    alpha = (policy.spike_cap.alpha_peak_cap_percent / 100.0) if policy.spike_cap else 0.25\n",
    "    post_city_cap = (1.0 + alpha) * base_city\n",
    "    post_city_used = np.zeros(T, dtype=float)  # accumulate post usage as we assign customers\n",
    "\n",
    "    # City-day regional moved budget\n",
    "    P_pct = (policy.regional_cap.regional_load_shift_percent_limit / 100.0) if policy.regional_cap else 0.10\n",
    "    city_avgs = policy.regional_cap.regional_total_daily_average_load_kWh if policy.regional_cap else {}\n",
    "    city_daily_avg = float(city_avgs.get(city, 0.0))\n",
    "    moved_budget_remaining = P_pct * city_daily_avg\n",
    "\n",
    "    # Customer order and optional shuffle\n",
    "    order, pct_map, tot_map = rank_customers_by_daily_kwh(df_city_day)\n",
    "    if shuffle_high_usage_order:\n",
    "        rng = np.random.default_rng()\n",
    "        order = list(order)  # avoid shuffling original view\n",
    "        rng.shuffle(order)\n",
    "\n",
    "    metrics_rows: List[Dict[str, Any]] = []\n",
    "    move_rows: List[Dict[str, Any]] = []\n",
    "    opt_rows: List[Dict[str, Any]] = []\n",
    "\n",
    "    for ca_id in order:\n",
    "        sub = df_city_day[df_city_day[\"ca_id\"] == ca_id]\n",
    "        arrs = build_arrays_for_group_pd(sub, slot_len_min=slot_len)\n",
    "        if arrs is None:\n",
    "            continue\n",
    "\n",
    "        usage = arrs[\"usage\"]; mef = arrs[\"mef\"]; aef = arrs[\"aef\"]\n",
    "        floor_vec = arrs.get(\"floor\", None)\n",
    "\n",
    "        # Peak comfort for this customer (based on user's config)\n",
    "        peak_cfg = policy.behavioral.peak_hours_reduction_limit_config\n",
    "        peak_mask, peak_groups, Z, cap_mode = city_peak_targets_for_day(\n",
    "            city, day_ts, peak_cfg, slot_len_min=slot_len\n",
    "        )\n",
    "\n",
    "        # Residual anti-spike capacity for this customer\n",
    "        resid_cap = np.maximum(0.0, post_city_cap - post_city_used)\n",
    "\n",
    "        # Remaining regional moved budget for this customer\n",
    "        moved_kwh_cap = max(0.0, float(moved_budget_remaining))\n",
    "\n",
    "        # Solve (LP shown; MILP/greedy could be wired similarly)\n",
    "        usage_opt, flows = solve_lp(\n",
    "            mef=mef, usage=usage, W_slots=int((policy.behavioral.shift_hours_window * 60)//slot_len),\n",
    "            cfg=solver,\n",
    "            peak_mask=peak_mask, peak_groups=peak_groups, Z=Z, cap_mode=cap_mode,\n",
    "            floor_vec=floor_vec,\n",
    "            dest_upper_bounds=resid_cap,\n",
    "            moved_kwh_cap=moved_kwh_cap,\n",
    "        )\n",
    "\n",
    "        # Update city-day residuals\n",
    "        post_city_used += usage_opt\n",
    "        moved_kwh = float(np.maximum(np.float32(0.0), usage.astype(np.float32) - usage_opt.astype(np.float32)).sum())\n",
    "        moved_budget_remaining -= moved_kwh\n",
    "        if moved_budget_remaining < 0:\n",
    "            moved_budget_remaining = 0.0  # guard for tiny negatives\n",
    "\n",
    "        # Metrics\n",
    "        base = compute_emissions_totals(usage, aef, mef)\n",
    "        post = compute_emissions_totals(usage_opt, aef, mef)\n",
    "        median_shift = weighted_median_shift_minutes(flows, slot_len)\n",
    "\n",
    "        metrics_rows.append({\n",
    "            \"ca_id\": ca_id, \"city\": city, \"day\": day_ts,\n",
    "            \"customer_daily_kwh\": tot_map.get(ca_id, float(usage.sum())),\n",
    "            \"customer_daily_percentile\": pct_map.get(ca_id, np.nan),\n",
    "            \"baseline_E_avg_g\": base[\"E_avg_g\"], \"post_E_avg_g\": post[\"E_avg_g\"],\n",
    "            \"delta_E_avg_g\": base[\"E_avg_g\"] - post[\"E_avg_g\"],\n",
    "            \"baseline_E_marg_g\": base[\"E_marg_g\"], \"post_E_marg_g\": post[\"E_marg_g\"],\n",
    "            \"delta_E_marg_g\": base[\"E_marg_g\"] - post[\"E_marg_g\"],\n",
    "            \"baseline_kwh\": float(usage.sum()), \"post_kwh\": float(usage_opt.sum()),\n",
    "            \"moved_kwh\": moved_kwh,\n",
    "            \"avg_shift_minutes_energy_weighted\": float(\n",
    "                (sum(abs(s - t) * val for (t, s, val) in flows) / max(moved_kwh, 1e-9)) * slot_len\n",
    "            ) if flows else 0.0,\n",
    "            \"median_shift_minutes_energy_weighted\": median_shift,\n",
    "            \"regional_budget_remaining_kwh_after\": moved_budget_remaining,\n",
    "        })\n",
    "\n",
    "        # Moves table rows\n",
    "        for (t, s, kwh) in flows:\n",
    "            t_ts = day_ts + timedelta(minutes=int(t * slot_len))\n",
    "            s_ts = day_ts + timedelta(minutes=int(s * slot_len))\n",
    "            move_rows.append({\n",
    "                \"ca_id\": ca_id, \"city\": city, \"day\": day_ts,\n",
    "                \"customer_daily_percentile\": pct_map.get(ca_id, np.nan),\n",
    "                \"original_time\": t_ts, \"proposed_shift_time\": s_ts,\n",
    "                \"delta_minutes\": int((s - t) * slot_len),\n",
    "                \"shift_direction\": \"forward\" if s > t else (\"backward\" if s < t else \"stay\"),\n",
    "                \"delta_kwh\": float(kwh),\n",
    "                \"marginal_emissions_before_shift_grams_co2\": grams_co2_from_kwh_grams_per_kwh(kwh, mef[t]),\n",
    "                \"marginal_emissions_after_shift_grams_co2\": grams_co2_from_kwh_grams_per_kwh(kwh, mef[s]),\n",
    "                \"marginal_emissions_delta_grams_co2\": grams_co2_from_kwh_grams_per_kwh(kwh, mef[t]) - grams_co2_from_kwh_grams_per_kwh(kwh, mef[s]),\n",
    "                \"average_emissions_before_shift_grams_co2\": grams_co2_from_kwh_grams_per_kwh(kwh, aef[t]),\n",
    "                \"average_emissions_after_shift_grams_co2\": grams_co2_from_kwh_grams_per_kwh(kwh, aef[s]),\n",
    "                \"average_emissions_delta_grams_co2\": grams_co2_from_kwh_grams_per_kwh(kwh, aef[t]) - grams_co2_from_kwh_grams_per_kwh(kwh, aef[s]),\n",
    "            })\n",
    "\n",
    "        # Optional post-usage rows\n",
    "        if emit_optimised_rows:\n",
    "            for s in range(T):\n",
    "                ts = day_ts + timedelta(minutes=int(s * slot_len))\n",
    "                opt_rows.append({\n",
    "                    \"ca_id\": ca_id, \"city\": city, \"day\": day_ts,\n",
    "                    \"slot\": s, \"date\": ts, \"optimised_value\": float(usage_opt[s]),\n",
    "                })\n",
    "\n",
    "    return metrics_rows, move_rows, opt_rows\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8697f9e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def worker_solve_group_one_move(args):\n",
    "    (ca_id, day_ts, city), sub_df, policy_d, solver_d, caps = args\n",
    "    # sub_df is dict-of-lists to avoid pandas pickling overhead\n",
    "    sub = pd.DataFrame(sub_df)\n",
    "\n",
    "    arrs = build_arrays_for_group_pd(sub, slot_len_min=policy_d[\"slot_length_minutes\"])\n",
    "    if arrs is None:\n",
    "        return None\n",
    "    usage, mef, aef = arrs[\"usage\"], arrs[\"mef\"], arrs[\"aef\"]\n",
    "    floor = arrs.get(\"floor\")\n",
    "\n",
    "    slot_len = policy_d[\"slot_length_minutes\"]\n",
    "\n",
    "    peak_cfg = policy_d.get(\"peak_hours_reduction_limit_config\")\n",
    "    peak_mask, peak_groups, Z = city_peak_targets_for_day(city, day_ts, peak_cfg, slot_len_min=slot_len)\n",
    "\n",
    "    # caps injected by orchestrator\n",
    "    dest_ub = caps[\"dest_upper_bounds\"]           # np.ndarray for this (city, day)\n",
    "    reg_rem = caps[\"remaining_regional_budget\"]   # float remaining for this (city, day)\n",
    "\n",
    "    W = int((policy_d[\"shift_hours_window\"] * 60) // slot_len)\n",
    "\n",
    "    family = solver_d[\"solver_family\"]\n",
    "    if family == \"greedy_one\":\n",
    "        usage_opt, flows = solve_one_move_greedy(\n",
    "            mef, usage, W,\n",
    "            slot_len_min=slot_len,\n",
    "            floor_vec=floor,\n",
    "            peak_mask=peak_mask,\n",
    "            peak_groups=peak_groups,\n",
    "            Z=Z,\n",
    "            dest_upper_bounds=dest_ub,\n",
    "            remaining_regional_budget_kwh=reg_rem,\n",
    "        )\n",
    "    elif family == \"milp_one\":\n",
    "        usage_opt, flows = solve_one_move_milp(\n",
    "            mef, usage, W,\n",
    "            floor_vec=floor,\n",
    "            peak_groups=peak_groups,\n",
    "            Z=Z,\n",
    "            slot_len_min=slot_len,\n",
    "            dest_upper_bounds=dest_ub,\n",
    "            remaining_regional_budget_kwh=reg_rem,\n",
    "            milp_solver=solver_d.get(\"milp_solver\",\"cbc\"),\n",
    "            milp_opts=solver_d.get(\"milp_solver_opts\"),\n",
    "        )\n",
    "    else:\n",
    "        raise ValueError(\"Use 'greedy_one' or 'milp_one' to enforce a single move.\")\n",
    "\n",
    "    # build metrics / rows as you already do...\n",
    "    # IMPORTANT: return also 'moved_kwh' so the orchestrator can decrement the city-day budget.\n",
    "    base = compute_emissions_totals(usage, aef, mef)\n",
    "    post = compute_emissions_totals(usage_opt, aef, mef)\n",
    "    moved_kwh = float(np.maximum(0.0, usage - usage_opt).sum())\n",
    "    # ... construct rows as in your existing worker ...\n",
    "    return metrics_row, rows, opt_rows, moved_kwh\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b327d7f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def solve_one_move_greedy(\n",
    "    mef: np.ndarray,\n",
    "    usage: np.ndarray,\n",
    "    W_slots: int,\n",
    "    *,\n",
    "    slot_len_min: int = 30,\n",
    "    floor_vec: Optional[np.ndarray] = None,          # per-slot min at source (kWh)\n",
    "    peak_mask: Optional[np.ndarray] = None,          # destination peak slots (boolean)\n",
    "    peak_groups: Optional[List[List[int]]] = None,   # hour grouping (for source cap)\n",
    "    Z: Optional[float] = None,                       # fraction (0..1)\n",
    "    dest_upper_bounds: Optional[np.ndarray] = None,  # city-day anti-spike caps (kWh)\n",
    "    remaining_regional_budget_kwh: Optional[float] = None,  # city-day remaining P% budget\n",
    ") -> Tuple[np.ndarray, List[Tuple[int,int,float]]]:\n",
    "    \"\"\"\n",
    "    Make exactly ONE move (t->s) that maximizes emissions reduction subject to caps.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    usage_opt : np.ndarray\n",
    "        New usage after the single move (or unchanged if no feasible positive-gain move).\n",
    "    flows : list[(t,s,kwh)]\n",
    "        Empty if no beneficial move exists, else one tuple for the executed move.\n",
    "    \"\"\"\n",
    "    T = len(usage)\n",
    "    usage_opt = usage.astype(float).copy()\n",
    "\n",
    "    slots = range(T)\n",
    "    best_gain = 0.0\n",
    "    best_pair = None\n",
    "    best_amt  = 0.0\n",
    "\n",
    "    # helper: hour group for source t (for 30-min default, two slots per hour)\n",
    "    slots_per_hour = 60 // slot_len_min\n",
    "\n",
    "    def hour_group_of(t: int) -> List[int]:\n",
    "        h = t // slots_per_hour\n",
    "        start = h * slots_per_hour\n",
    "        return list(range(start, start + slots_per_hour))\n",
    "\n",
    "    # consider all allowed pairs within the window\n",
    "    for t in slots:\n",
    "        if usage[t] <= 1e-12:\n",
    "            continue\n",
    "        s0, s1 = max(0, t - W_slots), min(T, t + W_slots + 1)\n",
    "        for s in range(s0, s1):\n",
    "            if s == t:\n",
    "                continue\n",
    "            # emissions gain per kWh if we move from t to s\n",
    "            delta_per_kwh = float(mef[t] - mef[s])\n",
    "            if delta_per_kwh <= 1e-12:\n",
    "                continue  # no environmental benefit\n",
    "\n",
    "            # upper bound 1: source availability after floor\n",
    "            src_floor = float(floor_vec[t]) if (floor_vec is not None) else 0.0\n",
    "            ub_src = max(0.0, usage[t] - src_floor)\n",
    "\n",
    "            if ub_src <= 1e-12:\n",
    "                continue\n",
    "\n",
    "            # upper bound 2: destination anti-spike remaining capacity\n",
    "            if dest_upper_bounds is not None and np.isfinite(dest_upper_bounds[s]):\n",
    "                # current post-usage at s (before move) = usage[s]\n",
    "                ub_dest = max(0.0, float(dest_upper_bounds[s] - usage[s]))\n",
    "            else:\n",
    "                ub_dest = float(\"inf\")\n",
    "\n",
    "            if ub_dest <= 1e-12:\n",
    "                continue\n",
    "\n",
    "            # upper bound 3: peak-hour *source* comfort cap (per hour)\n",
    "            if Z is not None and peak_groups is not None:\n",
    "                grp = hour_group_of(t)\n",
    "                base_hour = float(usage[grp].sum())  # base per-customer usage in that hour\n",
    "                ub_peak = Z * base_hour\n",
    "            else:\n",
    "                ub_peak = float(\"inf\")\n",
    "\n",
    "            # upper bound 4: remaining regional city-day budget\n",
    "            ub_reg = float(remaining_regional_budget_kwh) if (remaining_regional_budget_kwh is not None) else float(\"inf\")\n",
    "\n",
    "            # total feasible\n",
    "            m = min(ub_src, ub_dest, ub_peak, ub_reg)\n",
    "            if m <= 1e-12:\n",
    "                continue\n",
    "\n",
    "            gain = delta_per_kwh * m\n",
    "            if gain > best_gain:\n",
    "                best_gain = gain\n",
    "                best_pair = (t, s)\n",
    "                best_amt  = m\n",
    "\n",
    "    if best_pair is None:\n",
    "        return usage_opt, []  # no beneficial move\n",
    "\n",
    "    t, s = best_pair\n",
    "    m = best_amt\n",
    "\n",
    "    usage_opt[t] -= m\n",
    "    usage_opt[s] += m\n",
    "\n",
    "    return usage_opt, [(t, s, float(m))]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3f23448",
   "metadata": {},
   "outputs": [],
   "source": [
    "def worker_solve_group(args):\n",
    "    \"\"\"\n",
    "    args: (key_tuple, sub_df_dict, policy_dict, solver_dict)\n",
    "    To keep things Pool-friendly, pass simple dicts/tuples, not complex objects.\n",
    "    \"\"\"\n",
    "    (ca_id, day_ts, city), sub_df, policy_d, solver_d = args\n",
    "\n",
    "    # reconstruct small pandas frame for the group\n",
    "    sub = pd.DataFrame(sub_df)\n",
    "\n",
    "    # arrays\n",
    "    arrs = build_arrays_for_group_pd(sub, slot_len_min=policy_d[\"slot_length_minutes\"])\n",
    "    if arrs is None:\n",
    "        return None\n",
    "    usage, mef, aef = arrs[\"usage\"], arrs[\"mef\"], arrs[\"aef\"]\n",
    "    floor = arrs.get(\"floor\")\n",
    "\n",
    "    # peak comfort mask (per city, per customer)\n",
    "    peak_cfg = policy_d.get(\"peak_hours_reduction_limit_config\")\n",
    "    slot_len = policy_d[\"slot_length_minutes\"]\n",
    "    peak_mask, Z = city_peak_targets_for_day()(city, day_ts, peak_cfg, slot_len_min=slot_len)\n",
    "    base_peak_kwh = float(usage[peak_mask].sum()) if (peak_mask is not None) else None\n",
    "\n",
    "    # window in slots\n",
    "    W = int((policy_d[\"shift_hours_window\"] * 60) // slot_len)\n",
    "\n",
    "    # choose solver\n",
    "    solver_family = solver_d[\"solver_family\"]\n",
    "    if solver_family == \"lp\":\n",
    "        usage_opt, flows = solve_lp(mef, usage, W,\n",
    "                                    cfg=SolverConfig(solver_family=\"lp\",\n",
    "                                                     lp_solver=solver_d[\"lp_solver\"],\n",
    "                                                     lp_solver_opts=solver_d.get(\"lp_solver_opts\")),\n",
    "                                    peak_mask=peak_mask, Z=Z, base_peak_kwh=base_peak_kwh)\n",
    "    elif solver_family == \"milp\":\n",
    "        usage_opt, flows = solve_milp(mef, usage, W,\n",
    "                                      cfg=SolverConfig(solver_family=\"milp\",\n",
    "                                                       milp_solver=solver_d[\"milp_solver\"],\n",
    "                                                       milp_solver_opts=solver_d.get(\"milp_solver_opts\")),\n",
    "                                      peak_mask=peak_mask, Z=Z, base_peak_kwh=base_peak_kwh,\n",
    "                                      floor=floor)\n",
    "    else:\n",
    "        usage_opt, flows = solve_greedy(mef, usage, W,\n",
    "                                        cfg=SolverConfig(solver_family=\"greedy\",\n",
    "                                                         greedy_min_fraction_of_day_to_move=solver_d.get(\"greedy_min_fraction_of_day_to_move\")))\n",
    "\n",
    "    # metrics\n",
    "    base = compute_emissions_totals(usage, aef, mef)\n",
    "    post = compute_emissions_totals(usage_opt, aef, mef)\n",
    "    moved_kwh = float(np.maximum(0.0, usage - usage_opt).sum())\n",
    "    median_shift_minutes = weighted_median_shift_minutes(flows, slot_len)\n",
    "\n",
    "    metrics_row = {\n",
    "        \"ca_id\": ca_id, \"city\": city, \"day\": day_ts,\n",
    "        \"baseline_E_avg_g\": base[\"E_avg_g\"], \"post_E_avg_g\": post[\"E_avg_g\"],\n",
    "        \"delta_E_avg_g\": base[\"E_avg_g\"] - post[\"E_avg_g\"],\n",
    "        \"baseline_E_marg_g\": base[\"E_marg_g\"], \"post_E_marg_g\": post[\"E_marg_g\"],\n",
    "        \"delta_E_marg_g\": base[\"E_marg_g\"] - post[\"E_marg_g\"],\n",
    "        \"baseline_kwh\": float(usage.sum()),\n",
    "        \"post_kwh\": float(usage_opt.sum()),\n",
    "        \"moved_kwh\": moved_kwh,\n",
    "        \"avg_shift_minutes_energy_weighted\": float(\n",
    "            (sum(abs(s - t) * val for (t, s, val) in flows) / max(moved_kwh, 1e-9)) * slot_len\n",
    "        ) if flows else 0.0,\n",
    "        \"median_shift_minutes_energy_weighted\": median_shift_minutes,\n",
    "    }\n",
    "\n",
    "    # moves table rows\n",
    "    rows = []\n",
    "    for (t, s, kwh) in flows:\n",
    "        t_ts = day_ts + timedelta(minutes=int(t * slot_len))\n",
    "        s_ts = day_ts + timedelta(minutes=int(s * slot_len))\n",
    "        dirn = \"forward\" if s > t else (\"backward\" if s < t else \"stay\")\n",
    "        g_marg_before = grams_co2_from_kwh_grams_per_kwh(kwh, mef[t])\n",
    "        g_marg_after  = grams_co2_from_kwh_grams_per_kwh(kwh, mef[s])\n",
    "        g_avg_before  = grams_co2_from_kwh_grams_per_kwh(kwh, aef[t])\n",
    "        g_avg_after   = grams_co2_from_kwh_grams_per_kwh(kwh, aef[s])\n",
    "        rows.append({\n",
    "            \"ca_id\": ca_id, \"city\": city, \"day\": day_ts,\n",
    "            \"original_time\": t_ts, \"proposed_shift_time\": s_ts,\n",
    "            \"delta_minutes\": int((s - t) * slot_len), \"shift_direction\": dirn,\n",
    "            \"delta_kwh\": float(kwh),\n",
    "            \"marginal_emissions_before_shift_grams_co2\": g_marg_before,\n",
    "            \"marginal_emissions_after_shift_grams_co2\": g_marg_after,\n",
    "            \"marginal_emissions_delta_grams_co2\": g_marg_before - g_marg_after,\n",
    "            \"average_emissions_before_shift_grams_co2\": g_avg_before,\n",
    "            \"average_emissions_after_shift_grams_co2\": g_avg_after,\n",
    "            \"average_emissions_delta_grams_co2\": g_avg_before - g_avg_after,\n",
    "        })\n",
    "\n",
    "    # optimised 48-slot rows\n",
    "    opt_rows = []\n",
    "    for s in range(len(usage_opt)):\n",
    "        ts = day_ts + timedelta(minutes=int(s * slot_len))\n",
    "        opt_rows.append({\n",
    "            \"ca_id\": ca_id, \"city\": city, \"day\": day_ts,\n",
    "            \"slot\": s, \"date\": ts, \"optimised_value\": float(usage_opt[s]),\n",
    "        })\n",
    "\n",
    "    return metrics_row, rows, opt_rows\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "199082fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def solve_one(\n",
    "    sub_tuple: Tuple,\n",
    "    solver: SolverConfig,\n",
    "    W: float,\n",
    "    policy: ShiftPolicy,\n",
    "    emit_optimised_rows: bool,\n",
    ") -> Optional[Tuple[Dict[str, float], List[Dict[str, float]], List[Dict[str, float]]]]:\n",
    "    _, subdf = sub_tuple  # (key, frame)\n",
    "    ca_id = subdf[\"ca_id\"][0]\n",
    "    city  = subdf[\"city\"][0]\n",
    "    day   = subdf[\"day\"][0]\n",
    "\n",
    "    arrays = build_arrays_for_group(subdf.select([\n",
    "        \"slot\", \"value\",\n",
    "        \"marginal_emissions_factor_grams_co2_per_kWh\",\n",
    "        \"average_emissions_factor_grams_co2_per_kWh\"\n",
    "    ]))\n",
    "    if not arrays:\n",
    "        return None\n",
    "\n",
    "    usage, mef, aef = arrays[\"usage\"], arrays[\"mef\"], arrays[\"aef\"]\n",
    "\n",
    "    # Optional household per-slot floor vector\n",
    "    if \"floor_kwh\" in subdf.columns:\n",
    "        floor_vec = np.zeros(len(usage), dtype=float)\n",
    "        for s, fk in zip(subdf[\"slot\"].to_numpy().astype(int), subdf[\"floor_kwh\"].to_numpy()):\n",
    "            floor_vec[s] = float(max(0.0, fk))\n",
    "    else:\n",
    "        floor_vec = None\n",
    "\n",
    "    # Peak-hour mask/groups from user's full-hour inputs (per city)\n",
    "    peak_cfg = policy.behavioral.peak_hours_reduction_limit_config\n",
    "    peak_mask, peak_groups, Z = city_peak_targets_for_day(\n",
    "        city, day, peak_cfg, slot_len_min=policy.behavioral.slot_length_minutes\n",
    "    )\n",
    "    cap_mode = peak_cfg.cap_mode if peak_cfg else \"slot\"\n",
    "\n",
    "    # Solve\n",
    "    if solver.solver_family == \"lp\":\n",
    "        usage_opt, flows = solve_lp(\n",
    "            mef, usage, W, solver,\n",
    "            peak_mask=peak_mask, peak_groups=peak_groups, Z=Z,\n",
    "            cap_mode=cap_mode, floor_vec=floor_vec\n",
    "        )\n",
    "    elif solver.solver_family == \"milp\":\n",
    "        usage_opt, flows = solve_milp(\n",
    "            mef, usage, W, solver,\n",
    "            peak_mask=peak_mask, peak_groups=peak_groups, Z=Z,\n",
    "            cap_mode=cap_mode, floor_vec=floor_vec\n",
    "        )\n",
    "    elif solver.solver_family == \"greedy\":\n",
    "        usage_opt, flows = solve_greedy(mef, usage, W, solver)\n",
    "    else:\n",
    "        raise ValueError(\"Unknown solver family.\")\n",
    "\n",
    "    # Metrics (per day)\n",
    "    base = compute_emissions_totals(usage, aef, mef)\n",
    "    post = compute_emissions_totals(usage_opt, aef, mef)\n",
    "    moved_kwh = float(np.maximum(0.0, usage - usage_opt).sum())\n",
    "    median_shift_minutes = weighted_median_shift_minutes(flows, policy.behavioral.slot_length_minutes)\n",
    "\n",
    "    metrics_row = {\n",
    "            \"ca_id\": ca_id,\n",
    "            \"city\": city,\n",
    "            \"day\": day,\n",
    "            \"baseline_E_avg_g\": base[\"E_avg_g\"],\n",
    "            \"post_E_avg_g\": post[\"E_avg_g\"],\n",
    "            \"delta_E_avg_g\": base[\"E_avg_g\"] - post[\"E_avg_g\"],\n",
    "            \"baseline_E_marg_g\": base[\"E_marg_g\"],\n",
    "            \"post_E_marg_g\": post[\"E_marg_g\"],\n",
    "            \"delta_E_marg_g\": base[\"E_marg_g\"] - post[\"E_marg_g\"],\n",
    "            \"baseline_kwh\": float(usage.sum()),\n",
    "            \"post_kwh\": float(usage_opt.sum()),\n",
    "            \"moved_kwh\": moved_kwh,\n",
    "            \"avg_shift_minutes_energy_weighted\": float(\n",
    "                (sum(abs(s - t) * val for (t, s, val) in flows) / max(moved_kwh, 1e-9)) * policy.behavioral.slot_length_minutes\n",
    "            ) if flows else 0.0,\n",
    "            \"median_shift_minutes_energy_weighted\": median_shift_minutes,\n",
    "\n",
    "        }\n",
    "\n",
    "    # Moves table rows\n",
    "    move_rows = flows_table(ca_id, city, day, flows, mef, aef, slot_len_min=policy.behavioral.slot_length_minutes)\n",
    "\n",
    "    # Optimised 48-slot rows\n",
    "    opt_rows = None\n",
    "    if emit_optimised_rows:\n",
    "        rows = []\n",
    "        for s in range(len(usage_opt)):\n",
    "            ts = day + timedelta(minutes=int(s * policy.behavioral.slot_length_minutes))\n",
    "            rows.append({\n",
    "                    \"ca_id\": ca_id,\n",
    "                    \"city\": city,\n",
    "                    \"day\": day,\n",
    "                    \"slot\": s,\n",
    "                    \"date\": ts,\n",
    "                    \"optimised_value\": float(usage_opt[s]),\n",
    "            })\n",
    "        opt_rows = rows\n",
    "\n",
    "    return (metrics_row, move_rows, opt_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c013e708",
   "metadata": {},
   "outputs": [],
   "source": [
    "def solve_one_move_milp(\n",
    "    mef: np.ndarray,\n",
    "    usage: np.ndarray,\n",
    "    W_slots: int,\n",
    "    *,\n",
    "    floor_vec: Optional[np.ndarray] = None,\n",
    "    peak_groups: Optional[List[List[int]]] = None,\n",
    "    Z: Optional[float] = None,\n",
    "    slot_len_min: int = 30,\n",
    "    dest_upper_bounds: Optional[np.ndarray] = None,\n",
    "    remaining_regional_budget_kwh: Optional[float] = None,\n",
    "    milp_solver: str = \"cbc\",\n",
    "    milp_opts: Optional[Dict[str, Any]] = None,\n",
    ") -> Tuple[np.ndarray, List[Tuple[int,int,float]]]:\n",
    "    assert pyo is not None\n",
    "    T = len(usage)\n",
    "    slots_per_hour = 60 // slot_len_min\n",
    "\n",
    "    # Build allowed pairs and one big index\n",
    "    pairs = []\n",
    "    for t in range(T):\n",
    "        s0, s1 = max(0, t - W_slots), min(T, t + W_slots + 1)\n",
    "        for s in range(s0, s1):\n",
    "            if s != t:\n",
    "                pairs.append((t, s))\n",
    "    P = len(pairs)\n",
    "\n",
    "    m = pyo.ConcreteModel()\n",
    "    m.P = pyo.RangeSet(0, P-1)\n",
    "\n",
    "    # decision: pick at most one pair\n",
    "    m.z = pyo.Var(m.P, domain=pyo.Binary)\n",
    "    m.y = pyo.Var(m.P, domain=pyo.NonNegativeReals)  # moved kWh\n",
    "\n",
    "    # Big-M bounds on y if pair selected\n",
    "    # M_{t,s} from per-pair feasibility (same bounds as greedy)\n",
    "    M = []\n",
    "    for i, (t, s) in enumerate(pairs):\n",
    "        src_floor = float(floor_vec[t]) if (floor_vec is not None) else 0.0\n",
    "        ub_src = max(0.0, usage[t] - src_floor)\n",
    "\n",
    "        if dest_upper_bounds is not None and np.isfinite(dest_upper_bounds[s]):\n",
    "            ub_dest = max(0.0, float(dest_upper_bounds[s] - usage[s]))\n",
    "        else:\n",
    "            ub_dest = float(\"inf\")\n",
    "\n",
    "        if Z is not None and peak_groups is not None:\n",
    "            h = t // slots_per_hour\n",
    "            grp = list(range(h*slots_per_hour, (h+1)*slots_per_hour))\n",
    "            base_hour = float(usage[grp].sum())\n",
    "            ub_peak = Z * base_hour\n",
    "        else:\n",
    "            ub_peak = float(\"inf\")\n",
    "\n",
    "        ub_reg = float(remaining_regional_budget_kwh) if (remaining_regional_budget_kwh is not None) else float(\"inf\")\n",
    "        M_i = min(ub_src, ub_dest, ub_peak, ub_reg)\n",
    "        if not np.isfinite(M_i) or M_i < 0:\n",
    "            M_i = 0.0\n",
    "        M.append(M_i)\n",
    "\n",
    "    # y_i <= M_i * z_i\n",
    "    m.bigM = pyo.ConstraintList()\n",
    "    for i in range(P):\n",
    "        m.bigM.add(m.y[i] <= M[i] * m.z[i])\n",
    "\n",
    "    # at most one pair\n",
    "    m.one = pyo.Constraint(expr=sum(m.z[i] for i in m.P) <= 1)\n",
    "\n",
    "    # objective: maximize emissions reduction\n",
    "    # gain_i = (mef[t]-mef[s]) * y_i\n",
    "    gains = []\n",
    "    for i, (t, s) in enumerate(pairs):\n",
    "        gains.append((float(mef[t] - mef[s]), i))\n",
    "    m.obj = pyo.Objective(\n",
    "        expr=sum(max(0.0, g) * m.y[i] for (g, i) in gains),\n",
    "        sense=pyo.maximize\n",
    "    )\n",
    "\n",
    "    solver = pyo.SolverFactory(milp_solver or \"cbc\")\n",
    "    if milp_opts:\n",
    "        for k, v in milp_opts.items():\n",
    "            solver.options[k] = v\n",
    "    solver.solve(m, tee=False)\n",
    "\n",
    "    # build result\n",
    "    usage_opt = usage.astype(float).copy()\n",
    "    flows = []\n",
    "    for i in range(P):\n",
    "        yi = pyo.value(m.y[i])\n",
    "        zi = pyo.value(m.z[i])\n",
    "        if zi and yi and yi > 1e-12:\n",
    "            t, s = pairs[i]\n",
    "            usage_opt[t] -= yi\n",
    "            usage_opt[s] += yi\n",
    "            flows.append((t, s, float(yi)))\n",
    "            break  # only one move\n",
    "    return usage_opt, flows\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e782f648",
   "metadata": {},
   "source": [
    "#### LP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbd7ec64",
   "metadata": {},
   "outputs": [],
   "source": [
    "def solve_lp(\n",
    "    mef: np.ndarray,\n",
    "    usage: np.ndarray,\n",
    "    W_slots: int,\n",
    "    cfg: SolverConfig,\n",
    "    *,\n",
    "    peak_mask: Optional[np.ndarray] = None,\n",
    "    peak_groups: Optional[List[List[int]]] = None,\n",
    "    Z: Optional[float] = None,\n",
    "    cap_mode: Literal[\"slot\", \"hour\"] = \"slot\",\n",
    "    floor_vec: Optional[np.ndarray] = None,\n",
    "    dest_upper_bounds: Optional[np.ndarray] = None,\n",
    "    moved_kwh_cap: Optional[float] = None,\n",
    ") -> Tuple[np.ndarray, List[Tuple[int, int, float]]]:\n",
    "    \"\"\"\n",
    "    Linear program: minimise sum_s post_usage[s] * MEF[s] with constraints.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    mef, usage, W_slots, cfg : see pipeline docs\n",
    "    peak_mask : np.ndarray, optional\n",
    "        Boolean vector of destination peak slots.\n",
    "    peak_groups : List[List[int]], optional\n",
    "        Hour groups for 'hour' mode peak comfort.\n",
    "    Z : float, optional\n",
    "        Peak comfort reduction cap (e.g., 0.30).\n",
    "    cap_mode : {\"slot\",\"hour\"}, optional\n",
    "        Apply peak comfort per slot or per hour.\n",
    "    floor_vec : np.ndarray, optional\n",
    "        Per-slot household floors (kWh).\n",
    "    dest_upper_bounds : np.ndarray, optional\n",
    "        Per-destination max kWh for THIS customer (city anti-spike residuals).\n",
    "    moved_kwh_cap : float, optional\n",
    "        City-day remaining budget for kWh moved by THIS customer.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    usage_opt : np.ndarray\n",
    "        Optimised post-usage (kWh) per slot for this customer.\n",
    "    flows : List[Tuple[int,int,float]]\n",
    "        Non-zero flows (t -> s, kWh).\n",
    "    \"\"\"\n",
    "    assert cp is not None\n",
    "    T = len(mef)\n",
    "    pairs, by_src, by_dst = cached_pairs(T, W_slots)\n",
    "    P = len(pairs)\n",
    "\n",
    "    y = cp.Variable(P, nonneg=True)\n",
    "    cost = cp.sum(cp.multiply(y, np.array([mef[s] for (_, s) in pairs], dtype=float)))\n",
    "\n",
    "    cons = []\n",
    "\n",
    "    # Supply conservation: for each source slot t, sum_s y_{t->s} = usage[t]\n",
    "    for t in range(T):\n",
    "        idx = by_src[t]\n",
    "        cons.append(cp.sum(y[idx]) == float(usage[t]))\n",
    "\n",
    "    # Household per-slot minimum floors\n",
    "    if floor_vec is not None:\n",
    "        for s in range(T):\n",
    "            if floor_vec[s] > 0:\n",
    "                idx = by_dst[s]\n",
    "                if idx.size:\n",
    "                    cons.append(cp.sum(y[idx]) >= float(floor_vec[s]))\n",
    "\n",
    "    # Peak-hour comfort: do not reduce more than Z (per customer)\n",
    "    if peak_mask is not None and Z is not None:\n",
    "        if cap_mode == \"slot\":\n",
    "            for s in np.where(peak_mask)[0]:\n",
    "                base_s = float(usage[s])\n",
    "                if base_s > 1e-12:\n",
    "                    idx = by_dst[s]\n",
    "                    if idx.size:\n",
    "                        cons.append(cp.sum(y[idx]) >= (1.0 - Z) * base_s)\n",
    "        else:  # \"hour\"\n",
    "            if peak_groups:\n",
    "                for grp in peak_groups:\n",
    "                    base_h = float(usage[grp].sum())\n",
    "                    if base_h > 1e-12:\n",
    "                        idxs = np.unique(np.concatenate([by_dst[s] for s in grp if by_dst[s].size]))\n",
    "                        if idxs.size:\n",
    "                            cons.append(cp.sum(y[idxs]) >= (1.0 - Z) * base_h)\n",
    "\n",
    "    # Anti-spike per-slot residual capacity (city-day)\n",
    "    if dest_upper_bounds is not None:\n",
    "        for s in range(T):\n",
    "            if np.isfinite(dest_upper_bounds[s]):\n",
    "                idx = by_dst[s]\n",
    "                if idx.size:\n",
    "                    cons.append(cp.sum(y[idx]) <= float(dest_upper_bounds[s]))\n",
    "\n",
    "    # Regional moved-kWh cap for this customer (city-day remaining budget)\n",
    "    if moved_kwh_cap is not None:\n",
    "        stay = cp.sum([y[i] for i, (t, s) in enumerate(pairs) if t == s])\n",
    "        total = float(usage.sum())\n",
    "        cons.append(total - stay <= float(moved_kwh_cap))\n",
    "\n",
    "    prob = cp.Problem(cp.Minimize(cost), cons)\n",
    "    prob.solve(solver=cfg.lp_solver, **(cfg.lp_solver_opts or {}))\n",
    "\n",
    "    yv = np.asarray(y.value).reshape(-1)\n",
    "    usage_opt = np.zeros(T, dtype=float)\n",
    "    flows: List[Tuple[int, int, float]] = []\n",
    "    for i, val in enumerate(yv):\n",
    "        if val <= 1e-12:\n",
    "            continue\n",
    "        t, s = pairs[i]\n",
    "        usage_opt[s] += float(val)\n",
    "        flows.append((t, s, float(val)))\n",
    "    return usage_opt, flows\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "irpenv_4",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
