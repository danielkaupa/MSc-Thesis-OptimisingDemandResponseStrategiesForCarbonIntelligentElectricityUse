{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e0e511da",
   "metadata": {},
   "source": [
    "# irp-dbk24 - \"Optimising Demand Response Strategies for Carbon-Intelligent Electricity Use\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8069f2a",
   "metadata": {},
   "source": [
    "# Developing Optimisation Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bb3ff2b",
   "metadata": {},
   "source": [
    "https://www.ceicdata.com/en/india/electricity-consumption-utilities/electricity-consumption-utilities-delhi\n",
    "\n",
    "India Delhi Annual consumption (2023) 34,107 GWh\n",
    "--> Daily consumption (average) = 34,107 GWh / 365 = 93.4 GWh\n",
    "--> Average Hourly Consumption = 93.4 GWh / 24 = 3.89 GWh\n",
    "--> Average Half-Hourly Consumption = 3.89 GWh / 2 = 1.945 GWh\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ce35bed",
   "metadata": {},
   "source": [
    "**BEHAVIORAL CONSTRAINTS:**\n",
    "\n",
    "1. Maximum Moves Per Customer Per Day\n",
    "    * Constraint: moves_per_customer_per_day ≤ X (start with X = 1)\n",
    "    * Justification: I'm thinking that you can't ask a customer to change their routine too much, so we need to limit the number of moves that we would request in a single day. This minimises disruption to routine and makes it easier to communicate and for households to act on.\n",
    "    * Parameters Provided: X , timezone, day_boundaries (defaults to hours 0-23 inclusive), week boundaries (defaults to Monday-Sunday, but could also be ISO week)\n",
    "\n",
    "2. Maximum Moves Per Customer Per Week\n",
    "    * Constraint: moves_per_customer_per_week ≤ Y (start with Y = 3)\n",
    "    * Justification: Again, I'm thinking that you cannot ask a customer to change their routine too much, so we need to limit the number of moves that we would request in a single week. Prevents “alert fatigue” where customers stop responding if asked too often.\n",
    "    * Parameters Provided: Y, timezone, week boundaries (defaults to Monday-Sunday, but could also be ISO week)\n",
    "\n",
    "3. Max Shift Window\n",
    "    * Constraint: |t_original - t_shifted| ≤ H  (start with H=2 hours)\n",
    "    * Justification:  It is unreasonable to ask  customer to move all of their electricity usage to the middle of the night. There would likely be more success in requesting moves that are easily achievable. Therefore we set a constraint saying that power cannot be shifted more than H hours from its original usage time\n",
    "    * Parameters Provided: H_hours, slot_length=30min, inclusive=True\n",
    "\n",
    "4. Preserve Peak-Hour Comfort:\n",
    "    * Constraint: percentage_usage_reduced_in_peak_hours ≤ Z * customers usage in that peak hour (start with Z = 30%)\n",
    "    * Justification: This constraint is in place to ensure that we do not overly burden customers during peak usage times. By limiting the percentage of usage that can be shifted in a peak hour, we aim to maintain a level of comfort and convenience for customers during critical periods.\n",
    "    * Parameters Provided: Z_percent, scope={per_customer|per_city}, peak_hours={city{Delhi, Mumbai}, day{Monday, Tuesday, Wednesday, Thursday, Friday, Saturday, Sunday}, hour_slots(up to 6)}\n",
    "\n",
    "\n",
    "**PRACTICAL TECHNICAL CONSTRAINTS**\n",
    "\n",
    "1. Regional Maximum Shift Percentage\n",
    "    * Constraint: daily_total_shifted_power ≤ P% x regional_total_daily_average_load (start with p = 10%)\n",
    "    * Justification: The total amount of power moved through this optimisation should not be too much, otherwise it may be too much change for the grid to handle. As a starting point, I might recommend saying that the total amount of power can't be more than 10% of the total region's daily average load.\n",
    "    * Note - I have data for Delhi and Mumbai - > separated by city and so I can provide those statistics for the average daily load (based on external government statistics)\n",
    "    * Parameters Provided: P, regional_total_daily_average_load (kWh), city\n",
    "\n",
    "2. Household Minimum Usage per Slot:\n",
    "    * Constraint: shifted_usage_t ≥ max(min_baseline(customer,t), R% * robust_max(customer,t))\n",
    "    * Justification: When recommending a shifting of power, the power remaining cannot go below X. In other words - minimum electricity usage for that household must be maintained.\n",
    "    * Implementation Note: The minimum baseline of a customer's usage is calculated, as well as the robust maxiumum (excludes extreme outliers). The maximum of this minimum baseline and 10% of the robust maxiumum is taken and set as the boundary for minimum usage per slot.\n",
    "    * Note - This constraint is provided assuming the half-hourly usage.\n",
    "    * Parameters Provided: baseline_period={year|month|week|day}, baseline_type={average|absolute_min}, robust_max_percentile=95, R_percent=10\n",
    "\n",
    "3. Shifting Without Spiking\n",
    "    * Constraint: prevent shifting too many customers into the same low-MEF slot if it creates a regional peak.\n",
    "    * Justification: Avoids all of the power being shifted to one time slot, causing potential overloads and instability in the grid.\n",
    "    * Parameters Provided: alpha_peak_cap_percent=25 (per-slot upper cap: no more than +α% vs baseline)\n",
    "\n",
    "\n",
    "**HARDCODED CONSTRAINTS:**\n",
    "\n",
    "1. Total Consumption Conservation\n",
    "    * Constraint: Σ_t x_t = Σ_t original_x_t for each customer.\n",
    "    * Justification: The optimisation redistributes load; it cannot create or destroy energy.\n",
    "    * Parameters Provided: id_field: customer_id, conservation_horizon = {day|week|month|year} (defaults to day)\n",
    "\n",
    "2. Intra-Customer Conservation\n",
    "    * Constraint: All shifts remain within the same customer ID.\n",
    "    * Justification: No “borrowing” or “trading” energy between households.\n",
    "    * Parameters Provided: id_field: customer_id\n",
    "\n",
    "**Model Personality Levers**\n",
    "\n",
    "1. Convenience Prioritised Model:\n",
    "    * Shift the smallest total kWh possible while maximising emissions reduction per kWh shifted.\n",
    "    * Likely higher customer adoption\n",
    "    * Model: ConveniencePrioritised\n",
    "\n",
    "2. Impact Prioritised Model:\n",
    "    * Shift largest possible load from high-MEF to low-MEF within constraints.\n",
    "    * Maximises emissions reduction, but at higher inconvenience.\n",
    "    * Model: ImpactPrioritised\n",
    "\n",
    "3. High-Usage-Focus Model\n",
    "    * Target top percentile of consumers by total daily/weekly kWh.\n",
    "    * Potentially fewer customers affected for same emissions gain.\n",
    "    * Parameters Provided: target_high_usage_customers: True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50e82021",
   "metadata": {},
   "source": [
    "| **#**  | **Constraint Name**              | **Description**                                                        | **Parameters Provided**                                                              | **Notes / Implementation**                                                            |                                                                                          |                                 |                                                                    |                                                                                                                       |\n",
    "| ------ | -------------------------------- | ---------------------------------------------------------------------- | ------------------------------------------------------------------------------------ | ------------------------------------------------------------------------------------- | ---------------------------------------------------------------------------------------- | ------------------------------- | ------------------------------------------------------------------ | --------------------------------------------------------------------------------------------------------------------- |\n",
    "| **B1** | Max Moves per Customer per Day   | Limit number of slot shifts per customer in a day to avoid disruption. | `X`, `timezone`, `day_boundaries` (default 0–23), `week_boundaries` (Mon–Sun or ISO) | Count moves as slots with any nonzero shift; day boundaries in `Asia/Kolkata`.        |                                                                                          |                                 |                                                                    |                                                                                                                       |\n",
    "| **B2** | Max Moves per Customer per Week  | Limit number of shifts per week to prevent alert fatigue.              | `Y`, `timezone`, `week_boundaries` (Mon–Sun or ISO)                                  | Same counting method as daily; week can be fixed or ISO definition.                   |                                                                                          |                                 |                                                                    |                                                                                                                       |\n",
    "| **B3** | Max Shift Window                 | Restrict shifts to within H hours of original slot.                    | `H_hours`, `slot_length=30min`, `inclusive=True`                                     | Apply as a mask to disallow moves beyond H hours; ensures realism.                    |                                                                                          |                                 |                                                                    |                                                                                                                       |\n",
    "| **B4** | Peak-Hour Comfort                | Limit % reduction in peak hours to preserve convenience.               | `Z_percent`, \\`scope={per\\_customer                                                  | per\\_city}`, `peak\\_hours={city{Delhi,Mumbai},day,slots}\\`                            | Cap is total % reduction across all listed hours; peak hours list provided per city/day. |                                 |                                                                    |                                                                                                                       |\n",
    "| **T1** | Regional Maximum Shift %         | Cap total daily shifted load per city as % of average daily load.      | `P`, `regional_total_daily_average_load` (kWh), `city`                               | Apply per city; baseline from gov stats or calculated; prevents large system changes. |                                                                                          |                                 |                                                                    |                                                                                                                       |\n",
    "| **T2** | Household Minimum Usage per Slot | Keep usage above baseline or % of robust max to maintain essentials.   | \\`baseline\\_period={year                                                             | month                                                                                 | week                                                                                     | day}`, `baseline\\_type={average | absolute\\_min}`, `robust\\_max\\_percentile=95`, `R\\_percent=10\\`    | `robust_max` = percentile max per customer + time bucket; floor = max(baseline, R% \\* robust\\_max); exclude outliers. |\n",
    "| **T3** | Shifting Without Spiking         | Prevent overloading a low-MEF slot by shifting too much in.            | `alpha_peak_cap_percent=25`                                                          | Compare post-shift load to same-day baseline or historical slot avg; per city.        |                                                                                          |                                 |                                                                    |                                                                                                                       |\n",
    "| **H1** | Total Consumption Conservation   | Preserve total consumption per customer over horizon.                  | `id_field=customer_id`, \\`conservation\\_horizon={day                                 | week                                                                                  | month                                                                                    | year}\\` (default day)           | Sum of post-shift load must equal sum of original; tolerance 1e−6. |                                                                                                                       |\n",
    "| **H2** | Intra-Customer Conservation      | All shifts must stay within the same household.                        | `id_field=customer_id`                                                               | No inter-household energy transfer allowed.                                           |                                                                                          |                                 |                                                                    |                                                                                                                       |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "641e263a",
   "metadata": {},
   "source": [
    "| **Model**                  | **Description**                                                              | **Parameters**                                             |\n",
    "| -------------------------- | ---------------------------------------------------------------------------- | ---------------------------------------------------------- |\n",
    "| **ConveniencePrioritised** | Minimise total shifted kWh while maximising emissions reduction/kWh shifted. | `min_total_shift_kWh` (optional soft cap)                  |\n",
    "| **ImpactPrioritised**      | Maximise emissions reduction with largest possible shift within constraints. | `max_total_shift_kWh` (optional)                           |\n",
    "| **HighUsageFocus**         | Target top percentile by kWh use to concentrate shifts.                      | `target_high_usage_customers=True`, `percentile_threshold` |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2324b191",
   "metadata": {},
   "source": [
    "The user should easily be able to provide all of these parameters.\n",
    "There should be multiple models that are used (different solvers ,etc.) -> IU'm thinking three families of solvers (MILP , Greedy, and then Lp, but within those probably specifications for their different solvers - such as highs, GLPK etc. - the user should also be able to specify these parameters.\n",
    "There should be an option to show projgress to the user.\n",
    "The solver should have an option True/Flase for run with parallelisation yes or no, and then a paralellisation method specification - i.e. mpi if using a distributed environment like HPC, or other for local paralellisation., or None if false.\n",
    "\n",
    "Specialy notes:\n",
    "\n",
    "the emission factors are in grams per kWh - I'm not sure if any additional calulatons / 2 etc are need since I have the half hour data.\n",
    "\n",
    "The results should output all recommended shifts - listing the customer - the day , original time of usage, shifted (new) time, original emissions, emissions after shifting,delta in emissions, delta in power consumption, how many minutes forward or backward was moved, direction of the move, customer id"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54ea9221",
   "metadata": {},
   "source": [
    "Optimises using MEF only.\n",
    "\n",
    "Calculates both marginal savings and average-emissions savings for reporting.\n",
    "\n",
    "Outputs:\n",
    "\n",
    "“Marginal impact” (tCO₂ avoided)\n",
    "\n",
    "“% reduction of marginal emissions”\n",
    "\n",
    "“% reduction of total (average) emissions footprint”"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bef01600",
   "metadata": {},
   "source": [
    "### Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b7e360b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ────────────────────────────────────────────────────────────────────────────\n",
    "# Future (must be first)\n",
    "# ────────────────────────────────────────────────────────────────────────────\n",
    "from __future__ import annotations\n",
    "import pyomo.environ as pyo\n",
    "\n",
    "import numpy as np\n",
    "from scipy.optimize import minimize\n",
    "\n",
    "\n",
    "# ────────────────────────────────────────────────────────────────────────────\n",
    "# Jupyter/Notebook Setup\n",
    "# ────────────────────────────────────────────────────────────────────────────\n",
    "%matplotlib inline\n",
    "from IPython.display import display\n",
    "\n",
    "# ────────────────────────────────────────────────────────────────────────────\n",
    "# Standard Library\n",
    "# ────────────────────────────────────────────────────────────────────────────\n",
    "import binascii\n",
    "import calendar\n",
    "import json\n",
    "import logging\n",
    "import math\n",
    "import os\n",
    "import random\n",
    "import re\n",
    "import hashlib\n",
    "import inspect\n",
    "import time\n",
    "from copy import deepcopy\n",
    "from contextlib import contextmanager\n",
    "from dataclasses import dataclass\n",
    "from datetime import datetime, timedelta\n",
    "from functools import partial, wraps\n",
    "from itertools import combinations, product\n",
    "from multiprocessing import Manager, Pool, Lock, cpu_count\n",
    "from multiprocessing.pool import ThreadPool\n",
    "from pathlib import Path\n",
    "from typing import (\n",
    "    Any, Callable, Dict, Iterable, List, Mapping, Optional, Sequence, Tuple, Union\n",
    ")\n",
    "from zoneinfo import ZoneInfo\n",
    "import cvxpy as cp\n",
    "\n",
    "\n",
    "\n",
    "# ────────────────────────────────────────────────────────────────────────────\n",
    "# Core Data Handling\n",
    "# ────────────────────────────────────────────────────────────────────────────\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import polars as pl\n",
    "\n",
    "# ────────────────────────────────────────────────────────────────────────────\n",
    "# Machine Learning & Statistics\n",
    "# ────────────────────────────────────────────────────────────────────────────\n",
    "from feature_engine.creation import CyclicalFeatures\n",
    "from scipy.stats import kurtosis, skew, zscore\n",
    "import statsmodels.api as sm\n",
    "import statsmodels.formula.api as smf\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "\n",
    "from sklearn.base import BaseEstimator, TransformerMixin, clone\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.kernel_approximation import RBFSampler\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import (\n",
    "    mean_absolute_error,\n",
    "    mean_squared_error,\n",
    "    r2_score,\n",
    "    root_mean_squared_error,\n",
    ")\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import OneHotEncoder, SplineTransformer\n",
    "from sklearn.utils.validation import check_is_fitted\n",
    "\n",
    "# ────────────────────────────────────────────────────────────────────────────\n",
    "# Visualization\n",
    "# ────────────────────────────────────────────────────────────────────────────\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "import matplotlib.ticker as ticker\n",
    "import seaborn as sns\n",
    "\n",
    "# ────────────────────────────────────────────────────────────────────────────\n",
    "# Geospatial\n",
    "# ────────────────────────────────────────────────────────────────────────────\n",
    "import geopandas as gpd\n",
    "from shapely.geometry import Point, Polygon\n",
    "from shapely.wkb import loads\n",
    "from pyproj import Proj, transform\n",
    "\n",
    "\n",
    "from dataclasses import dataclass\n",
    "from typing import Optional\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52bf107d",
   "metadata": {},
   "source": [
    "## Loading Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfdc579d",
   "metadata": {},
   "source": [
    "### Directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5cab8a51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DIRECTORIES AND PATHS\n",
    "base_data_directory = \"data\"    # Base directory where the dataframes will be saved\n",
    "hitachi_data_directory = os.path.join(base_data_directory, \"hitachi_copy\")      # Directory where the dataframes will be saved\n",
    "meter_save_directory = os.path.join(hitachi_data_directory, \"meter_primary_files\")       # Directory for meter readings\n",
    "\n",
    "marginal_emissions_development_directory = os.path.join(base_data_directory, \"marginal_emissions_development\")  # Directory for marginal emissions development data\n",
    "marginal_emissions_results_directory = os.path.join(marginal_emissions_development_directory, \"results\")\n",
    "marginal_emissions_logs_directory = os.path.join(marginal_emissions_development_directory, \"logs\")\n",
    "\n",
    "optimisation_development_directory = os.path.join(base_data_directory, \"optimisation_development\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a3237fb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "Contents of 'data/optimisation_development' and subdirectories:\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "  - ../optimisation_development/.DS_Store\n",
      "  - ../optimisation_development/average_emissions_2022-05-04_to_2022-05-18.parquet\n",
      "  - ../optimisation_development/customers_ids_with_emissions.parquet\n",
      "  - ../optimisation_development/customers_ids_with_marginal_emissions.parquet\n",
      "  - ../optimisation_development/marginal_and_average_emissions_2022-05-04_to_2022-05-18.parquet\n",
      "  - ../optimisation_development/marginal_emissions_2022-05-04_to_2022-05-18.parquet\n",
      "  - ../optimisation_development/meter_readings_2022-05-04_to_2022-05-18.parquet\n",
      "  - ../optimisation_development/meter_readings_2022-05-04_to_2022-05-18_with_marginal_emissions.parquet\n",
      "  - ../optimisation_development/results/.DS_Store\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"-\" * 120)\n",
    "print(f\"Contents of '{optimisation_development_directory}' and subdirectories:\\n\" + \"-\" * 120)\n",
    "for root, dirs, files in os.walk(optimisation_development_directory):\n",
    "    for f in sorted(files):\n",
    "        rel_dir = os.path.relpath(root, hitachi_data_directory)\n",
    "        rel_file = os.path.join(rel_dir, f) if rel_dir != \".\" else f\n",
    "        print(f\"  - {rel_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50b12955",
   "metadata": {},
   "source": [
    "### File Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b76ff0d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining Full files\n",
    "marginal_emissions_filename = \"meter_readings_2022-05-04_to_2022-05-18_with_marginal_emissions\"\n",
    "marginal_emissions_filepath = os.path.join(optimisation_development_directory, marginal_emissions_filename + \".parquet\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5f69e249",
   "metadata": {},
   "outputs": [],
   "source": [
    "marginal_emissions_pldf = pl.read_parquet(marginal_emissions_filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "57903a22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------------------------------------------------------------------------\n",
      "Schema of marginal_emissions_pldf\n",
      "------------------------------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Schema([('ca_id', String),\n",
       "        ('date', Datetime(time_unit='us', time_zone='Asia/Kolkata')),\n",
       "        ('city', Categorical(ordering='physical')),\n",
       "        ('customer_longitude', Float64),\n",
       "        ('customer_latitude', Float64),\n",
       "        ('value', Float64),\n",
       "        ('demand_met_kWh', Float64),\n",
       "        ('marginal_emissions_grams_co2_per_kWh', Float64),\n",
       "        ('average_emissions_grams_co2_per_kWh', Float64)])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(\"-\"  *120)\n",
    "print(\"Schema of marginal_emissions_pldf\\n\"+ \"-\"  *120)\n",
    "display(marginal_emissions_pldf.schema)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e44fe6d",
   "metadata": {},
   "source": [
    "## Developing Optimisation Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d7d9ab7",
   "metadata": {},
   "source": [
    "### Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "eb6f28d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import polars as pl\n",
    "\n",
    "def add_day_slot(df: pl.DataFrame) -> pl.DataFrame:\n",
    "    # date already tz-aware Asia/Kolkata per your schema\n",
    "    return (\n",
    "        df\n",
    "        .with_columns([\n",
    "            pl.col(\"date\").dt.truncate(\"1d\").alias(\"day\"),\n",
    "            (pl.col(\"date\").dt.hour() * 2 + (pl.col(\"date\").dt.minute() >= 30).cast(pl.Int32)).alias(\"slot\")\n",
    "        ])\n",
    "        .select([\"ca_id\", \"city\", \"day\", \"date\", \"slot\", \"value\", \"ME\", \"customer_longitude\", \"customer_latitude\"])\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f8d7358f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def group_to_arrays(sub: pl.DataFrame):\n",
    "    # sub has columns: ['slot','value','ME', ...] for a single (ca_id, day)\n",
    "    # Build 48-slot arrays, default zeros for usage; ME must be present to solve.\n",
    "    usage = np.zeros(48, dtype=float)\n",
    "    mef   = np.full(48, np.nan, dtype=float)\n",
    "\n",
    "    sl = sub[\"slot\"].to_numpy().astype(int)\n",
    "    usage[sl] = sub[\"value\"].to_numpy()\n",
    "    mef[sl]   = sub[\"ME\"].to_numpy()\n",
    "\n",
    "    # If any ME is missing, you can either skip this day or impute.\n",
    "    if np.isnan(mef).any():\n",
    "        return None, None  # signal to skip\n",
    "    return mef, usage\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "3633b71f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import timedelta\n",
    "\n",
    "def materialise_optimised_rows(ca_id, city, day_ts, usage_opt, tz=\"Asia/Kolkata\"):\n",
    "    # day_ts is midnight (IST) for that day\n",
    "    rows = []\n",
    "    for s in range(48):\n",
    "        ts = day_ts + timedelta(minutes=30*s)  # still tz-aware if day_ts is tz-aware\n",
    "        rows.append({\n",
    "            \"ca_id\": ca_id,\n",
    "            \"city\": city,\n",
    "            \"date\": ts,\n",
    "            \"day\": day_ts,\n",
    "            \"slot\": s,\n",
    "            \"optimised_value\": float(usage_opt[s]),\n",
    "        })\n",
    "    return rows\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "5ff3f53b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Window conversion\n",
    "def hours_to_slots(window_hours: float) -> int:\n",
    "    # half-hour data → each slot = 0.5 hour\n",
    "    return int((window_hours * 60) // 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "b4c793ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build allowed pairs within a window\n",
    "def allowed_pairs(T: int, W_slots: int):\n",
    "    # returns list of (t, s) with |t-s| <= W_slots\n",
    "    pairs = []\n",
    "    for t in range(T):\n",
    "        s0 = max(0, t - W_slots)\n",
    "        s1 = min(T, t + W_slots + 1)\n",
    "        for s in range(s0, s1):\n",
    "            pairs.append((t, s))\n",
    "    return pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "d3e6ca83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick day-level pre-check (optional): upper bound of savings if you could freely reorder within window\n",
    "def potential_savings_upper_bound(mef, usage, W_slots):\n",
    "    # Conservative, fast heuristic: sort within window bands\n",
    "    # (For a true bound you’d solve the continuous LP on that day; this is enough to gate tiny days.)\n",
    "    # Return an estimated ΔE_upper (non-negative).\n",
    "    # For initial runs, just return a large number to effectively disable the trigger.\n",
    "    return float('inf')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "4485aca4",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class ShiftPolicy:\n",
    "    window_hours: float                # e.g., 1.5\n",
    "    min_day_trigger_frac: Optional[float] = None   # e.g., 0.05 → skip days with very low potential\n",
    "    max_shift_frac: Optional[float] = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8df40153",
   "metadata": {},
   "source": [
    "#### Metrics "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "4fe7011d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(mef, usage_base, usage_opt, flows, slot_minutes=30):\n",
    "    mef = np.asarray(mef, float)\n",
    "    u0  = np.asarray(usage_base, float)\n",
    "    u1  = np.asarray(usage_opt,  float)\n",
    "\n",
    "    E_base = float((u0 * mef).sum())\n",
    "    E_opt  = float((u1 * mef).sum())\n",
    "    dE     = E_base - E_opt\n",
    "\n",
    "    moved_total = float(np.maximum(0, u0 - u1).sum())\n",
    "    day_demand  = float(u0.sum())\n",
    "    share_moved = moved_total / day_demand if day_demand > 0 else 0.0\n",
    "\n",
    "    forward = sum(val for (t, s, val) in flows if s > t)\n",
    "    backward = sum(val for (t, s, val) in flows if s < t)\n",
    "\n",
    "    # avg shift distance in slots (and minutes)\n",
    "    if flows:\n",
    "        avg_steps = sum(abs(s - t) * val for (t, s, val) in flows) / moved_total\n",
    "    else:\n",
    "        avg_steps = 0.0\n",
    "    avg_minutes = avg_steps * slot_minutes\n",
    "\n",
    "    return {\n",
    "        \"E_base\": E_base,\n",
    "        \"E_opt\": E_opt,\n",
    "        \"delta_E\": dE,\n",
    "        \"moved_kWh\": moved_total,\n",
    "        \"share_moved\": share_moved,\n",
    "        \"forward_kWh\": forward,\n",
    "        \"backward_kWh\": backward,\n",
    "        \"avg_shift_steps\": avg_steps,\n",
    "        \"avg_shift_minutes\": avg_minutes,\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af61f206",
   "metadata": {},
   "source": [
    "#### Mixed Integer Linear Programming (MILP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "0ab7921b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimise_day_milp(mef, usage, policy: ShiftPolicy):\n",
    "    T = len(mef)\n",
    "    W = hours_to_slots(policy.window_hours)\n",
    "    pairs = allowed_pairs(T, W)\n",
    "    day_demand = float(sum(usage))\n",
    "\n",
    "    # Optional: skip day by trigger\n",
    "    # if policy.min_day_trigger_frac:\n",
    "    #   if potential_savings_upper_bound(...) < policy.min_day_trigger_frac * day_demand: return usage.copy(), None\n",
    "\n",
    "    m = pyo.ConcreteModel()\n",
    "    m.P = pyo.Set(initialize=list(range(len(pairs))))\n",
    "    idx = {i: pairs[i] for i in range(len(pairs))}\n",
    "\n",
    "    # Decision: flow from t to s\n",
    "    m.y = pyo.Var(m.P, domain=pyo.NonNegativeReals)\n",
    "\n",
    "    # Cost = emissions at destination s\n",
    "    def obj_rule(m):\n",
    "        return sum(m.y[i] * mef[idx[i][1]] for i in m.P)\n",
    "    m.obj = pyo.Objective(rule=obj_rule, sense=pyo.minimize)\n",
    "\n",
    "    # Every original slot’s energy must be fully re-assigned\n",
    "    # sum_s y_{t->s} = usage_t\n",
    "    def supply_rule(m, t):\n",
    "        return sum(m.y[i] for i in m.P if idx[i][0] == t) == usage[t]\n",
    "    m.supply = pyo.Constraint(range(T), rule=supply_rule)\n",
    "\n",
    "    # (Optional) Max total shift cap per day:\n",
    "    # Let \"stay\" be y_{t->t}. Then moved = day_demand - sum_t y_{t->t} <= max_shift_frac * day_demand\n",
    "    if policy.max_shift_frac is not None:\n",
    "        def cap_rule(m):\n",
    "            stay = sum(m.y[i] for i in m.P if idx[i][0] == idx[i][1])\n",
    "            return day_demand - stay <= policy.max_shift_frac * day_demand\n",
    "        m.cap = pyo.Constraint(rule=cap_rule)\n",
    "\n",
    "    # Solve\n",
    "    solver = pyo.SolverFactory(\"cbc\")  # or gurobi/cplex if available\n",
    "    solver.solve(m, tee=False)\n",
    "\n",
    "    # Reconstruct optimised usage and flows\n",
    "    y = [pyo.value(m.y[i]) for i in m.P]\n",
    "    usage_opt = [0.0] * T\n",
    "    for i, val in enumerate(y):\n",
    "        t, s = idx[i]\n",
    "        usage_opt[s] += val\n",
    "\n",
    "    # Optional: return the sparse flow list for metrics\n",
    "    flows = [(idx[i][0], idx[i][1], y[i]) for i in m.P if y[i] > 1e-9]\n",
    "    return usage_opt, flows\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02347f65",
   "metadata": {},
   "source": [
    "#### Continuous Optimisation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "e18d855d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimise_continuous(mef, usage, shift_window=2):\n",
    "    \"\"\"\n",
    "    mef, usage: arrays shape (T,)\n",
    "    shift_window: half-hour slots allowed for shifting\n",
    "    \"\"\"\n",
    "    T = len(mef)\n",
    "\n",
    "    def objective(x):\n",
    "        return np.dot(x, mef)\n",
    "\n",
    "    def constraint_total(x):\n",
    "        return np.sum(x) - np.sum(usage)\n",
    "\n",
    "    bounds = [(0, None) for _ in range(T)]\n",
    "\n",
    "    # Initial guess = original usage\n",
    "    x0 = usage.copy()\n",
    "\n",
    "    cons = [{'type': 'eq', 'fun': constraint_total}]\n",
    "    # Shift window constraint would need to be handled with penalty terms\n",
    "\n",
    "    res = minimize(objective, x0, method='SLSQP', bounds=bounds, constraints=cons)\n",
    "    return res.x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "c8a31ba8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimise_day_lp(mef, usage, policy: ShiftPolicy):\n",
    "    T = len(mef)\n",
    "    W = hours_to_slots(policy.window_hours)\n",
    "    pairs = allowed_pairs(T, W)\n",
    "    P = len(pairs)\n",
    "    day_demand = float(sum(usage))\n",
    "\n",
    "    y = cp.Variable(P, nonneg=True)\n",
    "    cost = cp.sum([y[i] * mef[pairs[i][1]] for i in range(P)])\n",
    "\n",
    "    cons = []\n",
    "    # Supply constraints: sum_s y_{t->s} = usage_t\n",
    "    for t in range(T):\n",
    "        cons.append(cp.sum([y[i] for i in range(P) if pairs[i][0] == t]) == usage[t])\n",
    "\n",
    "    # Max shift cap (optional)\n",
    "    if policy.max_shift_frac is not None:\n",
    "        stay = cp.sum([y[i] for i in range(P) if pairs[i][0] == pairs[i][1]])\n",
    "        cons.append(day_demand - stay <= policy.max_shift_frac * day_demand)\n",
    "\n",
    "    prob = cp.Problem(cp.Minimize(cost), cons)\n",
    "    prob.solve(solver=cp.GLPK)  # or ECOS/OSQP/CPLEX/Gurobi\n",
    "\n",
    "    usage_opt = [0.0] * T\n",
    "    for i, val in enumerate(y.value):\n",
    "        t, s = pairs[i]\n",
    "        usage_opt[s] += float(val)\n",
    "\n",
    "    flows = [(pairs[i][0], pairs[i][1], float(y.value[i])) for i in range(P) if y.value[i] > 1e-9]\n",
    "    return usage_opt, flows\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af71bdc1",
   "metadata": {},
   "source": [
    "#### Greedy Heuristic Skeleton\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "146ef4d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def greedy_shift(mef, usage, shift_window=2):\n",
    "    \"\"\"\n",
    "    Simple greedy shift:\n",
    "    Move load from highest-MEF slots to lowest-MEF slots within ±shift_window.\n",
    "    \"\"\"\n",
    "    usage = usage.copy()\n",
    "    T = len(usage)\n",
    "\n",
    "    for t in np.argsort(-mef):  # high to low MEF\n",
    "        if usage[t] > 0:\n",
    "            window_start = max(0, t - shift_window)\n",
    "            window_end = min(T, t + shift_window + 1)\n",
    "\n",
    "            # Find best slot in window\n",
    "            best_slot = min(range(window_start, window_end), key=lambda s: mef[s])\n",
    "            if mef[best_slot] < mef[t]:\n",
    "                shift_amount = usage[t]  # move all (can adapt to partial moves)\n",
    "                usage[t] -= shift_amount\n",
    "                usage[best_slot] += shift_amount\n",
    "    return usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "68da4ed4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimise_day_greedy(mef, usage, policy: ShiftPolicy):\n",
    "    T = len(mef)\n",
    "    W = hours_to_slots(policy.window_hours)\n",
    "    usage_opt = usage.astype(float).copy()\n",
    "    day_demand = float(usage_opt.sum())\n",
    "    moved_so_far = 0.0\n",
    "    flows = []\n",
    "\n",
    "    # Optional: early exit via trigger (skip tiny days)\n",
    "    # if policy.min_day_trigger_frac and day_demand < small_threshold: return usage_opt, []\n",
    "\n",
    "    order = np.argsort(-mef)  # start from dirtiest slots\n",
    "    for t in order:\n",
    "        if usage_opt[t] <= 0:\n",
    "            continue\n",
    "        s0, s1 = max(0, t - W), min(T, t + W + 1)\n",
    "\n",
    "        # pick the cleanest slot in the window\n",
    "        s_best = min(range(s0, s1), key=lambda s: mef[s])\n",
    "        if mef[s_best] >= mef[t]:\n",
    "            continue  # no gain\n",
    "\n",
    "        # How much can we move?\n",
    "        can_move = usage_opt[t]\n",
    "\n",
    "        # Respect max shift cap (optional)\n",
    "        if policy.max_shift_frac is not None:\n",
    "            cap_remaining = policy.max_shift_frac * day_demand - moved_so_far\n",
    "            if cap_remaining <= 1e-9:\n",
    "                break\n",
    "            can_move = min(can_move, cap_remaining)\n",
    "\n",
    "        # Respect optional min move trigger (slot-level)\n",
    "        if policy.min_day_trigger_frac is not None:\n",
    "            if can_move < policy.min_day_trigger_frac * day_demand:\n",
    "                continue\n",
    "\n",
    "        # Move the energy\n",
    "        usage_opt[t] -= can_move\n",
    "        usage_opt[s_best] += can_move\n",
    "        moved_so_far += can_move\n",
    "        flows.append((t, s_best, can_move))\n",
    "\n",
    "    return usage_opt, flows\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "555c1ed5",
   "metadata": {},
   "source": [
    "#### Orchestrator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "8716347e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Literal, Optional, Tuple, List, Dict\n",
    "\n",
    "def run_optimisation_pipeline(\n",
    "    df_raw: pl.DataFrame,\n",
    "    policy: ShiftPolicy,\n",
    "    method: Literal[\"lp\",\"milp\",\"greedy\"]=\"lp\",\n",
    "    emit_optimised_rows: bool=False,\n",
    ") -> Tuple[pl.DataFrame, pl.DataFrame, Optional[pl.DataFrame]]:\n",
    "\n",
    "    df = add_day_slot(df_raw)\n",
    "\n",
    "    # Ensure deterministic order for group iteration\n",
    "    df = df.sort([\"ca_id\", \"day\", \"slot\"])\n",
    "\n",
    "    metrics_rows: List[Dict] = []\n",
    "    flow_rows: List[Dict] = []\n",
    "    optim_rows: List[Dict] = [] if emit_optimised_rows else None\n",
    "\n",
    "    # Group by (ca_id, day)\n",
    "    gb = df.group_by([\"ca_id\", \"day\", \"city\"], maintain_order=True)\n",
    "\n",
    "    # Pick solver function\n",
    "    if method == \"lp\":\n",
    "        solve_fn = optimise_day_lp\n",
    "    elif method == \"milp\":\n",
    "        solve_fn = optimise_day_milp\n",
    "    elif method == \"greedy\":\n",
    "        solve_fn = optimise_day_greedy\n",
    "    else:\n",
    "        raise ValueError(\"method must be one of: 'lp', 'milp', 'greedy'\")\n",
    "\n",
    "    for sub in gb:\n",
    "        subdf = sub[1]  # (key, frame). Polars returns tuples when iterating groups.\n",
    "        ca_id = subdf[\"ca_id\"][0]\n",
    "        city  = subdf[\"city\"][0]\n",
    "        day   = subdf[\"day\"][0]\n",
    "\n",
    "        mef, usage = group_to_arrays(subdf.select([\"slot\",\"value\",\"ME\"]))\n",
    "        if mef is None:\n",
    "            # Skip days with missing ME (or implement imputation)\n",
    "            continue\n",
    "\n",
    "        # Solve\n",
    "        usage_opt, flows = solve_fn(mef, usage, policy)\n",
    "\n",
    "        # Metrics\n",
    "        m = compute_metrics(mef, usage, usage_opt, flows)\n",
    "        metrics_rows.append({\n",
    "            \"ca_id\": ca_id,\n",
    "            \"city\": city,\n",
    "            \"day\": day,\n",
    "            **m\n",
    "        })\n",
    "\n",
    "        # Flows (t->s)\n",
    "        for (t, s, val) in flows:\n",
    "            flow_rows.append({\n",
    "                \"ca_id\": ca_id,\n",
    "                \"city\": city,\n",
    "                \"day\": day,\n",
    "                \"t_slot\": int(t),\n",
    "                \"s_slot\": int(s),\n",
    "                \"kwh\": float(val),\n",
    "                \"direction\": \"forward\" if s > t else (\"backward\" if s < t else \"stay\"),\n",
    "                \"abs_steps\": int(abs(s - t)),\n",
    "            })\n",
    "\n",
    "        # Optional: materialise optimised 48 rows\n",
    "        if emit_optimised_rows:\n",
    "            optim_rows.extend(materialise_optimised_rows(ca_id, city, day, usage_opt))\n",
    "\n",
    "    metrics_df = pl.DataFrame(metrics_rows) if metrics_rows else pl.DataFrame()\n",
    "    flows_df   = pl.DataFrame(flow_rows)   if flow_rows   else pl.DataFrame()\n",
    "    optim_df   = pl.DataFrame(optim_rows)  if emit_optimised_rows and optim_rows else None\n",
    "\n",
    "    return metrics_df, flows_df, optim_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "d7201935",
   "metadata": {},
   "outputs": [],
   "source": [
    "# daily_avg = df.group_by(\"ca_id\").agg(pl.mean(\"value\").alias(\"daily_avg\"))\n",
    "# weekly_avg = df.group_by([\"ca_id\", \"week\"]).agg(pl.mean(\"value\").alias(\"weekly_avg\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "f1ea8af1",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[44], line 8\u001b[0m\n\u001b[1;32m      6\u001b[0m results \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m policy \u001b[38;5;129;01min\u001b[39;00m scenarios:\n\u001b[0;32m----> 8\u001b[0m     metrics, flows, _ \u001b[38;5;241m=\u001b[39m run_optimisation_pipeline(\u001b[43mdf\u001b[49m, policy, method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlp\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      9\u001b[0m     metrics \u001b[38;5;241m=\u001b[39m metrics\u001b[38;5;241m.\u001b[39mwith_columns(pl\u001b[38;5;241m.\u001b[39mlit(\u001b[38;5;28mstr\u001b[39m(policy))\u001b[38;5;241m.\u001b[39malias(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpolicy\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[1;32m     10\u001b[0m     results\u001b[38;5;241m.\u001b[39mappend(metrics)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "scenarios = [\n",
    "    ShiftPolicy(window_hours=1, max_shift_frac=0.3),\n",
    "    ShiftPolicy(window_hours=2, max_shift_frac=None),\n",
    "    ShiftPolicy(window_hours=4, min_day_trigger_frac=0.05),\n",
    "]\n",
    "results = []\n",
    "for policy in scenarios:\n",
    "    metrics, flows, _ = run_optimisation_pipeline(df, policy, method=\"lp\")\n",
    "    metrics = metrics.with_columns(pl.lit(str(policy)).alias(\"policy\"))\n",
    "    results.append(metrics)\n",
    "final_metrics = pl.concat(results)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d473233",
   "metadata": {},
   "source": [
    "### Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14596ac6",
   "metadata": {},
   "source": [
    "#### Mixed Integer Programming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0eb0ca02",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glpk\n",
    "import cvxpy as cp\n",
    "from cvxpy import GLPK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ed161b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example policy: 1.5h window, no trigger, no cap (baseline)\n",
    "policy = ShiftPolicy(window_hours=1.5, min_day_trigger_frac=None, max_shift_frac=None)\n",
    "\n",
    "metrics_df, flows_df, optim_df = run_optimisation_pipeline(\n",
    "    df_raw=marginal_emissions_pldf,   # your joined table with ME + value\n",
    "    policy=policy,\n",
    "    method=\"lp\",\n",
    "    emit_optimised_rows=True\n",
    ")\n",
    "\n",
    "# Save for analysis\n",
    "# metrics: one row per customer-day with emissions & movement stats\n",
    "metrics_df.write_parquet(\"metrics_by_day.parquet\")\n",
    "# flows: sparse flows per customer-day, great for distance/direction analysis\n",
    "flows_df.write_parquet(\"flows_by_day.parquet\")\n",
    "# optimised rows: 48 rows per customer-day with optimised_value\n",
    "if optim_df is not None:\n",
    "    optim_df.write_parquet(\"optimised_usage_rows.parquet\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "236f1fd0",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'ShiftPolicy' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m policy \u001b[38;5;241m=\u001b[39m \u001b[43mShiftPolicy\u001b[49m(window_hours\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1.0\u001b[39m, max_shift_frac\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.3\u001b[39m)  \u001b[38;5;66;03m# cap at 30% of day demand\u001b[39;00m\n\u001b[1;32m      2\u001b[0m metrics_milp, flows_milp, _ \u001b[38;5;241m=\u001b[39m run_optimisation_pipeline(marginal_emissions_pldf, policy, method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmilp\u001b[39m\u001b[38;5;124m\"\u001b[39m, emit_optimised_rows\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'ShiftPolicy' is not defined"
     ]
    }
   ],
   "source": [
    "policy = ShiftPolicy(window_hours=1.0, max_shift_frac=0.3)  # cap at 30% of day demand\n",
    "metrics_milp, flows_milp, _ = run_optimisation_pipeline(marginal_emissions_pldf, policy, method=\"milp\", emit_optimised_rows=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63a0b221",
   "metadata": {},
   "outputs": [],
   "source": [
    "policy = ShiftPolicy(window_hours=1.5, min_day_trigger_frac=None, max_shift_frac=0.30)\n",
    "metrics_greedy, flows_greedy, _ = run_optimisation_pipeline(df_raw=marginal_emissions_pldf, policy, method=\"greedy\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92499518",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (1_415_224, 12)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>ca_id</th><th>city</th><th>day</th><th>E_base</th><th>E_opt</th><th>delta_E</th><th>moved_kWh</th><th>share_moved</th><th>forward_kWh</th><th>backward_kWh</th><th>avg_shift_steps</th><th>avg_shift_minutes</th></tr><tr><td>str</td><td>str</td><td>datetime[μs, Asia/Kolkata]</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td></tr></thead><tbody><tr><td>&quot;60000005516&quot;</td><td>&quot;delhi&quot;</td><td>2022-05-03 00:00:00 IST</td><td>117.316896</td><td>113.919348</td><td>3.397549</td><td>89.4018</td><td>0.276361</td><td>45.6288</td><td>51.42</td><td>2.498187</td><td>74.945605</td></tr><tr><td>&quot;60000005516&quot;</td><td>&quot;delhi&quot;</td><td>2022-05-07 00:00:00 IST</td><td>160.976491</td><td>155.999636</td><td>4.976855</td><td>113.246</td><td>0.259375</td><td>47.952</td><td>83.0313</td><td>3.058557</td><td>91.756698</td></tr><tr><td>&quot;60000005516&quot;</td><td>&quot;delhi&quot;</td><td>2022-05-08 00:00:00 IST</td><td>153.855828</td><td>149.593574</td><td>4.262254</td><td>98.9624</td><td>0.246268</td><td>49.6704</td><td>70.884</td><td>3.201662</td><td>96.049873</td></tr><tr><td>&quot;60000005516&quot;</td><td>&quot;delhi&quot;</td><td>2022-05-09 00:00:00 IST</td><td>163.711032</td><td>157.587469</td><td>6.123563</td><td>117.948</td><td>0.263893</td><td>68.7002</td><td>65.386</td><td>3.090698</td><td>92.720928</td></tr><tr><td>&quot;60000005516&quot;</td><td>&quot;delhi&quot;</td><td>2022-05-10 00:00:00 IST</td><td>158.452079</td><td>152.711096</td><td>5.740984</td><td>109.1879</td><td>0.2521</td><td>22.924</td><td>107.0099</td><td>2.815476</td><td>84.464277</td></tr><tr><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td></tr><tr><td>&quot;60029920067&quot;</td><td>&quot;delhi&quot;</td><td>2022-05-09 00:00:00 IST</td><td>16.247836</td><td>15.879055</td><td>0.368782</td><td>12.897</td><td>0.3</td><td>6.307</td><td>6.59</td><td>2.428239</td><td>72.847174</td></tr><tr><td>&quot;60029920067&quot;</td><td>&quot;delhi&quot;</td><td>2022-05-10 00:00:00 IST</td><td>16.314472</td><td>16.034948</td><td>0.279524</td><td>11.95</td><td>0.267398</td><td>8.717</td><td>4.69</td><td>2.557908</td><td>76.737238</td></tr><tr><td>&quot;60029920067&quot;</td><td>&quot;delhi&quot;</td><td>2022-05-11 00:00:00 IST</td><td>20.166763</td><td>19.920712</td><td>0.246051</td><td>13.87</td><td>0.259349</td><td>5.184</td><td>10.86</td><td>2.711752</td><td>81.352559</td></tr><tr><td>&quot;60029920067&quot;</td><td>&quot;delhi&quot;</td><td>2022-05-12 00:00:00 IST</td><td>22.773951</td><td>21.966118</td><td>0.807834</td><td>17.215</td><td>0.275661</td><td>5.01</td><td>13.725</td><td>2.508278</td><td>75.24833</td></tr><tr><td>&quot;60029920067&quot;</td><td>&quot;delhi&quot;</td><td>2022-05-13 00:00:00 IST</td><td>19.603421</td><td>19.40534</td><td>0.198081</td><td>15.423</td><td>0.3</td><td>7.293</td><td>8.13</td><td>2.13013</td><td>63.90391</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (1_415_224, 12)\n",
       "┌────────────┬───────┬────────────┬────────────┬───┬───────────┬───────────┬───────────┬───────────┐\n",
       "│ ca_id      ┆ city  ┆ day        ┆ E_base     ┆ … ┆ forward_k ┆ backward_ ┆ avg_shift ┆ avg_shift │\n",
       "│ ---        ┆ ---   ┆ ---        ┆ ---        ┆   ┆ Wh        ┆ kWh       ┆ _steps    ┆ _minutes  │\n",
       "│ str        ┆ str   ┆ datetime[μ ┆ f64        ┆   ┆ ---       ┆ ---       ┆ ---       ┆ ---       │\n",
       "│            ┆       ┆ s, Asia/Ko ┆            ┆   ┆ f64       ┆ f64       ┆ f64       ┆ f64       │\n",
       "│            ┆       ┆ lkata]     ┆            ┆   ┆           ┆           ┆           ┆           │\n",
       "╞════════════╪═══════╪════════════╪════════════╪═══╪═══════════╪═══════════╪═══════════╪═══════════╡\n",
       "│ 6000000551 ┆ delhi ┆ 2022-05-03 ┆ 117.316896 ┆ … ┆ 45.6288   ┆ 51.42     ┆ 2.498187  ┆ 74.945605 │\n",
       "│ 6          ┆       ┆ 00:00:00   ┆            ┆   ┆           ┆           ┆           ┆           │\n",
       "│            ┆       ┆ IST        ┆            ┆   ┆           ┆           ┆           ┆           │\n",
       "│ 6000000551 ┆ delhi ┆ 2022-05-07 ┆ 160.976491 ┆ … ┆ 47.952    ┆ 83.0313   ┆ 3.058557  ┆ 91.756698 │\n",
       "│ 6          ┆       ┆ 00:00:00   ┆            ┆   ┆           ┆           ┆           ┆           │\n",
       "│            ┆       ┆ IST        ┆            ┆   ┆           ┆           ┆           ┆           │\n",
       "│ 6000000551 ┆ delhi ┆ 2022-05-08 ┆ 153.855828 ┆ … ┆ 49.6704   ┆ 70.884    ┆ 3.201662  ┆ 96.049873 │\n",
       "│ 6          ┆       ┆ 00:00:00   ┆            ┆   ┆           ┆           ┆           ┆           │\n",
       "│            ┆       ┆ IST        ┆            ┆   ┆           ┆           ┆           ┆           │\n",
       "│ 6000000551 ┆ delhi ┆ 2022-05-09 ┆ 163.711032 ┆ … ┆ 68.7002   ┆ 65.386    ┆ 3.090698  ┆ 92.720928 │\n",
       "│ 6          ┆       ┆ 00:00:00   ┆            ┆   ┆           ┆           ┆           ┆           │\n",
       "│            ┆       ┆ IST        ┆            ┆   ┆           ┆           ┆           ┆           │\n",
       "│ 6000000551 ┆ delhi ┆ 2022-05-10 ┆ 158.452079 ┆ … ┆ 22.924    ┆ 107.0099  ┆ 2.815476  ┆ 84.464277 │\n",
       "│ 6          ┆       ┆ 00:00:00   ┆            ┆   ┆           ┆           ┆           ┆           │\n",
       "│            ┆       ┆ IST        ┆            ┆   ┆           ┆           ┆           ┆           │\n",
       "│ …          ┆ …     ┆ …          ┆ …          ┆ … ┆ …         ┆ …         ┆ …         ┆ …         │\n",
       "│ 6002992006 ┆ delhi ┆ 2022-05-09 ┆ 16.247836  ┆ … ┆ 6.307     ┆ 6.59      ┆ 2.428239  ┆ 72.847174 │\n",
       "│ 7          ┆       ┆ 00:00:00   ┆            ┆   ┆           ┆           ┆           ┆           │\n",
       "│            ┆       ┆ IST        ┆            ┆   ┆           ┆           ┆           ┆           │\n",
       "│ 6002992006 ┆ delhi ┆ 2022-05-10 ┆ 16.314472  ┆ … ┆ 8.717     ┆ 4.69      ┆ 2.557908  ┆ 76.737238 │\n",
       "│ 7          ┆       ┆ 00:00:00   ┆            ┆   ┆           ┆           ┆           ┆           │\n",
       "│            ┆       ┆ IST        ┆            ┆   ┆           ┆           ┆           ┆           │\n",
       "│ 6002992006 ┆ delhi ┆ 2022-05-11 ┆ 20.166763  ┆ … ┆ 5.184     ┆ 10.86     ┆ 2.711752  ┆ 81.352559 │\n",
       "│ 7          ┆       ┆ 00:00:00   ┆            ┆   ┆           ┆           ┆           ┆           │\n",
       "│            ┆       ┆ IST        ┆            ┆   ┆           ┆           ┆           ┆           │\n",
       "│ 6002992006 ┆ delhi ┆ 2022-05-12 ┆ 22.773951  ┆ … ┆ 5.01      ┆ 13.725    ┆ 2.508278  ┆ 75.24833  │\n",
       "│ 7          ┆       ┆ 00:00:00   ┆            ┆   ┆           ┆           ┆           ┆           │\n",
       "│            ┆       ┆ IST        ┆            ┆   ┆           ┆           ┆           ┆           │\n",
       "│ 6002992006 ┆ delhi ┆ 2022-05-13 ┆ 19.603421  ┆ … ┆ 7.293     ┆ 8.13      ┆ 2.13013   ┆ 63.90391  │\n",
       "│ 7          ┆       ┆ 00:00:00   ┆            ┆   ┆           ┆           ┆           ┆           │\n",
       "│            ┆       ┆ IST        ┆            ┆   ┆           ┆           ┆           ┆           │\n",
       "└────────────┴───────┴────────────┴────────────┴───┴───────────┴───────────┴───────────┴───────────┘"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics_greedy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cdae489",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (18_406_797, 8)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>ca_id</th><th>city</th><th>day</th><th>t_slot</th><th>s_slot</th><th>kwh</th><th>direction</th><th>abs_steps</th></tr><tr><td>str</td><td>str</td><td>datetime[μs, Asia/Kolkata]</td><td>i64</td><td>i64</td><td>f64</td><td>str</td><td>i64</td></tr></thead><tbody><tr><td>&quot;60000005516&quot;</td><td>&quot;delhi&quot;</td><td>2022-05-03 00:00:00 IST</td><td>32</td><td>30</td><td>13.0</td><td>&quot;backward&quot;</td><td>2</td></tr><tr><td>&quot;60000005516&quot;</td><td>&quot;delhi&quot;</td><td>2022-05-03 00:00:00 IST</td><td>31</td><td>30</td><td>15.313</td><td>&quot;backward&quot;</td><td>1</td></tr><tr><td>&quot;60000005516&quot;</td><td>&quot;delhi&quot;</td><td>2022-05-03 00:00:00 IST</td><td>43</td><td>46</td><td>0.002</td><td>&quot;forward&quot;</td><td>3</td></tr><tr><td>&quot;60000005516&quot;</td><td>&quot;delhi&quot;</td><td>2022-05-03 00:00:00 IST</td><td>42</td><td>40</td><td>0.001</td><td>&quot;backward&quot;</td><td>2</td></tr><tr><td>&quot;60000005516&quot;</td><td>&quot;delhi&quot;</td><td>2022-05-03 00:00:00 IST</td><td>44</td><td>46</td><td>0.001</td><td>&quot;forward&quot;</td><td>2</td></tr><tr><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td></tr><tr><td>&quot;60029920067&quot;</td><td>&quot;delhi&quot;</td><td>2022-05-13 00:00:00 IST</td><td>34</td><td>31</td><td>1.41</td><td>&quot;backward&quot;</td><td>3</td></tr><tr><td>&quot;60029920067&quot;</td><td>&quot;delhi&quot;</td><td>2022-05-13 00:00:00 IST</td><td>44</td><td>46</td><td>1.68</td><td>&quot;forward&quot;</td><td>2</td></tr><tr><td>&quot;60029920067&quot;</td><td>&quot;delhi&quot;</td><td>2022-05-13 00:00:00 IST</td><td>43</td><td>46</td><td>1.4</td><td>&quot;forward&quot;</td><td>3</td></tr><tr><td>&quot;60029920067&quot;</td><td>&quot;delhi&quot;</td><td>2022-05-13 00:00:00 IST</td><td>8</td><td>7</td><td>0.8</td><td>&quot;backward&quot;</td><td>1</td></tr><tr><td>&quot;60029920067&quot;</td><td>&quot;delhi&quot;</td><td>2022-05-13 00:00:00 IST</td><td>45</td><td>46</td><td>1.563</td><td>&quot;forward&quot;</td><td>1</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (18_406_797, 8)\n",
       "┌─────────────┬───────┬─────────────────────────┬────────┬────────┬────────┬───────────┬───────────┐\n",
       "│ ca_id       ┆ city  ┆ day                     ┆ t_slot ┆ s_slot ┆ kwh    ┆ direction ┆ abs_steps │\n",
       "│ ---         ┆ ---   ┆ ---                     ┆ ---    ┆ ---    ┆ ---    ┆ ---       ┆ ---       │\n",
       "│ str         ┆ str   ┆ datetime[μs,            ┆ i64    ┆ i64    ┆ f64    ┆ str       ┆ i64       │\n",
       "│             ┆       ┆ Asia/Kolkata]           ┆        ┆        ┆        ┆           ┆           │\n",
       "╞═════════════╪═══════╪═════════════════════════╪════════╪════════╪════════╪═══════════╪═══════════╡\n",
       "│ 60000005516 ┆ delhi ┆ 2022-05-03 00:00:00 IST ┆ 32     ┆ 30     ┆ 13.0   ┆ backward  ┆ 2         │\n",
       "│ 60000005516 ┆ delhi ┆ 2022-05-03 00:00:00 IST ┆ 31     ┆ 30     ┆ 15.313 ┆ backward  ┆ 1         │\n",
       "│ 60000005516 ┆ delhi ┆ 2022-05-03 00:00:00 IST ┆ 43     ┆ 46     ┆ 0.002  ┆ forward   ┆ 3         │\n",
       "│ 60000005516 ┆ delhi ┆ 2022-05-03 00:00:00 IST ┆ 42     ┆ 40     ┆ 0.001  ┆ backward  ┆ 2         │\n",
       "│ 60000005516 ┆ delhi ┆ 2022-05-03 00:00:00 IST ┆ 44     ┆ 46     ┆ 0.001  ┆ forward   ┆ 2         │\n",
       "│ …           ┆ …     ┆ …                       ┆ …      ┆ …      ┆ …      ┆ …         ┆ …         │\n",
       "│ 60029920067 ┆ delhi ┆ 2022-05-13 00:00:00 IST ┆ 34     ┆ 31     ┆ 1.41   ┆ backward  ┆ 3         │\n",
       "│ 60029920067 ┆ delhi ┆ 2022-05-13 00:00:00 IST ┆ 44     ┆ 46     ┆ 1.68   ┆ forward   ┆ 2         │\n",
       "│ 60029920067 ┆ delhi ┆ 2022-05-13 00:00:00 IST ┆ 43     ┆ 46     ┆ 1.4    ┆ forward   ┆ 3         │\n",
       "│ 60029920067 ┆ delhi ┆ 2022-05-13 00:00:00 IST ┆ 8      ┆ 7      ┆ 0.8    ┆ backward  ┆ 1         │\n",
       "│ 60029920067 ┆ delhi ┆ 2022-05-13 00:00:00 IST ┆ 45     ┆ 46     ┆ 1.563  ┆ forward   ┆ 1         │\n",
       "└─────────────┴───────┴─────────────────────────┴────────┴────────┴────────┴───────────┴───────────┘"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flows_greedy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac9dd9bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save for analysis\n",
    "# metrics: one row per customer-day with emissions & movement stats\n",
    "metrics_df.write_parquet(\"metrics_by_day.parquet\")\n",
    "# flows: sparse flows per customer-day, great for distance/direction analysis\n",
    "flows_df.write_parquet(\"flows_by_day.parquet\")\n",
    "# optimised rows: 48 rows per customer-day with optimised_value\n",
    "if optim_df is not None:\n",
    "    optim_df.write_parquet(\"optimised_usage_rows.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7884c267",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Emissions saved by customer over the full period\n",
    "by_customer = (\n",
    "    metrics_df\n",
    "    .group_by(\"ca_id\")\n",
    "    .agg([\n",
    "        pl.sum(\"delta_E\").alias(\"total_delta_E\"),\n",
    "        pl.mean(\"share_moved\").alias(\"avg_share_moved\"),\n",
    "        pl.count().alias(\"num_days\"),\n",
    "    ])\n",
    ")\n",
    "\n",
    "# Emissions saved by city and month\n",
    "by_city_month = (\n",
    "    metrics_df\n",
    "    .with_columns(pl.col(\"day\").dt.strftime(\"%Y-%m\").alias(\"ym\"))\n",
    "    .group_by([\"city\", \"ym\"])\n",
    "    .agg([\n",
    "        pl.sum(\"delta_E\").alias(\"delta_E_sum\"),\n",
    "        pl.mean(\"share_moved\").alias(\"share_moved_mean\"),\n",
    "        pl.mean(\"avg_shift_minutes\").alias(\"avg_shift_minutes_mean\"),\n",
    "    ])\n",
    ")\n",
    "\n",
    "# Direction & distance of moves (forward/backward, avg steps)\n",
    "flow_summary = (\n",
    "    flows_df\n",
    "    .group_by([\"city\"])\n",
    "    .agg([\n",
    "        pl.sum(pl.when(pl.col(\"direction\")==\"forward\").then(pl.col(\"kwh\")).otherwise(0)).alias(\"forward_kWh\"),\n",
    "        pl.sum(pl.when(pl.col(\"direction\")==\"backward\").then(pl.col(\"kwh\")).otherwise(0)).alias(\"backward_kWh\"),\n",
    "        pl.mean(\"abs_steps\").alias(\"avg_steps_weighted?\"),  # careful: this isn't energy-weighted\n",
    "    ])\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64daced9",
   "metadata": {},
   "outputs": [],
   "source": [
    "energy_weighted_steps = (\n",
    "    flows_df\n",
    "    .with_columns((pl.col(\"abs_steps\") * pl.col(\"kwh\")).alias(\"steps_x_kwh\"))\n",
    "    .group_by([\"city\"])\n",
    "    .agg([\n",
    "        (pl.sum(\"steps_x_kwh\") / pl.sum(\"kwh\")).alias(\"avg_steps_energy_weighted\")\n",
    "    ])\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b7f5c8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "try: prob.solve(solver=cp.GLPK)\n",
    "except: prob.solve(solver=cp.ECOS)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d96c7f6",
   "metadata": {},
   "source": [
    "1. Mixed Integer Linear Programming (MILP)\n",
    "Why:\n",
    "\n",
    "Exact, globally optimal solution under your constraints.\n",
    "\n",
    "Handles discrete shifts (more realistic if you want to preserve original half-hour block structure).\n",
    "\n",
    "Can easily encode “±2 hour” shift constraint via a binary feasibility matrix.\n",
    "\n",
    "Pros:\n",
    "\n",
    "Guarantees optimality.\n",
    "\n",
    "Transparent formulation.\n",
    "\n",
    "Easy to interpret results.\n",
    "\n",
    "Cons:\n",
    "\n",
    "Computationally heavy for long time horizons and many customers.\n",
    "\n",
    "May need decomposition or batching for multi-year datasets.\n",
    "\n",
    "Tooling:\n",
    "\n",
    "PuLP, Pyomo, or ortools (Python) with solvers like CBC, Gurobi, CPLEX."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdefcb3a",
   "metadata": {},
   "source": [
    "#### Quadratic / Constrained Linear Optimisation "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b8d9b1d",
   "metadata": {},
   "source": [
    "2. Quadratic / Constrained Linear Optimisation (Continuous Relaxation)\n",
    "Why:\n",
    "\n",
    "Fast if consumption is treated as divisible (continuous variables).\n",
    "\n",
    "Works well if the real-world shifting flexibility is granular.\n",
    "\n",
    "Can be solved with methods like Newton, Quasi-Newton, or Krylov subspace.\n",
    "\n",
    "Pros:\n",
    "\n",
    "Much faster than MILP.\n",
    "\n",
    "Scales to multi-year data without decomposition.\n",
    "\n",
    "Easily integrates smooth penalties (e.g., comfort penalties).\n",
    "\n",
    "Cons:\n",
    "\n",
    "May produce fractional solutions (not directly interpretable if you need discrete slots).\n",
    "\n",
    "Not guaranteed to preserve \"integer\" block sizes without rounding.\n",
    "\n",
    "Tooling:\n",
    "\n",
    "scipy.optimize.minimize (SLSQP, trust-constr, Newton-CG, etc.),\n",
    "\n",
    "cvxpy (continuous convex optimisation)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5eb55310",
   "metadata": {},
   "source": [
    "#### Greedy / Heuristic Shift Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d7a62f9",
   "metadata": {},
   "source": [
    "3. Greedy / Heuristic Shift Algorithm\n",
    "Why:\n",
    "\n",
    "Computationally lightweight.\n",
    "\n",
    "Good for very large datasets when exact MILP is impractical.\n",
    "\n",
    "Works by iteratively moving load from highest-MEF slots to lowest-MEF available within ±2 hours.\n",
    "\n",
    "Pros:\n",
    "\n",
    "Very scalable.\n",
    "\n",
    "Simple to implement and debug.\n",
    "\n",
    "Produces feasible (though not guaranteed optimal) solutions.\n",
    "\n",
    "Cons:\n",
    "\n",
    "No optimality guarantee.\n",
    "\n",
    "May get stuck in local minima.\n",
    "\n",
    "Sensitive to tie-breaking rules.\n",
    "\n",
    "Tooling:\n",
    "\n",
    "Pure pandas/polars with NumPy-based operations."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "irpenv_4",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
